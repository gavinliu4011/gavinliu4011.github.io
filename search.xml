<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Flask(二)]]></title>
    <url>%2Fpost%2Fdc6a7e6c.html</url>
    <content type="text"><![CDATA[SessionFlask除请求对象之外，还有一个 session 对象。它允许你在不同请求间存储特定用户的信息。它是在 Cookies 的基础上实现的，并且对 Cookies 进行密钥签名要使用会话，你需要设置一个密钥。 设置：session[‘username’] ＝ ‘xxx’ 删除：session.pop(‘username’, None) 基本使用12345678910111213141516171819202122232425262728293031323334353637from flask import Flask, session, redirect, url_for, escape, requestapp = Flask(__name__)@app.route('/')def index(): if 'username' in session: return 'Logged in as %s' % escape(session['username']) return 'You are not logged in'@app.route('/login', methods=['GET', 'POST'])def login(): if request.method == 'POST': session['username'] = request.form['username'] return redirect(url_for('index')) return ''' &lt;form action="" method="post"&gt; &lt;p&gt;&lt;input type=text name=username&gt; &lt;p&gt;&lt;input type=submit value=Login&gt; &lt;/form&gt; '''@app.route('/logout')def logout(): # remove the username from the session if it's there session.pop('username', None) return redirect(url_for('index'))# set the secret key. keep this really secret:app.secret_key = 'A0Zr98j/3yX R~XHH!jmN]LWX/,?RT'if __name__ == '__main__': app.run() 第三方session（我喜欢用这种类型）这里使用Redis类型，首先在虚拟环境安装两个包，分别是pip install redis和pip install flask-session 1234567891011121314151617181920212223242526272829from flask import Flask, session, redirectfrom flask_session import Sessionfrom redis import Redisapp = Flask(__name__)app.debug = True# 这里可以任意填写一个字符串，我这里是采用base64.b64encode(os.urandom(48))app.secret_key = 'ix4En7l1Hau10aPq8kv8tuzcVl1s2Zo6eA+5+R+CXor8G3Jo0IJvcj001jz3XuXl'app.config['SESSION_TYPE'] = 'redis'app.config['SESSION_REDIS'] = Redis(host='127.0.0.1', port='6379')Session(app)@app.route('/login')def login(): session['username'] = 'gavinliu' return redirect('/index')@app.route('/index')def index(): name = session['username'] return nameif __name__ == '__main__': app.run() 蓝图什么是蓝图在Flask项目中可以用Blueprint(蓝图)实现模块化的应用，使用蓝图可以让应用层次更清晰，开发者更容易去维护和开发项目。蓝图将作用于相同的URL前缀的请求地址，将具有相同前缀的请求都放在一个模块中，这样查找问题，一看路由就很快的可以找到对应的视图，并解决问题了。 使用蓝图安装1pip install flask_blueprint 实例化蓝图1blue = Blueprint(&apos;first&apos;，__name__) 注意：Blueprint中传入了两个参数，第一个是蓝图的名称，第二个是蓝图所在的包或模块，__name__代表当前模块名或者包名 注册蓝图123app = Flask(__name__)app.register_blueprint(blue, url_prefix='/user') 注意：注意：第一个参数即我们定义初始化定义的蓝图对象，第二个参数url_prefix表示该蓝图下，所有的url请求必须以/user开始。这样对一个模块的url可以很好的进行统一管理 使用蓝图1234@blue.route('/', methods=['GET', 'POST'], endpoint='index')def hello(): # 视图函数 return 'Hello World' 注意：这时候访问应该是127.0.0.1:5000/user/ url_for反向解析1url_for(endpoint, 参数名=value) # 如果在路由上没有配置endpoint，第一个参数可以传递&quot;蓝图中定义的第一个参数.函数名&quot; 12345from flask import url_for, redirect@blue.route('/redirect/')def make_redirect(): return redirect(url_for('index')) 请求扩展（装饰器实现） @app.before_request @app.after_request @app.errorhandler # 状态码错误信息 @app.before_first_request # 请求来只执行第一次 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859from flask import Flaskapp = Flask(__name__)app.debug = True@app.before_requestdef process_request1(*args, **kwargs): print('process_request1进来了')@app.before_requestdef process_request2(*args, **kwargs): print('process_request2进来了')@app.after_requestdef process_response1(response): print('process_response1走了') return response@app.after_requestdef process_response2(response): print('process_response2走了') return response@app.before_first_requestdef first(*args, **kwargs): print('first')@app.errorhandler(404)def error_404(arg): return '404错误了'# 可自定义模板@app.template_global()def sb(a1, a2): return a1 + a2# 调用方式：&#123;&#123;sb(1,2)&#125;&#125;@app.template_filter()def db(a1, a2, a3): return a1 + a2 + a3# 调用方式：&#123;&#123; 1|db(2,3)&#125;&#125;@app.route('/')def index(): print('index函数') return 'Hello World!'if __name__ == '__main__': app.run() 这个执行流程类似django中间件，可以参考django中间件 中间件其实Flask也有中间件，Django中也有中间件，但是Flask中的请求扩展就相当于Django中的中间件，而Django中的中间件却不相当于Flask中的中间件 123456789101112131415161718192021222324from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return 'Hello World!'class Md(object): def __init__(self, old_wsgi_app): self.old_wsgi_app = old_wsgi_app def __call__(self, environ, start_response): print('开始之前') ret = self.old_wsgi_app(environ, start_response) print('结束之后') return retif __name__ == '__main__': app.wsgi_app = Md(app.wsgi_app) app.run()]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask(一)]]></title>
    <url>%2Fpost%2F735be50e.html</url>
    <content type="text"><![CDATA[Flask介绍Flask是一个用Python编写的Web应用程序框架。 它由Armin Ronacher开发，他领导着一个名为Pocco的Python爱好者的国际组织。 Flask基于Werkzeug WSGI工具包和Jinja2模板引擎。 WSGIWeb服务器网关接口(WSGI)已被采纳为Python Web应用程序开发的标准。 WSGI是Web服务器和Web应用程序之间通用接口的规范。 WERKZEUG它是一个WSGI工具包，它实现了请求，响应对象和其他实用程序功能。 这可以在其上构建Web框架。 Flask框架使用Werkzeug作为其一个基础模块之一。 浅谈Django、Flask和Tornado区别？ Django：简单的说Django是一个大而全的Web框架，内置了很多组件，ORM、admin、Form、 ModelForm、中间件、信号和缓存等。基于wsgi协议部署的，使用wsgiref模块实现此协议； Flask：微型的小而精的Web框架，可扩展性强，内置的组件很少，需要引入第三方组件实现功能业务，如果开发简单的项目，使用Flask比较快速和方便。如果开发大型项目，需要引入大量的第三方组件，这时Flask会越来越像Django框架。基于wsgi协议部署，使用werkzeug模块实现此协议，模板系统由 Jinja2提供。 Tornado：是一个轻量级的Web框架，可扩展性强，用于快速开发简单程序，用于强大的异步非阻塞和内置WebSocket功能。 Flask快速入门安装在创建项目之前我们需要安装Flask，当然在这之前应该创建虚拟环境,在虚拟环境中安装Flask，在这里就不多说： 1pip install Flask 基本使用123456789101112from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 参数介绍： flask(import_name, static_url_path, template_folder, instance_path, instance_relative_config, root_path, static_folder) import_name：要创建的app的名称 static_url_path：用来指定url路径中static代表的路径，可以看作别名，类似Django中的STATICFILES_DIRS配置，static用来存放静态文件，默认为static static_folder：指定静态文件的存放目录，默认为static template_folder：指定模板文件的存放目录，默认为templates root_path：应用搜索static、templates等目录的根目录，也就是说，会在root_path指定的目录下搜索static、templates文件夹 如果你没有指定root_path，那么Flask就会将import_name所在的目录作为root_oath instance_relative_config：这个参数只在为app生成配置的时候有用，app在生成Config的时候，make_config传递进入的是root_path还是在实例化app时指定的instance_path instance_path：当instance_relative_config=True的时候该参数才有效 ，如果instance_path=None(默认)，默认搜索配置文件的路径就是root_path下的instance/目录 配置文件flask中的配置文件是一个flask.config.Config对象（继承字典）,默认配置为 : 123456789101112131415161718192021222324252627282930&#123; &apos;DEBUG&apos;: get_debug_flag(default=False), # 是否开启Debug模式 &apos;TESTING&apos;: False, # 是否开启测试模式 &apos;PROPAGATE_EXCEPTIONS&apos;: None, &apos;PRESERVE_CONTEXT_ON_EXCEPTION&apos;: None, &apos;SECRET_KEY&apos;: None, &apos;PERMANENT_SESSION_LIFETIME&apos;: timedelta(days=31), &apos;USE_X_SENDFILE&apos;: False, &apos;LOGGER_NAME&apos;: None, &apos;LOGGER_HANDLER_POLICY&apos;: &apos;always&apos;, &apos;SERVER_NAME&apos;: None, &apos;APPLICATION_ROOT&apos;: None, &apos;SESSION_COOKIE_NAME&apos;: &apos;session&apos;, &apos;SESSION_COOKIE_DOMAIN&apos;: None, &apos;SESSION_COOKIE_PATH&apos;: None, &apos;SESSION_COOKIE_HTTPONLY&apos;: True, &apos;SESSION_COOKIE_SECURE&apos;: False, &apos;SESSION_REFRESH_EACH_REQUEST&apos;: True, &apos;MAX_CONTENT_LENGTH&apos;: None, &apos;SEND_FILE_MAX_AGE_DEFAULT&apos;: timedelta(hours=12), &apos;TRAP_BAD_REQUEST_ERRORS&apos;: False, &apos;TRAP_HTTP_EXCEPTIONS&apos;: False, &apos;EXPLAIN_TEMPLATE_LOADING&apos;: False, &apos;PREFERRED_URL_SCHEME&apos;: &apos;http&apos;, &apos;JSON_AS_ASCII&apos;: True, &apos;JSON_SORT_KEYS&apos;: True, &apos;JSONIFY_PRETTYPRINT_REGULAR&apos;: True, &apos;JSONIFY_MIMETYPE&apos;: &apos;application/json&apos;, &apos;TEMPLATES_AUTO_RELOAD&apos;: None, &#125; 如果需要修改配置，可以根据下面的方式修改： 方式一 app.config[DEBUG]=True 注意：由于Config对象本质上是字典，可以使用app.config.update(…) 方式二 app.config.form_pyfile(‘python文件名称’) 如:settings.py 1DEBUG = True app.config.from_pyfile(‘settings.py’) 方式三 app.config.form_envvar(‘环境变量名称’) 环境变量的值为python文件名称，内部调用form_pyfile方法 方式四 app.config.form_json(‘json文件名称’) json文件必须是json格式，内部会执行json.loads 方式五 app.config.from_mapping({‘DEBUG’:True}) 方式六（推荐使用） app.config.from_object(‘python类或类的路径’) #默认从根目录开始 settings.py文件 12345678910111213141516171819202122232425262728293031323334class Config(object): &quot;&quot;&quot;应用程序配置类&quot;&quot;&quot; # 开启调试模式 DEBUG = True # logging等级 LOGGIONG_LEVEL = logging.DEBUG # flask-sqlalchemy使用的参数 SQLALCHEMY_DATABASE_URI = &apos;mysql+pymysql://root:root@127.0.0.1:3306/ehome&apos; # 追踪数据库的修改行为，如果不设置会报警告，不影响代码的执行 SQLALCHEMY_TRACK_MODIFICATIONS = True # 显示sql语句 # SQLALCHEMY_ECHO = Trueclass DevelopConfig(Config): &quot;&quot;&quot;开发阶段下的配置子类&quot;&quot;&quot; # logging等级 LOGGIONG_LEVEL = logging.DEBUGclass UnitTestConfig(Config): &quot;&quot;&quot;单元测试配置子类&quot;&quot;&quot; # logging等级 LOGGIONG_LEVEL = logging.DEBUG SQLALCHEMY_DATABASE_URI = &apos;mysql://root:root@127.0.0.1:3306/ehome_test&apos;class ProductionConfig(Config): &quot;&quot;&quot;生产环境下配置子类&quot;&quot;&quot; # logging等级 LOGGIONG_LEVEL = logging.WARNING DEBUG = False SQLALCHEMY_DATABASE_URI = &apos;mysql://root:root@47.106.93.190:3306127.0.0.1:3306/ehome&apos; 注意：settings.py文件默认路径要放在程序root_path目录，如果instance_relative_config为True，则就是instance_path目录 路由系统12345# @app.route('/user/&lt;username&gt;')# @app.route('/post/&lt;int:post_id&gt;')# @app.route('/post/float:post_id')# @app.route('/post/path:path')# @app.route('/login', methods=['GET', 'POST']) 以上五种路由是最常用的，下面再来了解一下路由的执行过程： 123@app.route('/', methods=['GET','POST'], endpoint='hello')def index(): return 'Hello World!' 当执行app.route(‘/‘,methods=[‘GET’,’POST’],endpoint=’hello’)时会执行如下代码： 123456789def route(self, rule, **options): # self 是app对象 # rule = '/' # options = &#123;methods=['GET','POST'], endpoint='hello'&#125; def decorator(f): endpoint = options.pop('endpoint', None) self.add_url_rule(rule, endpoint, f, **options) return f return decorator 实质就是decorator = app.route(‘/‘,methods=[‘GET’,’POST’],endpoint=’hello’) 有返回值后，实际上就是@decorator，这次执行decorator函数会把index函数当做参数传入 我们这里写的路由其实就是执行了app.add_url_rule(‘/‘, ‘hello’, index, methods=[‘GET’,’POST‘])这句代码，这就跟django框架一样了 在来看看 @app.route和app.add_url_rule常用参数（只是部分）： rule：URL规则 view_func：视图函数名称 defaults=None：默认值，当URL中无参数，函数需要参数时，使用defaults={‘k’:’v’}为函数提供参数 endpoint=None：名称，用于反向生成URL，即： url_for(‘名称’) methods=None：允许的请求方式，如：[‘GET’,’POST’] strict_slashes=None：对URL最后的 / 符号是否严格要求 redirect_to=None：重定向到指定地址，如下： 12345@app.route(&apos;/index/&lt;int:nid&gt;&apos;, redirect_to=&apos;/home/&lt;nid&gt;&apos;)或def func(adapter, nid): return &quot;/home/888&quot;@app.route(&apos;/index/&lt;int:nid&gt;&apos;, redirect_to=func) 所有的路由系统都是基于一下对应关系来处理 ： 123456789DEFAULT_CONVERTERS = &#123; &apos;default&apos;: UnicodeConverter, &apos;string&apos;: UnicodeConverter, &apos;any&apos;: AnyConverter, &apos;path&apos;: PathConverter, &apos;int&apos;: IntegerConverter, &apos;float&apos;: FloatConverter, &apos;uuid&apos;: UUIDConverter,&#125; 注意：此配置在werkzeug包下routing.py文件中 以上的路由其实已经够用了，但是不排除有一些比较另类的，这时可以考虑自定制正则路由匹配 1234567891011121314151617181920212223242526272829303132333435363738394041from flask import Flask, views, url_forfrom werkzeug.routing import BaseConverterapp = Flask(import_name=__name__)class RegexConverter(BaseConverter): """ 自定义URL匹配正则表达式 """ def __init__(self, url_map, regex): super(RegexConverter, self).__init__(url_map) self.regex = regex def to_python(self, value): """ 路由匹配时，匹配成功后传递给视图函数中参数的值 :param value: :return: """ return int(value) def to_url(self, value): """ 使用url_for反向生成URL时，传递的参数经过该方法处理，返回的值用于生成URL中的参数 :param value: :return: """ val = super(RegexConverter, self).to_url(value) return val # 为app中的url路由添加正则表达式匹配app.url_map.converters['regex'] = RegexConverter@app.route('/index/&lt;regex("\d+"):nid&gt;')def index(nid): print(url_for('index', nid='666')) return 'Index' 请求和响应请求相关信息1234567891011121314151617# request.method# request.args# request.form# request.values# request.cookies# request.headers# request.path# request.full_path# request.script_root# request.url# request.base_url# request.url_root# request.host_url# request.host# request.files# obj = request.files['the_file_name']# obj.save('/var/www/uploads/' + secure_filename(f.filename)) 响应相关信息12345678910# return '字符串'# return render_template('html模板路径',**&#123;&#125;)# return redirect('/index.html')# response = make_response(render_template('index.html'))# response是flask.wrappers.Response类型# response.delete_cookie('key')# response.set_cookie('key', 'value')# response.headers['X-Something'] = 'A value'# return response]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版RabbitMQ消息队列之RPC（六）]]></title>
    <url>%2Fpost%2F8d587096.html</url>
    <content type="text"><![CDATA[远程过程调用（RPC）在第二篇教程中我们介绍了如何使用工作队列（work queue）在多个工作者（woker）中间分发耗时的任务。 可是如果我们需要将一个函数运行在远程计算机上并且等待从那儿获取结果时，该怎么办呢？这就是另外的故事了。这种模式通常被称为远程过程调用（Remote Procedure Call）或者RPC。 这篇教程中，我们会使用RabbitMQ来构建一个RPC系统：包含一个客户端和一个RPC服务器。现在的情况是，我们没有一个值得被分发的足够耗时的任务，所以接下来，我们会创建一个模拟RPC服务来返回斐波那契数列。 客户端接口为了展示RPC服务如何使用，我们创建了一个简单的客户端类。它会暴露出一个名为“call”的方法用来发送一个RPC请求，并且在收到回应前保持阻塞。 123fibonacci_rpc = FibonacciRpcClient()result = fibonacci_rpc.call(4)print(&quot;fib(4) is %r&quot; % (result,)) 关于RPC的注意事项：尽管RPC在计算领域是一个常用模式，但它也经常被诟病。当一个问题被抛出的时候，程序员往往意识不到这到底是由本地调用还是由较慢的RPC调用引起的。同样的困惑还来自于系统的不可预测性和给调试工作带来的不必要的复杂性。跟软件精简不同的是，滥用RPC会导致不可维护的面条代码. 考虑到这一点，牢记以下建议： 确保能够明确的搞清楚哪个函数是本地调用的，哪个函数是远程调用的。给你的系统编写文档。保持各个组件间的依赖明确。处理错误案例。明了客户端改如何处理RPC服务器的宕机和长时间无响应情况。 当对避免使用RPC有疑问的时候。如果可以的话，你应该尽量使用异步管道来代替RPC类的阻塞。结果被异步地推送到下一个计算场景。 回调队列一般来说通过RabbitMQ来实现RPC是很容易的。一个客户端发送请求信息，服务器端将其应用到一个回复信息中。为了接收到回复信息，客户端需要在发送请求的时候同时发送一个回调队列（callback queue）的地址。我们试试看： 1234567891011result = channel.queue_declare(exclusive=True)callback_queue = result.method.queuechannel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;rpc_queue&apos;, properties=pika.BasicProperties( reply_to = callback_queue, ), body=request)# ... and some code to read a response message from the callback_queue ... 消息属性AMQP协议给消息预定义了一系列的14个属性。大多数属性很少会用到，除了以下几个： delivery_mode（投递模式）：将消息标记为持久的（值为2）或暂存的（除了2之外的其他任何值）。第二篇教程里接触过这个属性，记得吧？ content_type（内容类型）:用来描述编码的mime-type。例如在实际使用中常常使用application/json来描述JOSN编码类型。 reply_to（回复目标）：通常用来命名回调队列。 correlation_id（关联标识）：用来将RPC的响应和请求关联起来。 关联标识上边介绍的方法中，我们建议给每一个RPC请求新建一个回调队列。这不是一个高效的做法，幸好这儿有一个更好的办法 —— 我们可以为每个客户端只建立一个独立的回调队列。 这就带来一个新问题，当此队列接收到一个响应的时候它无法辨别出这个响应是属于哪个请求的。correlation_id 就是为了解决这个问题而来的。我们给每个请求设置一个独一无二的值。稍后，当我们从回调队列中接收到一个消息的时候，我们就可以查看这条属性从而将响应和请求匹配起来。如果我们接手到的消息的correlation_id是未知的，那就直接销毁掉它，因为它不属于我们的任何一条请求。 你也许会问，为什么我们接收到未知消息的时候不抛出一个错误，而是要将它忽略掉？这是为了解决服务器端有可能发生的竞争情况。尽管可能性不大，但RPC服务器还是有可能在已将应答发送给我们但还未将确认消息发送给请求的情况下死掉。如果这种情况发生，RPC在重启后会重新处理请求。这就是为什么我们必须在客户端优雅的处理重复响应，同时RPC也需要尽可能保持幂等性。 总结 我们的RPC如此工作: 当客户端启动的时候，它创建一个匿名独享的回调队列。 在RPC请求中，客户端发送带有两个属性的消息：一个是设置回调队列的 reply_to 属性，另一个是设置唯一值的 correlation_id 属性。 将请求发送到一个 rpc_queue 队列中。 RPC工作者（又名：服务器）等待请求发送到这个队列中来。当请求出现的时候，它执行他的工作并且将带有执行结果的消息发送给reply_to字段指定的队列。 客户端等待回调队列里的数据。当有消息出现的时候，它会检查correlation_id属性。如果此属性的值与请求匹配，将它返回给应用。 整合到一起rpc_server.py代码： 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env pythonimport pikaconnection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;rpc_queue&apos;)def fib(n): if n == 0: return 0 elif n == 1: return 1 else: return fib(n-1) + fib(n-2)def on_request(ch, method, props, body): n = int(body) print(&quot; [.] fib(%s)&quot; % (n,)) response = fib(n) ch.basic_publish(exchange=&apos;&apos;, routing_key=props.reply_to, properties=pika.BasicProperties(correlation_id = \ props.correlation_id), body=str(response)) ch.basic_ack(delivery_tag = method.delivery_tag)channel.basic_qos(prefetch_count=1)channel.basic_consume(on_request, queue=&apos;rpc_queue&apos;)print(&quot; [x] Awaiting RPC requests&quot;)channel.start_consuming() 服务器端代码相当简单： （4）像往常一样，我们建立连接，声明队列 （11）我们声明我们的fibonacci函数，它假设只有合法的正整数当作输入。（别指望这个函数能处理很大的数值，函数递归你们都懂得…） （19）我们为 basic_consume 声明了一个回调函数，这是RPC服务器端的核心。它执行实际的操作并且作出响应。 （32）或许我们希望能在服务器上多开几个线程。为了能将负载平均地分摊到多个服务器，我们需要将 prefetch_count 设置好。 rpc_client.py 代码: 12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/env pythonimport pikaimport uuidclass FibonacciRpcClient(object): def __init__(self): self.connection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;)) self.channel = self.connection.channel() result = self.channel.queue_declare(exclusive=True) self.callback_queue = result.method.queue self.channel.basic_consume(self.on_response, no_ack=True, queue=self.callback_queue) def on_response(self, ch, method, props, body): if self.corr_id == props.correlation_id: self.response = body def call(self, n): self.response = None self.corr_id = str(uuid.uuid4()) self.channel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;rpc_queue&apos;, properties=pika.BasicProperties( reply_to = self.callback_queue, correlation_id = self.corr_id, ), body=str(n)) while self.response is None: self.connection.process_data_events() return int(self.response)fibonacci_rpc = FibonacciRpcClient()print(&quot; [x] Requesting fib(30)&quot;)response = fibonacci_rpc.call(30)print(&quot; [.] Got %r&quot; % (response,)) 客户端代码稍微有点难懂： （7）建立连接、通道并且为回复（replies）声明独享的回调队列。 （16）我们订阅这个回调队列，以便接收RPC的响应。 （18）“on_response”回调函数对每一个响应执行一个非常简单的操作，检查每一个响应消息的correlation_id属性是否与我们期待的一致，如果一致，将响应结果赋给self.response，然后跳出consuming循环。 （23）接下来，我们定义我们的主要方法 call 方法。它执行真正的RPC请求。 （24）在这个方法中，首先我们生成一个唯一的 correlation_id 值并且保存起来，’on_response’回调函数会用它来获取符合要求的响应。 （25）接下来，我们将带有 reply_to 和 correlation_id 属性的消息发布出去。 （32）现在我们可以坐下来，等待正确的响应到来。 （33）最后，我们将响应返回给用户。 我们的RPC服务已经准备就绪了，现在启动服务器端： 12$ python rpc_server.py [x] Awaiting RPC requests 运行客户端，请求一个fibonacci队列。 12$ python rpc_client.py [x] Requesting fib(30) 此处呈现的设计并不是实现RPC服务的唯一方式，但是他有一些重要的优势： 如果RPC服务器运行的过慢的时候，你可以通过运行另外一个服务器端轻松扩展它。试试在控制台中运行第二个 rpc_server.py 。 在客户端，RPC请求只发送或接收一条消息。不需要像 queue_declare 这样的异步调用。所以RPC客户端的单个请求只需要一个网络往返。 我们的代码依旧非常简单，而且没有试图去解决一些复杂（但是重要）的问题，如： 当没有服务器运行时，客户端如何作出反映。 客户端是否需要实现类似RPC超时的东西。 如果服务器发生故障，并且抛出异常，应该被转发到客户端吗？ 在处理前，防止混入无效的信息（例如检查边界） 如果你想做一些实验，你会发现rabbitmq-management plugin在观测队列方面是很有用处的。 （完整的rpc_client.py 和 rpc_server.py代码)]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版RabbitMQ消息队列之主题交换机（五）]]></title>
    <url>%2Fpost%2F3a0f7196.html</url>
    <content type="text"><![CDATA[为什么需要主题交换机？上一篇教程里，我们改进了我们的日志系统。我们使用直连交换机替代了扇型交换机，从只能盲目的广播消息改进为有可能选择性的接收日志。 尽管直连交换机能够改善我们的系统，但是它也有它的限制 —— 没办法基于多个标准执行路由操作。 在我们的日志系统中，我们不只希望订阅基于严重程度的日志，同时还希望订阅基于发送来源的日志。Unix工具syslog就是同时基于严重程度-severity (info/warn/crit…) 和 设备-facility (auth/cron/kern…)来路由日志的。 如果这样的话，将会给予我们非常大的灵活性，我们既可以监听来源于“cron”的严重程度为“critical errors”的日志，也可以监听来源于“kern”的所有日志。 为了实现这个目的，接下来我们学习如何使用另一种更复杂的交换机 —— 主题交换机。 主题交换机发送到主题交换机（topic exchange）的消息不可以携带随意什么样子的路由键（routing_key），它的路由键必须是一个由.分隔开的词语列表。这些单词随便是什么都可以，但是最好是跟携带它们的消息有关系的词汇。以下是几个推荐的例子：”stock.usd.nyse”, “nyse.vmw”, “quick.orange.rabbit”。词语的个数可以随意，但是不要超过255字节。 绑定键也必须拥有同样的格式。主题交换机背后的逻辑跟直连交换机很相似 —— 一个携带着特定路由键的消息会被主题交换机投递给绑定键与之想匹配的队列。但是它的绑定键和路由键有两个特殊应用方式： * (星号) 用来表示一个单词. # (井号) 用来表示任意数量（零个或多个）单词。 下边用图说明： 这个例子里，我们发送的所有消息都是用来描述小动物的。发送的消息所携带的路由键是由三个单词所组成的，这三个单词被两个.分割开。路由键里的第一个单词描述的是动物的手脚的利索程度，第二个单词是动物的颜色，第三个是动物的种类。所以它看起来是这样的： &lt;celerity&gt;.&lt;colour&gt;.&lt;species&gt;。 我们创建了三个绑定：Q1的绑定键为 *.orange.*，Q2的绑定键为 *.*.rabbit 和 lazy.# 。 这三个绑定键被可以总结为： Q1 对所有的桔黄色动物都感兴趣。 Q2 则是对所有的兔子和所有懒惰的动物感兴趣。 一个携带有 quick.orange.rabbit 的消息将会被分别投递给这两个队列。携带着 lazy.orange.elephant 的消息同样也会给两个队列都投递过去。另一方面携带有 quick.orange.fox 的消息会投递给第一个队列，携带有 lazy.brown.fox 的消息会投递给第二个队列。携带有 lazy.pink.rabbit 的消息只会被投递给第二个队列一次，即使它同时匹配第二个队列的两个绑定。携带着 quick.brown.fox 的消息不会投递给任何一个队列。 如果我们违反约定，发送了一个携带有一个单词或者四个单词（&quot;orange&quot; or &quot;quick.orange.male.rabbit&quot;）的消息时，发送的消息不会投递给任何一个队列，而且会丢失掉。 但是另一方面，即使 &quot;lazy.orange.male.rabbit&quot; 有四个单词，他还是会匹配最后一个绑定，并且被投递到第二个队列中。 主题交换机主题交换机是很强大的，它可以表现出跟其他交换机类似的行为 当一个队列的绑定键为 “#”（井号） 的时候，这个队列将会无视消息的路由键，接收所有的消息。 当 * (星号) 和 # (井号) 这两个特殊字符都未在绑定键中出现的时候，此时主题交换机就拥有的直连交换机的行为。 组合在一起接下来我们会将主题交换机应用到我们的日志系统中。在开始工作前，我们假设日志的路由键由两个单词组成，路由键看起来是这样的：&lt;facility&gt;.&lt;severity&gt; 代码跟上一篇教程差不多。 emit_log_topic.py的代码： 123456789101112131415161718#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;topic_logs&apos;, type=&apos;topic&apos;)routing_key = sys.argv[1] if len(sys.argv) &gt; 1 else &apos;anonymous.info&apos;message = &apos; &apos;.join(sys.argv[2:]) or &apos;Hello World!&apos;channel.basic_publish(exchange=&apos;topic_logs&apos;, routing_key=routing_key, body=message)print(&quot; [x] Sent %r:%r&quot; % (routing_key, message))connection.close() receive_logs_topic.py的代码： 12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;topic_logs&apos;, type=&apos;topic&apos;)result = channel.queue_declare(exclusive=True)queue_name = result.method.queuebinding_keys = sys.argv[1:]if not binding_keys: sys.stderr.write(&quot;Usage: %s [info] [warning] [error]\n&quot; % sys.argv[0]) #定义了三种接收消息方式info,warning,error sys.exit(1)for binding_key in binding_keys: channel.queue_bind(exchange=&apos;topic_logs&apos;, queue=queue_name, routing_key=binding_key)print(&apos; [*] Waiting for logs. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] %r:%r&quot; % (method.routing_key, body,))channel.basic_consume(callback, queue=queue_name, no_ack=True)channel.start_consuming() 执行下边命令 接收所有日志：python receive_logs_topic.py &quot;#&quot; 执行下边命令 接收来自”kern“设备的日志：python receive_logs_topic.py &quot;kern.*&quot; 执行下边命令 只接收严重程度为”critical“的日志：python receive_logs_topic.py &quot;*.critical&quot; 执行下边命令 建立多个绑定：python receive_logs_topic.py &quot;kern.*&quot; &quot;*.critical&quot; 执行下边命令 发送路由键为 “kern.critical” 的日志：python emit_log_topic.py &quot;kern.critical&quot; &quot;A critical kernel error&quot; 执行上边命令试试看效果吧。另外，上边代码不会对路由键和绑定键做任何假设，所以你可以在命令中使用超过两个路由键参数。 如果你现在还没被搞晕，想想下边问题: 绑定键为 * 的队列会取到一个路由键为空的消息吗？ 绑定键为 #.* 的队列会获取到一个名为..的路由键的消息吗？它会取到一个路由键为单个单词的消息吗？ a.*.# 和 a.#的区别在哪儿？ （完整代码参见emit_logs_topic.py and receive_logs_topic.py)]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版RabbitMQ消息队列之路由（四）]]></title>
    <url>%2Fpost%2F9d637a8e.html</url>
    <content type="text"><![CDATA[路由(Routing)在前面的教程中，我们实现了一个简单的日志系统。可以把日志消息广播给多个接收者。 本篇教程中我们打算新增一个功能 —— 使得它能够只订阅消息的一个字集。例如，我们只需要把严重的错误日志信息写入日志文件（存储到磁盘），但同时仍然把所有的日志信息输出到控制台中 绑定（Bindings）前面的例子，我们已经创建过绑定（bindings），代码如下： 1channel.queue_bind(exchange=exchange_name, queue=queue_name) 绑定（binding）是指交换机（exchange）和队列（queue）的关系。可以简单理解为：这个队列（queue）对这个交换机（exchange）的消息感兴趣。 绑定的时候可以带上一个额外的routing_key参数。为了避免与basic_publish的参数混淆，我们把它叫做绑定键（binding key）。以下是如何创建一个带绑定键的绑定。 1channel.queue_bind(exchange=exchange_name, queue=queue_name, routing_key=&apos;black&apos;) 绑定键的意义取决于交换机（exchange）的类型。我们之前使用过的扇型交换机（fanout exchanges）会忽略这个值。 直连交换机（Direct exchange）我们的日志系统广播所有的消息给所有的消费者（consumers）。我们打算扩展它，使其基于日志的严重程度进行消息过滤。例如我们也许只是希望将比较严重的错误（error）日志写入磁盘，以免在警告（warning）或者信息（info）日志上浪费磁盘空间。 我们使用的扇型交换机（fanout exchange）没有足够的灵活性 —— 它能做的仅仅是广播。 我们将会使用直连交换机（direct exchange）来代替。路由的算法很简单 —— 交换机将会对绑定键（binding key）和路由键（routing key）进行精确匹配，从而确定消息该分发到哪个队列。 下图能够很好的描述这个场景： 在这个场景中，我们可以看到直连交换机 X和两个队列进行了绑定。第一个队列使用orange作为绑定键，第二个队列有两个绑定，一个使用black作为绑定键，另外一个使用green。 这样以来，当路由键为orange的消息发布到交换机，就会被路由到队列Q1。路由键为black或者green的消息就会路由到Q2。其他的所有消息都将会被丢弃。 多个绑定（Multiple bindings） 多个队列使用相同的绑定键是合法的。这个例子中，我们可以添加一个X和Q1之间的绑定，使用black绑定键。这样一来，直连交换机就和扇型交换机的行为一样，会将消息广播到所有匹配的队列。带有black路由键的消息会同时发送到Q1和Q2。 发送日志我们将会发送消息到一个直连交换机，把日志级别作为路由键。这样接收日志的脚本就可以根据严重级别来选择它想要处理的日志。我们先看看发送日志。 我们需要创建一个交换机（exchange）： 1channel.exchange_declare(exchange=&apos;direct_logs&apos;, type=&apos;direct&apos;) 然后我们发送一则消息： 1channel.basic_publish(exchange=&apos;direct_logs&apos;, routing_key=severity, body=message) 我们先假设“severity”的值是info、warning、error中的一个。 订阅处理接收消息的方式和之前差不多，只有一个例外，我们将会为我们感兴趣的每个严重级别分别创建一个新的绑定。 12345result = channel.queue_declare(exclusive=True)queue_name = result.method.queuefor severity in severities: channel.queue_bind(exchange=&apos;direct_logs&apos;, queue=queue_name, routing_key=severity) 代码整合 emit_log_direct.py的代码： 1234567891011121314#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;direct_logs&apos;, type=&apos;direct&apos;)severity = sys.argv[1] if len(sys.argv) &gt; 1 else &apos;info&apos;message = &apos; &apos;.join(sys.argv[2:]) or &apos;Hello World!&apos;channel.basic_publish(exchange=&apos;direct_logs&apos;, routing_key=severity, body=message)print(&quot; [x] Sent %r:%r&quot; % (severity, message))connection.close() receive_logs_direct.py的代码： 1234567891011121314151617181920212223242526272829303132#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;direct_logs&apos;, type=&apos;direct&apos;)result = channel.queue_declare(exclusive=True)queue_name = result.method.queueseverities = sys.argv[1:]if not severities: sys.stderr.write(&quot;Usage: %s [info] [warning] [error]\n&quot; % sys.argv[0]) #定义了三种接收消息方式info,warning,error sys.exit(1)for severity in severities: channel.queue_bind(exchange=&apos;direct_logs&apos;, queue=queue_name, routing_key=severity)print(&apos; [*] Waiting for logs. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] %r:%r&quot; % (method.routing_key, body))channel.basic_consume(callback, queue=queue_name, no_ack=True)channel.start_consuming() 如果你希望只是保存warning和error级别的日志到磁盘，只需要打开控制台并输入： 1$ python receive_logs_direct.py warning error &gt; logs_from_rabbit.log 如果你希望所有的日志信息都输出到屏幕中，打开一个新的终端，然后输入： 12$ python receive_logs_direct.py info warning error [*] Waiting for logs. To exit press CTRL+C 如果要触发一个error级别的日志，只需要输入： 12$ python emit_log_direct.py error &quot;Run. Run. Or it will explode.&quot; [x] Sent &apos;error&apos;:&apos;Run. Run. Or it will explode.&apos; 这里是完整的代码：(emit_log_direct.py和receive_logs_direct.py)]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版RabbitMQ消息队列之发布／订阅（三）]]></title>
    <url>%2Fpost%2F2092e5cc.html</url>
    <content type="text"><![CDATA[发布／订阅在上篇教程中，我们搭建了一个工作队列，每个任务只分发给一个工作者（worker）。在本篇教程中，我们要做的跟之前完全不一样 —— 分发一个消息给多个消费者（consumers）。这种模式被称为“发布／订阅”。 为了描述这种模式，我们将会构建一个简单的日志系统。它包括两个程序——第一个程序负责发送日志消息，第二个程序负责获取消息并输出内容。 在我们的这个日志系统中，所有正在运行的接收方程序都会接受消息。我们用其中一个接收者（receiver）把日志写入硬盘中，另外一个接受者（receiver）把日志输出到屏幕上。 最终，日志消息被广播给所有的接受者（receivers）。 交换机（Exchanges）前面的教程中，我们发送消息到队列并从中取出消息。现在是时候介绍RabbitMQ中完整的消息模型了。 让我们简单的概括一下之前的教程： 发布者（producer）是发布消息的应用程序。 队列（queue）用于消息存储的缓冲。 消费者（consumer）是接收消息的应用程序。 RabbitMQ消息模型的核心理念是：发布者（producer）不会直接发送任何消息给队列。事实上，发布者（producer）甚至不知道消息是否已经被投递到队列。 发布者（producer）只需要把消息发送给一个交换机（exchange）。交换机非常简单，它一边从发布者方接收消息，一边把消息推送到队列。交换机必须知道如何处理它接收到的消息，是应该推送到指定的队列还是是多个队列，或者是直接忽略消息。这些规则是通过交换机类型（exchange type）来定义的。 有几个可供选择的交换机类型：直连交换机（direct）, 主题交换机（topic）, （头交换机）headers和 扇型交换机（fanout）。我们在这里主要说明最后一个 —— 扇型交换机（fanout）。先创建一个fanout类型的交换机，命名为logs： 1channel.exchange_declare(exchange=&apos;logs&apos;, type=&apos;fanout&apos;) 扇型交换机（fanout）很简单，你可能从名字上就能猜测出来，它把消息发送给它所知道的所有队列。这正是我们的日志系统所需要的。 交换器列表rabbitmqctl能够列出服务器上所有的交换器： 123456789&gt; $ sudo rabbitmqctl list_exchanges&gt; Listing exchanges ...&gt; logs fanout&gt; amq.direct direct&gt; amq.topic topic&gt; amq.fanout fanout&gt; amq.headers headers&gt; ...done.&gt; &gt; 这个列表中有一些叫做amq.*的交换器。这些都是默认创建的，不过这时候你还不需要使用他们。 匿名的交换器前面的教程中我们对交换机一无所知，但仍然能够发送消息到队列中。因为我们使用了命名为空字符串(“”)默认的交换机。 回想我们之前是如何发布一则消息： 12&gt; channel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;hello&apos;, body=message)&gt; &gt; exchange参数就是交换机的名称。空字符串代表默认或者匿名交换机：消息将会根据指定的routing_key分发到指定的队列。 现在，我们就可以发送消息到一个具名交换机了： 1channel.basic_publish(exchange=&apos;logs&apos;, routing_key=&apos;&apos;, body=message) 临时队列你还记得之前我们使用的队列名吗（ hello和task_queue）？给一个队列命名是很重要的——我们需要把工作者（workers）指向正确的队列。如果你打算在发布者（producers）和消费者（consumers）之间共享同队列的话，给队列命名是十分重要的。 但是这并不适用于我们的日志系统。我们打算接收所有的日志消息，而不仅仅是一小部分。我们关心的是最新的消息而不是旧的。为了解决这个问题，我们需要做两件事情。 首先，当我们连接上RabbitMQ的时候，我们需要一个全新的、空的队列。我们可以手动创建一个随机的队列名，或者让服务器为我们选择一个随机的队列名（推荐）。我们只需要在调用queue_declare方法的时候，不提供queue参数就可以了： 1result = channel.queue_declare() 这时候我们可以通过result.method.queue获得已经生成的随机队列名。它可能是这样子的：amq.gen-U0srCoW8TsaXjNh73pnVAw==。 第二步，当与消费者（consumer）断开连接的时候，这个队列应当被立即删除。exclusive标识符即可达到此目的。 1result = channel.queue_declare(exclusive=True) 绑定（Bindings） 我们已经创建了一个扇型交换机（fanout）和一个队列。现在我们需要告诉交换机如何发送消息给我们的队列。交换器和队列之间的联系我们称之为绑定（binding）。 1channel.queue_bind(exchange=&apos;logs&apos;, queue=result.method.queue) 现在，logs交换机将会把消息添加到我们的队列中。 绑定（binding）列表你可以使用rabbitmqctl list_bindings 列出所有现存的绑定。 代码整合 发布日志消息的程序看起来和之前的没有太大区别。最重要的改变就是我们把消息发送给logs交换机而不是匿名交换机。在发送的时候我们需要提供routing_key参数，但是它的值会被扇型交换机（fanout exchange）忽略。以下是emit_log.py脚本： 1234567891011121314151617#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;logs&apos;, type=&apos;fanout&apos;)message = &apos; &apos;.join(sys.argv[1:]) or &quot;info: Hello World!&quot;channel.basic_publish(exchange=&apos;logs&apos;, routing_key=&apos;&apos;, body=message)print(&quot; [x] Sent %r&quot; % (message,))connection.close() (emit_log.py 源文件) 正如你看到的那样，在连接成功之后，我们声明了一个交换器，这一个是很重要的，因为不允许发布消息到不存在的交换器。 如果没有绑定队列到交换器，消息将会丢失。但这个没有所谓，如果没有消费者监听，那么消息就会被忽略。 receive_logs.py的代码： 1234567891011121314151617181920212223242526#!/usr/bin/env pythonimport pikaconnection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;))channel = connection.channel()channel.exchange_declare(exchange=&apos;logs&apos;, type=&apos;fanout&apos;)result = channel.queue_declare(exclusive=True)queue_name = result.method.queuechannel.queue_bind(exchange=&apos;logs&apos;, queue=queue_name)print(&apos; [*] Waiting for logs. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] %r&quot; % (body,))channel.basic_consume(callback, queue=queue_name, no_ack=True)channel.start_consuming() (receive_logs.py source) 这样我们就完成了。如果你想把日志保存到文件中，只需要打开控制台输入： 1$ python receive_logs.py &gt; logs_from_rabbit.log 如果你想在屏幕中查看日志，那么打开一个新的终端然后运行： 1$ python receive_logs.py 当然还要发送日志： 1$ python emit_log.py 使用rabbitmqctl list_bindings你可确认已经创建的队列绑定。你可以看到运行中的两个receive_logs.py程序： 123456$ sudo rabbitmqctl list_bindingsListing bindings ... ...logs amq.gen-TJWkez28YpImbWdRKMa8sg== []logs amq.gen-x0kymA4yPzAT6BoC/YP+zw== []...done. 显示结果很直观：logs交换器把数据发送给两个系统命名的队列。这就是我们所期望的。]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版RabbitMQ消息队列之工作队列（二）]]></title>
    <url>%2Fpost%2F2efe5f21.html</url>
    <content type="text"><![CDATA[工作队列 在第一篇教程中，我们已经写了一个从已知队列中发送和获取消息的程序。在这篇教程中，我们将创建一个工作队列（Work Queue），它会发送一些耗时的任务给多个工作者（Worker）。 工作队列（又称：任务队列——Task Queues）是为了避免等待一些占用大量资源、时间的操作。当我们把任务（Task）当作消息发送到队列中，一个运行在后台的工作者（worker）进程就会取出任务然后处理。当你运行多个工作者（workers），任务就会在它们之间共享。 这个概念在网络应用中是非常有用的，它可以在短暂的HTTP请求中处理一些复杂的任务。 准备之前的教程中，我们发送了一个包含“Hello World!”的字符串消息。现在，我们将发送一些字符串，把这些字符串当作复杂的任务。我们没有真实的例子，例如图片缩放、pdf文件转换。所以使用time.sleep()函数来模拟这种情况。我们在字符串中加上点号（.）来表示任务的复杂程度，一个点（.）将会耗时1秒钟。比如”Hello…”就会耗时3秒钟。 我们对之前教程的send.py做些简单的调整，以便可以发送随意的消息。这个程序会按照计划发送任务到我们的工作队列中。我们把它命名为new_task.py： 1234567import sysmessage = &apos; &apos;.join(sys.argv[1:]) or &quot;Hello World!&quot;channel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;hello&apos;, body=message)print(&quot; [x] Sent %r&quot; % (message,)) 我们的旧脚本（receive.py）同样需要做一些改动：它需要为消息体中每一个点号（.）模拟1秒钟的操作。它会从队列中获取消息并执行，我们把它命名为worker.py： 123456import timedef callback(ch, method, properties, body): print(&quot; [x] Received %r&quot; % (body,)) time.sleep( body.count(&apos;.&apos;) ) print(&quot; [x] Done&quot;) 循环调度:使用工作队列的一个好处就是它能够并行的处理队列。如果堆积了很多任务，我们只需要添加更多的工作者（workers）就可以了，扩展很简单。 首先，我们先同时运行两个worker.py脚本，它们都会从队列中获取消息，到底是不是这样呢？我们看看。 你需要打开三个终端，两个用来运行worker.py脚本，这两个终端就是我们的两个消费者（consumers）—— C1 和 C2。 12shell1$ python worker.py [*] Waiting for messages. To exit press CTRL+C 12shell2$ python worker.py [*] Waiting for messages. To exit press CTRL+C 第三个终端，我们用来发布新任务。你可以发送一些消息给消费者（consumers）： 12345shell3$ python new_task.py First message.shell3$ python new_task.py Second message..shell3$ python new_task.py Third message...shell3$ python new_task.py Fourth message....shell3$ python new_task.py Fifth message..... 看看到底发送了什么给我们的工作者（workers）： 12345shell1$ python worker.py [*] Waiting for messages. To exit press CTRL+C [x] Received &apos;First message.&apos; [x] Received &apos;Third message...&apos; [x] Received &apos;Fifth message.....&apos; 1234shell2$ python worker.py [*] Waiting for messages. To exit press CTRL+C [x] Received &apos;Second message..&apos; [x] Received &apos;Fourth message....&apos; 默认来说，RabbitMQ会按顺序得把消息发送给每个消费者（consumer）。平均每个消费者都会收到同等数量得消息。这种发送消息得方式叫做——轮询（round-robin）。试着添加三个或更多得工作者（workers）。 消息确认当处理一个比较耗时得任务的时候，你也许想知道消费者（consumers）是否运行到一半就挂掉。当前的代码中，当消息被RabbitMQ发送给消费者（consumers）之后，马上就会在内存中移除。这种情况，你只要把一个工作者（worker）停止，正在处理的消息就会丢失。同时，所有发送到这个工作者的还没有处理的消息都会丢失。 我们不想丢失任何任务消息。如果一个工作者（worker）挂掉了，我们希望任务会重新发送给其他的工作者（worker）。 为了防止消息丢失，RabbitMQ提供了消息响应（acknowledgments）。消费者会通过一个ack（响应），告诉RabbitMQ已经收到并处理了某条消息，然后RabbitMQ就会释放并删除这条消息。 如果消费者（consumer）挂掉了，没有发送响应，RabbitMQ就会认为消息没有被完全处理，然后重新发送给其他消费者（consumer）。这样，及时工作者（workers）偶尔的挂掉，也不会丢失消息。 消息是没有超时这个概念的；当工作者与它断开连的时候，RabbitMQ会重新发送消息。这样在处理一个耗时非常长的消息任务的时候就不会出问题了。 消息响应默认是开启的。之前的例子中我们可以使用no_ack=True标识把它关闭。是时候移除这个标识了，当工作者（worker）完成了任务，就发送一个响应。 1234567def callback(ch, method, properties, body): print(&quot; [x] Received %r&quot; % (body,)) time.sleep( body.count(&apos;.&apos;) ) print(&quot; [x] Done&quot;) ch.basic_ack(delivery_tag = method.delivery_tag)channel.basic_consume(callback, queue=&apos;hello&apos;) 运行上面的代码，我们发现即使使用CTRL+C杀掉了一个工作者（worker）进程，消息也不会丢失。当工作者（worker）挂掉这后，所有没有响应的消息都会重新发送。 忘记确认一个很容易犯的错误就是忘了basic_ack，后果很严重。消息在你的程序退出之后就会重新发送，如果它不能够释放没响应的消息，RabbitMQ就会占用越来越多的内存。 为了排除这种错误，你可以使用rabbitmqctl命令，输出messages_unacknowledged字段： 12345&gt; $ sudo rabbitmqctl list_queues name messages_ready messages_unacknowledged&gt; Listing queues ...&gt; hello 0 0&gt; ...done.&gt; 消息持久化如果你没有特意告诉RabbitMQ，那么在它退出或者崩溃的时候，将会丢失所有队列和消息。为了确保信息不会丢失，有两个事情是需要注意的：我们必须把“队列”和“消息”设为持久化。 首先，为了不让队列消失，需要把队列声明为持久化（durable）： 1channel.queue_declare(queue=&apos;hello&apos;, durable=True) 尽管这行代码本身是正确的，但是仍然不会正确运行。因为我们已经定义过一个叫hello的非持久化队列。RabbitMq不允许你使用不同的参数重新定义一个队列，它会返回一个错误。但我们现在使用一个快捷的解决方法——用不同的名字，例如task_queue。 1channel.queue_declare(queue=&apos;task_queue&apos;, durable=True) 这个queue_declare必须在生产者（producer）和消费者（consumer）对应的代码中修改。 这时候，我们就可以确保在RabbitMq重启之后queue_declare队列不会丢失。另外，我们需要把我们的消息也要设为持久化——将delivery_mode的属性设为2。 123456channel.basic_publish(exchange=&apos;&apos;, routing_key=&quot;task_queue&quot;, body=message, properties=pika.BasicProperties( delivery_mode = 2, # make message persistent )) 注意：消息持久化将消息设为持久化并不能完全保证不会丢失。以上代码只是告诉了RabbitMq要把消息存到硬盘，但从RabbitMq收到消息到保存之间还是有一个很小的间隔时间。因为RabbitMq并不是所有的消息都使用fsync(2)——它有可能只是保存到缓存中，并不一定会写到硬盘中。并不能保证真正的持久化，但已经足够应付我们的简单工作队列。如果你一定要保证持久化，你需要改写你的代码来支持事务（transaction）。 公平调度你应该已经发现，它仍旧没有按照我们期望的那样进行分发。比如有两个工作者（workers），处理奇数消息的比较繁忙，处理偶数消息的比较轻松。然而RabbitMQ并不知道这些，它仍然一如既往的派发消息。 这时因为RabbitMQ只管分发进入队列的消息，不会关心有多少消费者（consumer）没有作出响应。它盲目的把第n-th条消息发给第n-th个消费者。 我们可以使用basic.qos方法，并设置prefetch_count=1。这样是告诉RabbitMQ，再同一时刻，不要发送超过1条消息给一个工作者（worker），直到它已经处理了上一条消息并且作出了响应。这样，RabbitMQ就会把消息分发给下一个空闲的工作者（worker）。 1channel.basic_qos(prefetch_count=1) 关于队列大小如果所有的工作者都处理繁忙状态，你的队列就会被填满。你需要留意这个问题，要么添加更多的工作者（workers），要么使用其他策略。 整合代码new_task.py的完整代码： 12345678910111213141516171819#!/usr/bin/env pythonimport pikaimport sysconnection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;task_queue&apos;, durable=True)message = &apos; &apos;.join(sys.argv[1:]) or &quot;Hello World!&quot;channel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;task_queue&apos;, body=message, properties=pika.BasicProperties( delivery_mode = 2, # make message persistent ))print(&quot; [x] Sent %r&quot; % (message,))connection.close() (new_task.py源码) 我们的worker： 12345678910111213141516171819202122#!/usr/bin/env pythonimport pikaimport timeconnection = pika.BlockingConnection(pika.ConnectionParameters( host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;task_queue&apos;, durable=True)print(&apos; [*] Waiting for messages. To exit press CTRL+C&apos;)def callback(ch, method, properties, body): print(&quot; [x] Received %r&quot; % (body,)) time.sleep( body.count(&apos;.&apos;) ) print(&quot; [x] Done&quot;) ch.basic_ack(delivery_tag = method.delivery_tag)channel.basic_qos(prefetch_count=1)channel.basic_consume(callback, queue=&apos;task_queue&apos;)channel.start_consuming() (worker.py source) 使用消息响应和prefetch_count你就可以搭建起一个工作队列了。这些持久化的选项使得在RabbitMQ重启之后仍然能够恢复。]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python版RabbitMQ消息队列之Hello World（一）]]></title>
    <url>%2Fpost%2F79b12668.html</url>
    <content type="text"><![CDATA[RabbitMQ安装请移步努力哥博客！！！ RabbitMQ队列RabbitMQ是一个消息代理。它的工作就是接收和转发消息。你可以把它想像成一个邮局：你把信件放入邮箱，邮递员就会把信件投递到你的收件人处。在这个比喻中，RabbitMQ就扮演着邮箱、邮局以及邮递员的角色。 RabbitMQ和邮局的主要区别在于，它处理纸张，而是接收、存储和发送消息（message）这种二进制数据。 下面是RabbitMQ和消息所涉及到的一些术语。 生产(Producing)的意思就是发送。发送消息的程序就是一个生产者(producer)。我们一般用”P”来表示: 队列(queue)就是存在于RabbitMQ中邮箱的名称。虽然消息的传输经过了RabbitMQ和你的应用程序，但是它只能被存储于队列当中。实质上队列就是个巨大的消息缓冲区，它的大小只受主机内存和硬盘限制。多个生产者（producers）可以把消息发送给同一个队列，同样，多个消费者（consumers）也能够从同一个队列（queue）中获取数据。队列可以绘制成这样（图上是队列的名称）： 在这里，消费（Consuming）和接收(receiving)是同一个意思。一个消费者（consumer）就是一个等待获取消息的程序。我们把它绘制为”C”： 需要指出的是生产者、消费者、代理需不要待在同一个设备上；事实上大多数应用也确实不在会将他们放在一台机器上。 Hello World接下来我们用Python写两个小程序。一个发送单条消息的生产者（producer）和一个接收消息并将其输出的消费者（consumer）。传递的消息是”Hello World”。 下图中，“P”代表生产者，“C”代表消费者，中间的盒子代表为消费者保留的消息缓冲区，也就是我们的队列。 生产者（producer）把消息发送到一个名为“hello”的队列中。消费者（consumer）从这个队列中获取消息。 RabbitMQ库RabbitMQ使用的是AMQP 0.9.1协议。这是一个用于消息传递的开放、通用的协议。针对不同编程语言有大量的RabbitMQ客户端可用。在这个系列教程中，RabbitMQ团队推荐使用Pika这个Python客户端。大家可以通过pip这个包管理工具进行安装： 发送 我们第一个程序send.py会发送一个消息到队列中。首先要做的事情就是建立一个到RabbitMQ服务器的连接。 1234import pikaconnection = pika.BlockingConnection(pika.ConnectionParameters(&apos;localhost&apos;))channel = connection.channel() 现在我们已经跟本地机器的代理建立了连接。如果你想连接到其他机器的代理上，需要把代表本地的localhost改为指定的名字或IP地址。 接下来，在发送消息之前，我们需要确认服务于消费者的队列已经存在。如果将消息发送给一个不存在的队列，RabbitMQ会将消息丢弃掉。下面我们创建一个名为”hello”的队列用来将消息投递进去。 1channel.queue_declare(queue=&apos;hello&apos;) 这时候我们就可以发送消息了，我们第一条消息只包含了Hello World!字符串，我们打算把它发送到hello队列。 在RabbitMQ中，消息是不能直接发送到队列中的，这个过程需要通过交换机（exchange）来进行。但是为了不让细节拖累我们的进度，这里我们只需要知道如何使用由空字符串表示的默认交换机即可。如果你想要详细了解交换机，可以查看我们教程的第三部分来获取更多细节。默认交换机比较特别，它允许我们指定消息究竟需要投递到哪个具体的队列中，队列名字需要在routing_key参数中指定。 12channel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;hello&apos;, body=&apos;Hello World!&apos;)print(&quot; [x] Sent &apos;Hello World!&apos;&quot;) 在退出程序之前，我们需要确认网络缓冲已经被刷写、消息已经投递到RabbitMQ。通过安全关闭连接可以做到这一点。 1connection.close() 发送不成功！ 如果这是你第一次使用RabbitMQ，并且没有看到“Sent”消息出现在屏幕上，你可能会抓耳挠腮不知所以。这也许是因为没有足够的磁盘空间给代理使用所造成的（代理默认需要200MB的空闲空间），所以它才会拒绝接收消息。查看一下代理的日志文件进行确认，如果需要的话也可以减少限制。配置文件文档会告诉你如何更改磁盘空间限制（disk_free_limit）。 接收 我们的第二个程序receive.py，将会从队列中获取消息并将其打印到屏幕上。 这次我们还是需要要先连接到RabbitMQ服务器。连接服务器的代码和之前是一样的。 下一步也和之前一样，我们需要确认队列是存在的。我们可以多次使用queue_declare命令来创建同一个队列，但是只有一个队列会被真正的创建。 1channel.queue_declare(queue=&apos;hello&apos;) 你也许要问: 为什么要重复声明队列呢 —— 我们已经在前面的代码中声明过它了。如果我们确定了队列是已经存在的，那么我们可以不这么做，比如此前预先运行了send.py程序。可是我们并不确定哪个程序会首先运行。这种情况下，在程序中重复将队列重复声明一下是种值得推荐的做法。 列出所有队列你也许希望查看RabbitMQ中有哪些队列、有多少消息在队列中。此时你可以使用rabbitmqctl工具（使用有权限的用户）： 12&gt; sudo rabbitmqctl list_queues&gt; （在Windows中不需要sudo命令） 12&gt; rabbitmqctl list_queues&gt; 从队列中获取消息相对来说稍显复杂。需要为队列定义一个回调（callback）函数。当我们获取到消息的时候，Pika库就会调用此回调函数。这个回调函数会将接收到的消息内容输出到屏幕上。 12def callback(ch, method, properties, body): print(&quot; [x] Received %r&quot; % body) 下一步，我们需要告诉RabbitMQ这个回调函数将会从名为”hello”的队列中接收消息： 1channel.basic_consume(callback, queue=&apos;hello&apos;, no_ack=True) 要成功运行这些命令，我们必须保证队列是存在的，我们的确可以确保它的存在——因为我们之前已经使用queue_declare将其声明过了。 no_ack参数稍后会进行介绍。 最后，我们运行一个用来等待消息数据并且在需要的时候运行回调函数的无限循环。 12print(&apos; [*] Waiting for messages. To exit press CTRL+C&apos;)channel.start_consuming() 将代码整合到一起send.py的完整代码： 12345678910111213import pikaconnection =pika.BlockingConnection(pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;hello&apos;)channel.basic_publish(exchange=&apos;&apos;, routing_key=&apos;hello&apos;, body=&apos;Hello World!&apos;)print(&quot; [x] Sent &apos;Hello World!&apos;&quot;)connection.close() (send.py源码) receive.py的完整代码： 1234567891011121314151617import pikaconnection =pika.BlockingConnection(pika.ConnectionParameters(host=&apos;localhost&apos;))channel = connection.channel()channel.queue_declare(queue=&apos;hello&apos;)def callback(ch, method, properties, body): print(&quot; [x] Received %r&quot; % body)channel.basic_consume(callback, queue=&apos;hello&apos;, no_ack=True)print(&apos; [*] Waiting for messages. To exit press CTRL+C&apos;)channel.start_consuming() (receive.py源码) 现在我们可以在终端中尝试一下我们的程序了。首先我们启动一个消费者，它会持续的运行来等待投递到达。 123python receive.py# =&gt; [*] Waiting for messages. To exit press CTRL+C# =&gt; [x] Received &apos;Hello World!&apos; 然后启动生产者，生产者程序每次执行后都会停止运行。 12python send.py# =&gt; [x] Sent &apos;Hello World!&apos; 成功了！我们已经通过RabbitMQ发送第一条消息。你也许已经注意到了，receive.py程序并没有退出。它一直在准备获取消息，你可以通过Ctrl-C来中止它。 试下在新的终端中再次运行send.py。]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之concurrent.futures]]></title>
    <url>%2Fpost%2Fb1bc006d.html</url>
    <content type="text"><![CDATA[concurrent.futuresProcessPoolExecutor进程池，提供异步调用 基本方法 submit(fn, args, *kwargs) 异步提交任务 map(func, *iterables, timeout=None, chunksize=1) 取代for循环submit的操作 shutdown(wait=True) 相当于进程池的pool.close()+pool.join()操作 wait=True，等待池内所有任务执行完毕回收完资源后才继续wait=False，立即返回，并不会等待池内的任务执行完毕但不管wait参数为何值，整个程序都会等到所有任务执行完毕submit和map必须在shutdown之前 result(timeout=None)取得结果 add_done_callback(fn)回调函数 1234567891011121314151617181920212223from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutorimport os, time, randomdef task(n): print('%s is runing' % os.getpid()) time.sleep(random.randint(1, 3)) return n ** 2if __name__ == '__main__': executor = ProcessPoolExecutor(max_workers=3) futures = [] for i in range(11): future = executor.submit(task, i) futures.append(future) executor.shutdown(True) print('+++&gt;') for future in futures: print(future.result()) ThreadPoolExecutor线程池，提供异步调用，用法与ProcessPoolExecutor相同，只需要将ProcessPoolExecutor换成ThreadPoolExecutor 123456789101112131415161718import timefrom concurrent.futures import ThreadPoolExecutordef func(n): time.sleep(2) print(n) return n * ndef call_back(m): print('结果是 %s' % m.result())if __name__ == '__main__': tpool = ThreadPoolExecutor(max_workers=5) # 默认 不要超过cpu个数*5 for i in range(20): tpool.submit(func, i).add_done_callback(call_back)]]></content>
      <categories>
        <category>Python线程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>进程池</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python线程——信号量（Semaphore）、队列（Queue）]]></title>
    <url>%2Fpost%2Fd068a68c.html</url>
    <content type="text"><![CDATA[信号量（Semaphore）之前讲的线程锁（互斥锁）同时只允许一个线程更改数据，而Semaphore是同时允许一定数量的线程更改数据 ，比如厕所有4个坑，那最多只允许4个人上厕所，后面的人只能等里面有人出来了才能再进去。 说白了就是在同一时间，可以只允许设定的数量的线程去执行 。 123456789101112131415import timefrom threading import Semaphore, Threaddef func(sem, a, b): sem.acquire() time.sleep(1) print(a + b) sem.release()sem = Semaphore(4)for i in range(10): t = Thread(target=func, args=(sem, i, i + 5)) t.start() 队列（Queue）queue队列 ：使用import queue，用法与进程Queue一样 ####class queue.Queue(maxsize=0) 先进先出 12345678910111213141516import queueq=queue.Queue()q.put('first')q.put('second')q.put('third')print(q.get())print(q.get())print(q.get())# 执行结果结果(先进先出):firstsecondthird class queue.LifoQueue(maxsize=0) 后进先出12345678910111213141516import queueq=queue.LifoQueue()q.put('first')q.put('second')q.put('third')print(q.get())print(q.get())print(q.get())# 执行结果结果(后进先出):thirdsecondfirst class queue.PriorityQueue(maxsize=0)根据优先级来取数据。存放数据的格式 : Queue.put((priority_number,data))，priority_number越小，优先级越高，data代表存入的值 12345678910111213import queueq = queue.PriorityQueue()q.put((1, "d1"))q.put((-1, "d2"))q.put((6, "d3"))print(q.get())print(q.get())print(q.get()) #执行结果(-1, 'd2')(1, 'd1')(6, 'd3') 注意：maxsize代表这个队列最大能够put的长度]]></content>
      <categories>
        <category>Python线程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>线程</tag>
        <tag>信号量</tag>
        <tag>队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python线程——GIL锁、线程锁（互斥锁）、递归锁（RLock）]]></title>
    <url>%2Fpost%2F5b18ac8.html</url>
    <content type="text"><![CDATA[GIL锁​ 计算机有4核，代表着同一时间，可以干4个任务。如果单核cpu的话，我启动10个线程，我看上去也是并发的，因为是执行了上下文的切换，让看上去是并发的。但是单核永远肯定时串行的，它肯定是串行的，cpu真正执行的时候，因为一会执行1，一会执行2.。。。。正常的线程就是这个样子的。但是，在python中，无论有多少核，永远都是假象。无论是4核，8核，还是16核…….不好意思，同一时间执行的线程只有一个(线程)，它就是这个样子的。 全局解释器锁(GIL)​ Python代码的执行由Python虚拟机(也叫解释器主循环)来控制。Python在设计之初就考虑到要在主循环中，同时只有一个线程在执行。虽然 Python 解释器中可以“运行”多个线程，但在任意时刻只有一个线程在解释器中运行。 对Python虚拟机的访问由全局解释器锁(GIL)来控制，正是这个锁能保证同一时刻只有一个线程在运行。 在多线程环境中，Python 虚拟机按以下方式执行： 1、设置 GIL； 2、切换到一个线程去运行； 3、运行指定数量的字节码指令或者线程主动让出控制(可以调用 time.sleep(0))； 4、把线程设置为睡眠状态； 5、解锁 GIL； 6、再次重复以上所有步骤。 在调用外部代码(如 C/C++扩展函数)的时候，GIL将会被锁定，直到这个函数结束为止(由于在这期间没有Python的字节码被运行，所以不会做线程切换)编写扩展的程序员可以主动解锁GIL。 GIL锁关系图GIL(全局解释器锁)是加在python解释器里面的，效果如图 : 为什么GIL锁要加在python解释器这一层，而却不加在其他地方？​ 很多资料说是因为python调用的所有线程都是原生线程。原生线程是通过C语言提供原生接口，相当于C语言的一个函数。你一调它，你就控制不了了它了，就必须等它给你返回结果。只要已通过python虚拟机，再往下就不受python控制了，就是C语言自己控制了。你加在python虚拟机以下，你是加不上去的。同一时间，只有一个线程穿过这个锁去真正执行。其他的线程，只能在python虚拟机这边等待。 总结​ 需要明确的一点是GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。就好比C++是一套语言（语法）标准，但是可以用不同的编译器来编译成可执行代码。有名的编译器例如GCC，INTEL C++，Visual C++等。Python也一样，同样一段代码可以通过CPython，JPython，PyPy，Psyco等不同的Python执行环境来执行。而JPython就没有GIL。然而因为CPython是大部分环境下默认的Python执行环境。所以在很多人的概念里CPython就是Python，也就想当然的把GIL归结为Python语言的缺陷。所以这里要先明确一点：GIL并不是Python的特性，Python完全可以不依赖于GIL。 线程锁（互斥锁）12345678910111213141516171819202122232425262728from threading import Threadimport timedef work(): # 在这里模拟一个底层做运算的过程，具体为什么这个我不知道，看某资料写的， # 如果这里直接对num进行运算很难出现数据不安全的结果 global num # 把num变成全局变量 temp = num time.sleep(1) # 注意了sleep的时候是不占有cpu的，这个时候cpu直接把这个线程挂起了，此时cpu去干别的事情去了 num = temp + 1 # 所有的线程都做+1操作if __name__ == '__main__': num = 0 # 初始化num为0 t_obj = list() for i in range(100): t = Thread(target=work) t.start() t_obj.append(t) for t in t_obj: t.join() print("num:", num) # 输出最后的num值，可能是1 # 执行结果 num: 1 下面我们就用一张图来解释一下这个原因 ： 上面的例子中出现数据不安全问题，那么我们应该怎么解决呢？在这里我们引用线程锁来解决这个问题 12345678910111213141516171819202122232425262728293031from threading import Lock,Threadimport timedef work(lock): # 在这里模拟一个底层做运算的过程，具体为什么这个我不知道，看某资料写的， # 如果这里直接对num进行运算很难出现数据不安全的结果 global num # 把num变成全局变量 lock.acquire() temp = num time.sleep(0.1) # 注意了sleep的时候是不占有cpu的，这个时候cpu直接把这个线程挂起了，此时cpu去干别的事情去了 num = temp + 1 # 所有的线程都做+1操作 lock.release()if __name__ == '__main__': num = 0 # 初始化num为0 t_obj = list() lock = Lock() for i in range(100): t = Thread(target=work, args=(lock,)) t.start() t_obj.append(t) for t in t_obj: t.join() print("num:", num) # 输出最后的num值 # 执行结果 num: 100 注意：这里的Lock创建的锁和GIL没有关系 ， 递归锁（RLock）线程死锁​ 所谓死锁： 是指两个或两个以上的进程或线程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程，比如下面例子中“科学家吃面”： 12345678910111213141516171819202122232425262728293031323334353637383940from threading import Lock, Threadimport timenoodle_lock = Lock()fork_lock = Lock()def eat1(name): noodle_lock.acquire() print('%s拿到面条啦' % name) fork_lock.acquire() print('%s拿到叉子了' % name) print('%s吃面' % name) fork_lock.release() noodle_lock.release()def eat2(name): fork_lock.acquire() print('%s拿到叉子了' % name) time.sleep(1) noodle_lock.acquire() print('%s拿到面条啦' % name) print('%s吃面' % name) noodle_lock.release() fork_lock.release()if __name__ == '__main__': Thread(target=eat1, args=('张三',)).start() Thread(target=eat2, args=('李四',)).start() Thread(target=eat1, args=('王五',)).start() Thread(target=eat2, args=('赵六',)).start()# 执行结果张三拿到面条啦张三拿到叉子了张三吃面李四拿到叉子了王五拿到面条啦 上面例子中情况是在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源，就会造成死锁，因为系统判断这部分资源都正在使用，所有这两个线程在无外力作用下将一直等待下去。执行结果，是无限的进入死循环，所以不能这么加，这个时候就需要用到递归锁。 递归锁(RLock)1234567891011121314151617181920212223242526272829303132from threading import Thread, RLock # 递归锁import timefork_lock = noodle_lock = RLock() # 一个钥匙串上的两把钥匙def eat1(name): noodle_lock.acquire() # 一把钥匙 print('%s拿到面条啦' % name) fork_lock.acquire() print('%s拿到叉子了' % name) print('%s吃面' % name) fork_lock.release() noodle_lock.release()def eat2(name): fork_lock.acquire() print('%s拿到叉子了' % name) time.sleep(1) noodle_lock.acquire() print('%s拿到面条啦' % name) print('%s吃面' % name) noodle_lock.release() fork_lock.release()if __name__ == '__main__': Thread(target=eat1, args=('张三',)).start() Thread(target=eat2, args=('李四',)).start() Thread(target=eat1, args=('王五',)).start() Thread(target=eat2, args=('赵六',)).start() 自我理解递归锁原理其实很简单：就是每开一把门，在字典里面存一份数据，退出的时候去到door1或者door2里面找到这个钥匙退出 注意：递归锁用于多重锁的情况，如果只是一层锁，就用不上递归锁]]></content>
      <categories>
        <category>Python线程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进程——守护线程]]></title>
    <url>%2Fpost%2F5965383b.html</url>
    <content type="text"><![CDATA[守护进程在上一遍文章中我们已经介绍了创建线程，对线程也有一些了解，现在一起来看看守护进程。 无论是进程还是线程，都遵循：守护xx会等待主xx运行完毕后被销毁。需要强调的是：运行完毕并非终止运行 12- 主进程在其代码结束后就已经算运行完毕了（守护进程在此时就被回收）,然后主进程会一直等非守护的子进程都运行完毕后回收子进程的资源(否则会产生僵尸进程)，才会结束。- 主线程在其他非守护线程运行完毕后才算运行完毕（守护线程在此时就被回收）。因为主线程的结束意味着进程的结束，进程整体的资源都将被回收，而进程必须保证非守护线程都运行完毕后才能结束。 举个例，设置一个主人，在设置几个仆人，这几个仆人都是为主人服务的。可以帮主人做很多事情，一个主人（主线程）可以有多个仆人（守护线程），服务的前提是，主线程必须存在，如果主线程不存在，则守护进程也没了。那守护进程是干嘛的呢？可以管理一些资源，打开一些文件，监听一些端口，监听一些资源，把一些垃圾资源回收，可以干很多事情，可以随便定义。 守护线程设置123456789101112131415161718192021222324252627import threadingimport timedef run(n): print('task:', n) time.sleep(2) print('task done', n)start_time = time.time()for i in range(5): t = threading.Thread(target=run, args=('t-&#123;0&#125;'.format(i),)) t.setDaemon(True) # Daemon意思是守护进程，这边是把当前线程设置为守护线程 t.start()print("所有线程已经完成")print('cost:', time.time() - start_time)# 执行结果task: t-0task: t-1task: t-2task: t-3task: t-4所有线程已经完成cost: 0.0019915103912353516 注意：守护进程一定要在start之前设置，start之后就不能设置了，之后设置会报错，所以必须在start之前设置 统计线程数(补充点)12345678910111213141516import threading, timedef run(n): print('task:', n) time.sleep(2) print('task done', n, threading.current_thread()) # 查看每个子线程start_time = time.time()for i in range(5): t = threading.Thread(target=run, args=('t-&#123;0&#125;'.format(i),)) t.start()print('--------', threading.current_thread(), threading.active_count()) # 查看主线程和当前活动的所有线程数print('cost:', time.time() - start_time) 上面例子是查看当前线程和统计活动线程个数，用theading.current_thead() 查看当前线程；用theading.active_count()来统计当前活动的线程数，线程个数=子线程数+主线程数]]></content>
      <categories>
        <category>Python线程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>守护线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之线程]]></title>
    <url>%2Fpost%2Fc52e9d45.html</url>
    <content type="text"><![CDATA[线程 CPU上的执行单位，线程也是程序运行过程中的一个抽象。 一个进程下可以有多个线程。 主线程：操作系统中的每一个进程都会对应一个地址空间。每一个进程中都会默认有一个控制线程，主线程随着进程的创建而出现。所以一个进程中主线程存在就代表了这个进程的存在，当进程中的主线程结束的时候，操作系统就会将该进程回收 有了进程为什么还要线程?进程有很多优点，它提供了多道编程，让我们感觉我们每个人都拥有自己的CPU和其他资源，可以提高计算机的利用率。很多人就不理解了，既然进程这么优秀，为什么还要线程呢？其实，仔细观察就会发现进程还是有很多缺陷的，主要体现在两点上： 进程只能在一个时间干一件事，如果想同时干两件事或多件事，进程就无能为力了。 进程在执行的过程中如果阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。 例如，我们在使用qq聊天， qq做为一个独立进程如果同一时间只能干一件事，那他如何实现在同一时刻 即能监听键盘输入、又能监听其它人给你发的消息、同时还能把别人发的消息显示在屏幕上呢？你会说，操作系统不是有分时么？但是分时是指在不同进程间的分时， 即操作系统处理一会你的qq任务，又切换到word文档任务上了，每个cpu时间片分给你的qq程序时，事实上，你的qq还是同一时间只能干一件事情。 再直白一点， 一个操作系统就像是一个工厂，工厂里面有很多个生产车间，不同的车间生产不同的产品，每个车间就相当于一个进程，且你的工厂又穷，供电不足，同一时间只能给一个车间供电，为了能让所有车间都能同时生产，你的工厂的电工只能给不同的车间分时供电，但是轮到你的qq车间时，发现只有一个干活的工人，结果生产效率极低，为了解决这个问题，就需要多加几个工人，让几个人工人并行工作，这每个工人，就是线程！ 线程的特点在多线程的操作系统中，通常是在一个进程中包括多个线程，每个线程都是作为利用CPU的基本单位，是花费最小开销的实体。线程具有以下属性。 1）轻型实体 线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证独立运行的资源。 线程的实体包括程序、数据和TCB。线程是动态概念，它的动态特性由线程控制块TCB（Thread Control Block）描述。 ​ 2）独立调度和分派的基本单位 在多线程OS中，线程是能独立运行的基本单位，因而也是独立调度和分派的基本单位。由于线程很“轻”，故线程的切换非常迅速且开销小（在同一进程中的）。 ​ 3）共享进程资源 线程在同一进程中的各个线程，都可以共享该进程所拥有的资源，这首先表现在：所有线程都具有相同的进程id，这意味着，线程可以访问该进程的每一个内存资源；此外，还可以访问进程所拥有的已打开文件、定时器、信号量机构等。由于同一个进程内的线程共享内存和文件，所以线程之间互相通信不必调用内核。 4）可并发执行 在一个进程中的多个线程之间，可以并发执行，甚至允许在一个进程中所有线程都能并发执行；同样，不同进程中的线程也能并发执行，充分利用和发挥了处理机与外围设备并行工作的能力。 TCB包括以下信息：（1）线程状态。（2）当线程不运行时，被保存的现场资源。（3）一组执行堆栈。（4）存放每个线程的局部变量主存区。（5）访问同一个进程中的主存和其它资源。用于指示被执行指令序列的程序计数器、保留局部变量、少数状态参数和返回地址等的一组寄存器和堆栈。 进程和线程的区别 线程是共享内存空间的；进程的内存是独立的。 线程可以直接访问此进程中的数据部分；进程有他们独立拷贝自己父进程的数据部分，每个进程是独立的 同一进程的线程之间直接交流(直接交流涉及到数据共享，信息传递)；两个进程想通信，必须通过一个中间代理来实现。 创建一个新的线程很容易；创建新的进程需要对其父进程进行一次克隆。 一个线程可以控制和操作同一进程里的其他线程；但是进程只能操作子进程。 对主线程的修改，可能会影响到进程中其他线程的修改；对于一个父进程的修改不会影响其他子进程(只要不删除父进程即可) 疑问进程和线程那个运行快？ 它俩是没有可比性的，线程是寄生在进程中的，你问它俩谁快。说白了，就是问在问两个线程谁快。因为进程只是资源的集合，进程也是要起一个线程的，它俩没有可比性。 进程和线程那个启动快？ 答案是：线程快。因为进程相当于在修一个屋子。线程只是一下把一个来过来就行了。进程是一堆资源的集合。它要去内存里面申请空间，它要各种各样的东西去跟OS去申请。但是启动起来一运行，它俩是一样的，因为进程也是通过线程来运行的。 自己理解 线程是操作系统最小的调度单位，是一串指令的集合。 进程要操作CPU，必须先创建一个线程。 进程本身是不可以执行的，操作cpu是通过线程实现的，因为它是一堆执行，而进程是不具备执行概念的。就像一个屋子，屋子就是进程，但是屋子里面的每一个人就是线程，屋子就是内存空间。 单核CPU只能同时干一件事，但是为什么给我们的感觉是在干了很多件事？因为上线的切换，刚才也说了跟读书那个例子一样。因为CPU太快了，可以有N多次切换，其实它都是在排着队呐。 寄存器是存上下文关系的。 进程是通过PID来区分的，并不是通过进程名来区分的。进程里面的第一个线程就是主线程，父线程和子线程是相互独立的，只是父线程创建了子线程，父线程down了，子线程不会受到影响的。 主线程修改影响其他线程的行文，因为它们是共享数据的。 线程的创建Threading.Thread类线程的创建12345678910from threading import Threadimport timedef sayhi(name): time.sleep(2) print('%s say hello' %name)if __name__ == '__main__': t=Thread(target=sayhi,args=('egon',)) t.start() print('主线程') 123456789101112131415from threading import Threadimport timeclass Sayhi(Thread): def __init__(self,name): super().__init__() self.name=name def run(self): time.sleep(2) print('%s say hello' % self.name)if __name__ == '__main__': t = Sayhi('egon') t.start() print('主线程') 多线程与多进程pid的比较123456789101112131415161718192021from threading import Threadfrom multiprocessing import Processimport osdef work(): print('hello',os.getpid())if __name__ == '__main__': #part1:在主进程下开启多个线程,每个线程都跟主进程的pid一样 t1=Thread(target=work) t2=Thread(target=work) t1.start() t2.start() print('主线程/主进程pid',os.getpid()) #part2:开多个进程,每个进程都有不同的pid p1=Process(target=work) p2=Process(target=work) p1.start() p2.start() print('主线程/主进程pid',os.getpid()) 开启效率的较量123456789101112131415161718192021222324252627from threading import Threadfrom multiprocessing import Processimport osdef work(): print('hello')if __name__ == '__main__': #在主进程下开启线程 t=Thread(target=work) t.start() print('主线程/主进程') ''' 打印结果: hello 主线程/主进程 ''' #在主进程下开启子进程 t=Process(target=work) t.start() print('主线程/主进程') ''' 打印结果: 主线程/主进程 hello ''' 内存数据的共享问题1234567891011121314151617181920from threading import Threadfrom multiprocessing import Processimport osdef work(): global n n=0if __name__ == '__main__': # n=100 # p=Process(target=work) # p.start() # p.join() # print('主',n) #毫无疑问子进程p已经将自己的全局的n改成了0,但改的仅仅是它自己的,查看父进程的n仍然为100 n=1 t=Thread(target=work) t.start() t.join() print('主',n) #查看结果为0,因为同一进程内的线程之间共享进程内的数据 上面的例子中最多只启动了一个2个线程，还是用那种古老的方式t1,t2。要是一下子起10个或者100个线程，这种方式就不适用了，其实可以在启动线程的时候，把它加到循环里面去，并且来计算一下它的时间 ： 12345678910111213141516171819202122232425262728import threading,time def run(n): #这边的run方法的名字是自行定义的，跟继承式多线程不一样，那个是强制的 print("task:",n) time.sleep(2) print("task done",n) start_time = time.time() #开始时间for i in range(5): #一次性启动5个线程 t = threading.Thread(target=run,args=("t-&#123;0&#125;".format(i),)) t.start() print("--------all thead has finished")print("cost:",time.time()-start_time) #计算总耗时 #执行结果task: t-0task: t-1task: t-2task: t-3task: t-4--------all thead has finishedcost: 0.00096893310546875task done t-1task done t-2task done t-0task done t-4task done t-3 这里设置成启动5个线程，并且计算一下时间。这里有个疑问，为什么不启动1000个线程或者更多一点的线程？这是因为：计算机是4核的，它能干的事情，就是4个任务。启动的线程越多，就代表着要在这个很多线程之间进行上下文切换。相当于教室里有一本书，某个人只看了半页，因为cpu要确保每个人都能执行，也就是这本是要确保教室每个同学都能看到，那就相当于每个人看书的时间非常少。也就是说某个同学刚刚把这本书拿过来，一下子又被第二个人，第三个人拿走了。所以就导致所有的人都慢了，所以说如果线程启动1000就没有意义了，导致机器越来越慢，所以要适当设置 从上面的程序发现，就是我主线程没有等其他的子线程执行完毕，就直接往下执行了，这是为什么呢？而且这个计算的时间根本不是我们想要的时间，中间的sleep 2秒哪里去了? 其实一个程序至少有一个线程，那先往下走的，没有等的就是主线程，主线程启动了子线程之后，子线程就是独立的，跟主线程就没有关系了。主线程和它启动的子线程是并行关系，这就解释了为什么我的主线程启动子线程之后，没有等子线程，而继续往下走了。所以计算不出来线程总共耗时时间，因为程序已经不是串行的了。程序本身就是一个线程，就是主线程。如果要想测试这五个线程总共花了多长时间，就需要用到线程的join()方法 join 12345678910111213141516171819202122232425262728293031323334import threadingimport timeclass MyThread(threading.Thread): # 继承threading.Thread """继承式多线程""" def __init__(self, n, sleep_time): # 增加时间属性 threading.Thread.__init__(self) # 也可以写成这样super(MyThread,self).__init__() self.n = n self.sleep_time = sleep_time def run(self): # 重写run方法 print('run task', self.n) time.sleep(self.sleep_time) # 每个线程可以传入不不同的时间 print('task done,', self.n)t1 = MyThread('t1', 2) # 实例化t2 = MyThread('t2', 4)t1.start() # 启动一个多线程t2.start()t1.join() # 把t1.join()放在线程启动之后print("main thread.....") #执行结果run task t1run task t2task done, t1main thread.....task done, t2 注意：t1.join() 只等t1的结果，然后主线程继续往下走，因为t2需要等4秒，所以，最后打出来的是t2的执行结果。t1的结果到了，就立刻算结果。这边只计算了t1的结果，没有t2的结果 那我们怎么计算多个线程的执行时间呢？来我们一起看一下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import threadingimport timedef run(n): # 这边的run方法的名字是自行定义的，跟继承式多线程不一样，那个是强制的 print('task:', n) time.sleep(2) print('task done', n)start_time = time.time() # 开始时间t_obj = [] # 存放子线程实例for i in range(10): # 一次性启动10个线程 t = threading.Thread(target=run, args=('t-&#123;0&#125;'.format(i),)) t.start() t_obj.append(t) # 为了不阻塞后面线程的启动，不在这里join，先放到一个列表中for t in t_obj: # 循环线程实例列表，等待所有线程执行完毕 t.join()print('所有线程已经完成')print('cost:', time.time() - start_time) # 计算总耗时 # 执行结果task: t-0task: t-1task: t-2task: t-3task: t-4task: t-5task: t-6task: t-7task: t-8task: t-9task done t-0task done t-1task done t-2task done t-4task done t-3task done t-5task done t-6task done t-9task done t-7task done t-8所有线程已经完成cost: 2.007737159729004 上面的例子在不加join的时候，主线程和子线程完全是并行的，没有了依赖关系，主线程执行了，子线程也执行了。但是加了join之后，主线程依赖子线程执行完毕才往下走。 下面将介绍守护线程]]></content>
      <categories>
        <category>Python线程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Python操作Redis]]></title>
    <url>%2Fpost%2F9f312a12.html</url>
    <content type="text"><![CDATA[Redis安装和使用Window 下安装下载地址：https://github.com/MSOpenTech/redis/releases。 Redis 支持 32 位和 64 位。这个需要根据你系统平台的实际情况选择，这里我们下载 Redis-x64-xxx.zip压缩包到 C 盘，解压后，将文件夹重新命名为 redis。 打开一个 cmd 窗口 使用cd命令切换目录到 C:\redis 运行 redis-server.exe redis.windows.conf 。 如果想方便的话，可以把 redis 的路径加到系统的环境变量里，这样就省得再输路径了，后面的那个 redis.windows.conf 可以省略，如果省略，会启用默认的。输入之后，会显示如下界面： 这时候另启一个cmd窗口，原来的不要关闭，不然就无法访问服务端了。 切换到redis目录下运行 redis-cli.exe -h 127.0.0.1 -p 6379 。 设置键值对 set myKey abc 取出键值对 get myKey Linux 下安装下载地址：http://redis.io/download，下载最新稳定版本。 本文档使用3.2.11,下载并安装: 1234$ wget http://download.redis.io/releases/redis-3.2.11.tar.gz$ tar -xvf redis-3.2.11.tar.gz$ cd redis-3.2.11$ make &amp;&amp; make install 安装完成后,我们先改一下redis.conf的配置: 123456789$ cp redis.conf ../myredis.conf # 将文件redis.conf复制到上以及文件夹下并命名为myredis.conf$ cd ..$ vim myredis.conf# redis.conf是一个默认的配置文件。我们可以根据需要使用自己的配置文件。# 建议修改以下地方:# 找到bind这个地方并修改成[bind 本机内网地址]# 再找到requirepass这个地方,这里是修改redis的密码# 完成上述操作后保存退出$ redis-server myredis.conf 启动redis服务进程后，就可以使用测试客户端程序redis-cli和redis服务交互了,如: 12345$ redis-cli -h host -p port$ auth [password] # 这里输入的密码就是你上面设置的密码,如果没有则跳过此步骤redis 127.0.0.1:6379&gt; pingPONG$ redis-cli -h host -p port -a password # 这个命令也能登录 用Python连接Redis当前在本地我已经安装了 Redis 并运行在 9736 端口，密码设置为 123456 用下面例子连接Redis进行测试: 123456789101112131415161718import redisdef main(): config = &#123; 'host': 'localhost', 'port': 9736, 'db': 0, 'password': '123456' &#125; client = redis.Redis(**config) if client.ping(): client.set('name', 'gavinliu') print(client.get('name'))if __name__ == '__main__': main() 在这里我们使用redis.Redis传入参数连接本地Redis服务,传入的四个参数分别为Redis的地址、运行端口、使用的数据库、密码，我们这里使用的Redis其实是继承了StrictRedis，所以在默认情况下不传这四个参数时，参数值会有默认值分别为localhost、6379、0、None。 上面运行的结果： 1b'gavinliu' 在Python中使用Redis其实很方便，Redis的命令(命令详细情况可以参照Redis命令参考)在Python中都能找到相应的方法，这里引用崔庆才老师对Redis总结介绍一下Key(键)、String(字符串)、Hash(哈希表)、List(列表)、Set(集合)、SortedSet(有序集合) Key(键) 方法 作用 参数说明 示例 示例说明 示例结果 exists(name) 判断一个key是否存在 name: key名 redis.exists(‘name’) 是否存在name这个key True delete(name) 删除一个key name: key名 redis.delete(‘name’) 删除name这个key 1 type(name) 判断key类型 name: key名 redis.type(‘name’) 判断name这个key类型 b’string’ keys(pattern) 获取所有符合规则的key pattern: 匹配规则 redis.keys(‘n*’) 获取所有以n开头的key [b’name’] randomkey() 获取随机的一个key randomkey() 获取随机的一个key b’name’ rename(src, dst) 将key重命名 src: 原key名 dst: 新key名 redis.rename(‘name’, ‘nickname’) 将name重命名为nickname True dbsize() 获取当前数据库中key的数目 dbsize() 获取当前数据库中key的数目 100 expire(name, time) 设定key的过期时间，单位秒 name: key名 time: 秒数 redis.expire(‘name’, 2) 将name这key的过期时间设置2秒 True ttl(name) 获取key的过期时间，单位秒，-1为永久不过期 name: key名 redis.ttl(‘name’) 获取name这key的过期时间 -1 move(name, db) 将key移动到其他数据库 name: key名 db: 数据库代号 move(‘name’, 2) 将name移动到2号数据库 True flushdb() 删除当前选择数据库中的所有key flushdb() 删除当前选择数据库中的所有key True flushall() 删除所有数据库中的所有key flushall() 删除所有数据库中的所有key True String(字符串) 方法 作用 参数说明 示例 示例说明 示例结果 set(name, value) 给数据库中key为name的string赋予值value name: key名 value: 值 redis.set(‘name’, ‘Bob’) 给name这个key的value赋值为Bob True get(name) 返回数据库中key为name的string的value name: key名 redis.get(‘name’) 返回name这个key的value b’Bob’ getset(name, value) 给数据库中key为name的string赋予值value并返回上次的value name: key名 value: 新值 redis.getset(‘name’, ‘Mike’) 赋值name为Mike并得到上次的value b’Bob’ mget(keys, *args) 返回多个key对应的value keys: key的列表 redis.mget([‘name’, ‘nickname’]) 返回name和nickname的value [b’Mike’, b’Miker’] setnx(name, value) 如果key不存在才设置value name: key名 redis.setnx(‘newname’, ‘James’) 如果newname这key不存在则设置值为James 第一次运行True，第二次False setex(name, time, value) 设置可以对应的值为string类型的value，并指定此键值对应的有效期 name: key名 time: 有效期 value: 值 redis.setex(‘name’, 1, ‘James’) 将name这key的值设为James，有效期1秒 True setrange(name, offset, value) 设置指定key的value值的子字符串 name: key名 offset: 偏移量 value: 值 redis.set(‘name’, ‘Hello’) redis.setrange(‘name’, 6, ‘World’) 设置name为Hello字符串，并在index为6的位置补World 11，修改后的字符串长度 mset(mapping) 批量赋值 mapping: 字典 redis.mset({‘name1’: ‘Durant’, ‘name2’: ‘James’}) 将name1设为Durant，name2设为James True msetnx(mapping) key均不存在时才批量赋值 mapping: 字典 redis.msetnx({‘name3’: ‘Smith’, ‘name4’: ‘Curry’}) 在name3和name4均不存在的情况下才设置二者值 True incr(name, amount=1) key为name的value增值操作，默认1，key不存在则被创建并设为amount name: key名 amount:增长的值 redis.incr(‘age’, 1) age对应的值增1，若不存在则会创建并设置为1 1，即修改后的值 decr(name, amount=1) key为name的value减值操作，默认1，key不存在则被创建并设置为-amount name: key名 amount:减少的值 redis.decr(‘age’, 1) age对应的值减1，若不存在则会创建并设置为-1 -1，即修改后的值 append(key, value) key为name的string的值附加value key: key名 redis.append(‘nickname’, ‘OK’) 向key为nickname的值后追加OK 13，即修改后的字符串长度 substr(name, start, end=-1) 返回key为name的string的value的子串 name: key名 start: 起始索引 end: 终止索引，默认-1截取到末尾 redis.substr(‘name’, 1, 4) 返回key为name的值的字符串，截取索引为1-4的字符 b’ello’ getrange(key, start, end) 获取key的value值从start到end的子字符串 key: key名 start: 起始索引 end: 终止索引 redis.getrange(‘name’, 1, 4) 返回key为name的值的字符串，截取索引为1-4的字符 b’ello’ Hash(哈希表) Hash，即哈希。Redis 还提供了哈希表的数据结构，我们可以用name指定一个哈希表的名称，然后表内存储了各个键值对，用法总结如下： 方法 作用 参数说明 示例 示例说明 示例结果 hset(name, key, value) 向key为name的hash中添加映射 name: key名 key: 映射键名 value: 映射键值 hset(‘price’, ‘cake’, 5) 向key为price的hash中添加映射关系，cake的值为5 1，即添加的映射个数 hsetnx(name, key, value) 向key为name的hash中添加映射，如果映射键名不存在 name: key名 key: 映射键名 value: 映射键值 hsetnx(‘price’, ‘book’, 6) 向key为price的hash中添加映射关系，book的值为6 1，即添加的映射个数 hget(name, key) 返回key为name的hash中field对应的value name: key名 key: 映射键名 redis.hget(‘price’, ‘cake’) 获取key为price的hash中键名为cake的value 5 hmget(name, keys, *args) 返回key为name的hash中各个键对应的value name: key名 keys: 映射键名列表 redis.hmget(‘price’, [‘apple’, ‘orange’]) 获取key为price的hash中apple和orange的值 [b’3’, b’7’] hmset(name, mapping) 向key为name的hash中批量添加映射 name: key名 mapping: 映射字典 redis.hmset(‘price’, {‘banana’: 2, ‘pear’: 6}) 向key为price的hash中批量添加映射 True hincrby(name, key, amount=1) 将key为name的hash中映射的value增加amount name: key名 key: 映射键名 amount: 增长量 redis.hincrby(‘price’, ‘apple’, 3) key为price的hash中apple的值增加3 6，修改后的值 hexists(name, key) key为namehash中是否存在键名为key的映射 name: key名 key: 映射键名 redis.hexists(‘price’, ‘banana’) key为price的hash中banana的值是否存在 True hdel(name, *keys) key为namehash中删除键名为key的映射 name: key名 key: 映射键名 redis.hdel(‘price’, ‘banana’) 从key为price的hash中删除键名为banana的映射 True hlen(name) 从key为name的hash中获取映射个数 name: key名 redis.hlen(‘price’) 从key为price的hash中获取映射个数 6 hkeys(name) 从key为name的hash中获取所有映射键名 name: key名 redis.hkeys(‘price’) 从key为price的hash中获取所有映射键名 [b’cake’, b’book’, b’banana’, b’pear’] hvals(name) 从key为name的hash中获取所有映射键值 name: key名 redis.hvals(‘price’) 从key为price的hash中获取所有映射键值 [b’5’, b’6’, b’2’, b’6’] hgetall(name) 从key为name的hash中获取所有映射键值对 name: key名 redis.hgetall(‘price’) 从key为price的hash中获取所有映射键值对 {b’cake’: b’5’, b’book’: b’6’, b’orange’: b’7’, b’pear’: b’6’} List(列表) Redis 还提供了列表存储，列表内的元素可以重复，而且可以从两端存储 方法 作用 参数说明 示例 示例说明 示例结果 rpush(name, *values) 在key为name的list尾添加值为value的元素，可以传多个 name: key名 values: 值 redis.rpush(‘list’, 1, 2, 3) 给list这个key的list尾添加1、2、3 3，list大小 lpush(name, *values) 在key为name的list头添加值为value的元素，可以传多个 name: key名 values: 值 redis.lpush(‘list’, 0) 给list这个key的list头添加0 4，list大小 llen(name) 返回key为name的list的长度 name: key名 redis.llen(‘list’) 返回key为list的列表的长度 4 lrange(name, start, end) 返回key为name的list中start至end之间的元素 name: key名 start: 起始索引 end: 终止索引 redis.lrange(‘list’, 1, 3) 返回起始为1终止为3的索引范围对应的list [b&#39;3&#39;, b&#39;2&#39;, b&#39;1&#39;] ltrim(name, start, end) 截取key为name的list，保留索引为start到end的内容 name:key名 start: 起始索引 end: 终止索引 ltrim(‘list’, 1, 3) 保留key为list的索引为1到3的元素 True lindex(name, index) 返回key为name的list中index位置的元素 name: key名 index: 索引 redis.lindex(‘list’, 1) 返回key为list的列表index为1的元素 b’2’ lset(name, index, value) 给key为name的list中index位置的元素赋值，越界则报错 name: key名 index: 索引位置 value: 值 redis.lset(‘list’, 1, 5) 将key为list的list索引1位置赋值为5 True lrem(name, count, value) 删除count个key的list中值为value的元素 name: key名 count: 删除个数 value: 值 redis.lrem(‘list’, 2, 3) 将key为list的列表删除2个3 1，即删除的个数 lpop(name) 返回并删除key为name的list中的首元素 name: key名 redis.lpop(‘list’) 返回并删除名为list的list第一个元素 b’5’ rpop(name) 返回并删除key为name的list中的尾元素 name: key名 redis.rpop(‘list’) 返回并删除名为list的list最后一个元素 b’2’ blpop(keys, timeout=0) 返回并删除名称为在keys中的list中的首元素，如果list为空，则会一直阻塞等待 keys: key列表 timeout: 超时等待时间，0为一直等待 redis.blpop(‘list’) 返回并删除名为list的list的第一个元素 [b’5’] brpop(keys, timeout=0) 返回并删除key为name的list中的尾元素，如果list为空，则会一直阻塞等待 keys: key列表 timeout: 超时等待时间，0为一直等待 redis.brpop(‘list’) 返回并删除名为list的list的最后一个元素 [b’2’] rpoplpush(src, dst) 返回并删除名称为src的list的尾元素，并将该元素添加到名称为dst的list的头部 src: 源list的key dst: 目标list的key redis.rpoplpush(‘list’, ‘list2’) 将key为list的list尾元素删除并返回并将其添加到key为list2的list头部 b’2’ Set(集合) Redis 还提供了集合存储，集合中的元素都是不重复的 方法 作用 参数说明 示例 示例说明 示例结果 sadd(name, *values) 向key为name的set中添加元素 name: key名 values: 值，可为多个 redis.sadd(‘tags’, ‘Book’, ‘Tea’, ‘Coffee’) 向key为tags的set中添加Book、Tea、Coffee三个内容 3，即插入的数据个数 srem(name, *values) 从key为name的set中删除元素 name: key名 values: 值，可为多个 redis.srem(‘tags’, ‘Book’) 从key为tags的set中删除Book 1，即删除的数据个数 spop(name) 随机返回并删除key为name的set中一个元素 name: key名 redis.spop(‘tags’) 从key为tags的set中随机删除并返回该元素 b’Tea’ smove(src, dst, value) 从src对应的set中移除元素并添加到dst对应的set中 src: 源set dst: 目标set value: 元素值 redis.smove(‘tags’, ‘tags2’, ‘Coffee’) 从key为tags的set中删除元素Coffee并添加到key为tags2的set True scard(name) 返回key为name的set的元素个数 name: key名 redis.scard(‘tags’) 获取key为tags的set中元素个数 3 sismember(name, value) 测试member是否是key为name的set的元素 name:key值 redis.sismember(‘tags’, ‘Book’) 判断Book是否为key为tags的set元素 True sinter(keys, *args) 返回所有给定key的set的交集 keys: key列表 redis.sinter([‘tags’, ‘tags2’]) 返回key为tags的set和key为tags2的set的交集 {b’Coffee’} sinterstore(dest, keys, *args) 求交集并将交集保存到dest的集合 dest:结果集合 keys:key列表 redis.sinterstore(‘inttag’, [‘tags’, ‘tags2’]) 求key为tags的set和key为tags2的set的交集并保存为inttag 1 sunion(keys, *args) 返回所有给定key的set的并集 keys: key列表 redis.sunion([‘tags’, ‘tags2’]) 返回key为tags的set和key为tags2的set的并集 {b’Coffee’, b’Book’, b’Pen’} sunionstore(dest, keys, *args) 求并集并将并集保存到dest的集合 dest:结果集合 keys:key列表 redis.sunionstore(‘inttag’, [‘tags’, ‘tags2’]) 求key为tags的set和key为tags2的set的并集并保存为inttag 3 sdiff(keys, *args) 返回所有给定key的set的差集 keys: key列表 redis.sdiff([‘tags’, ‘tags2’]) 返回key为tags的set和key为tags2的set的差集 {b’Book’, b’Pen’} sdiffstore(dest, keys, *args) 求差集并将差集保存到dest的集合 dest:结果集合 keys:key列表 redis.sdiffstore(‘inttag’, [‘tags’, ‘tags2’]) 求key为tags的set和key为tags2的set的差集并保存为inttag 3 smembers(name) 返回key为name的set的所有元素 name: key名 redis.smembers(‘tags’) 返回key为tags的set的所有元素 {b’Pen’, b’Book’, b’Coffee’} srandmember(name) 随机返回key为name的set的一个元素，但不删除元素 name: key值 redis.srandmember(‘tags’) 随机返回key为tags的set的一个元素 SortedSet(有序集合) 有序集合，它相比集合多了一个分数字段，利用它我们可以对集合中的数据进行排序 方法 作用 参数说明 示例 示例说明 示例结果 zadd(name, args, *kwargs) 向key为name的zset中添加元素member，score用于排序。如果该元素存在，则更新其顺序 name: key名 args: 可变参数 redis.zadd(‘grade’, 100, ‘Bob’, 98, ‘Mike’) 向key为grade的zset中添加Bob，score为100，添加Mike，score为98 2，即添加的元素个数 zrem(name, *values) 删除key为name的zset中的元素 name: key名 values: 元素 redis.zrem(‘grade’, ‘Mike’) 从key为grade的zset中删除Mike 1，即删除的元素个数 zincrby(name, value, amount=1) 如果在key为name的zset中已经存在元素value，则该元素的score增加amount，否则向该集合中添加该元素，其score的值为amount name: key名 value: 元素 amount: 增长的score值 redis.zincrby(‘grade’, ‘Bob’, -2) key为grade的zset中Bob的score减2 98.0，即修改后的值 zrank(name, value) 返回key为name的zset中元素的排名（按score从小到大排序）即下标 name: key名 value: 元素值 redis.zrank(‘grade’, ‘Amy’) 得到key为grade的zset中Amy的排名 1 zrevrank(name, value) 返回key为name的zset中元素的倒数排名（按score从大到小排序）即下标 name: key名 value: 元素值 redis.zrevrank(‘grade’, ‘Amy’) 得到key为grade的zset中Amy的倒数排名 2 zrevrange(name, start, end, withscores=False) 返回key为name的zset（按score从大到小排序）中的index从start到end的所有元素 name: key值 start: 开始索引 end: 结束索引 withscores: 是否带score redis.zrevrange(‘grade’, 0, 3) 返回key为grade的zset前四名元素 [b’Bob’, b’Mike’, b’Amy’, b’James’] zrangebyscore(name, min, max, start=None, num=None, withscores=False) 返回key为name的zset中score在给定区间的元素 name:key名 min: 最低score max:最高score start: 起始索引 num: 个数 withscores: 是否带score redis.zrangeby score(‘grade’, 80, 95) 返回key为grade的zset中score在80和95之间的元素 [b’Amy’, b’James’] zcount(name, min, max) 返回key为name的zset中score在给定区间的数量 name:key名 min: 最低score max: 最高score redis.zcount(‘grade’, 80, 95) 返回key为grade的zset中score在80到95的元素个数 2 zcard(name) 返回key为name的zset的元素个数 name: key名 redis.zcard(‘grade’) 获取key为grade的zset中元素个数 3 zremrangebyrank(name, min, max) 删除key为name的zset中排名在给定区间的元素 name:key名 min: 最低位次 max: 最高位次 redis.zremran gebyrank(‘grade’, 0, 0) 删除key为grade的zset中排名第一的元素 1，即删除的元素个数 zremrangebyscore(name, min, max) 删除key为name的zset中score在给定区间的元素 name:key名 min: 最低score max:最高score redis.zremran gebyscore (‘grade’, 80, 90) 删除score在80到90之间的元素 1，即删除的元素个数 注意：上面代码中由于展示显示不完全，本人做了相应的处理，原本是一个方法名的可能方法名中间会出现空格 以上便是用Python操作Redis的总结了，在后面的项目中会常用到Redis操作，所以我们还是需要掌握一些常用操作。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进程池]]></title>
    <url>%2Fpost%2Fd1bc6a63.html</url>
    <content type="text"><![CDATA[进程池为什么要有进程池? 在程序实际处理问题过程中，忙时会有成千上万的任务需要被执行，闲时可能只有零星任务。那么在成千上万个任务需要被执行的时候，我们就需要去创建成千上万个进程么？首先，创建进程需要消耗时间，销毁进程也需要消耗时间。第二即便开启了成千上万的进程，操作系统也不能让他们同时执行，这样反而会影响程序的效率。因此我们不能无限制的根据任务开启或者结束进程。那么我们要怎么做呢？ 在这里，要给大家介绍一个进程池的概念，定义一个池子，在里面放上固定数量的进程，有需求来了，就拿一个池中的进程来处理任务，等到处理完毕，进程并不关闭，而是将进程再放回进程池中继续等待任务。如果有很多任务需要执行，池中的进程数量不够，任务就要等待之前的进程执行任务完毕归来，拿到空闲进程才能继续执行。也就是说，池中进程的数量是固定的，那么同一时间最多有固定数量的进程在运行。这样不会增加操作系统的调度难度，还节省了开闭进程的时间，也一定程度上能够实现并发效果。 multiprocess.Pool模块1Pool([numprocess [,initializer [, initargs]]]):创建进程池 参数介绍 ： numprocess：要创建的进程数，如果省略，将默认使用cpu_count()的值 initializer：是每个工作进程启动时要执行的可调用对象，默认为None initargs：是要传给initializer的参数组 主要方法 ： p.apply(func [, args [, kwargs]]):在一个池工作进程中执行func(args,*kwargs),然后返回结果。 注意：此操作并不会在所有池工作进程中并执行func函数。如果要通过不同参数并发地执行func函数，必须从不同线程调用p.apply()函数或者使用p.apply_async() p.apply_async(func [, args [, kwargs [, callback]]]):在一个池工作进程中执行func(args,*kwargs),然后返回结果。 此方法的结果是AsyncResult类的实例，callback是可调用对象，接收输入参数。当func的结果变为可用时，将理解传递给callback。callback禁止执行任何阻塞操作，否则将接收其他异步操作中的结果。 p.close():关闭进程池，防止进一步操作。如果所有操作持续挂起，它们将在工作进程终止前完成 P.jion():等待所有工作进程退出。此方法只能在close（）或teminate()之后调用 其他方法： ​ 方法apply_async()和map_async（）的返回值是AsyncResul的实例obj。实例具有以下方法 obj.get():返回结果，如果有必要则等待结果到达。timeout是可选的。如果在指定时间内还没有到达，将引发异常。如果远程操作中引发了异常，它将在调用此方法时再次被引发。 obj.ready():如果调用完成，返回True obj.successful():如果调用完成且没有引发异常，返回True，如果在结果就绪之前调用此方法，引发异常 obj.wait([timeout]):等待结果变为可用 obj.terminate()：立即终止所有工作进程，同时不执行任何清理或结束任何挂起工作。如果p被垃圾回收，将自动调用此函数 进程池的同步调用123456789101112131415import os,timefrom multiprocessing import Pooldef work(n): print('%s run' %os.getpid()) time.sleep(3) return n**2if __name__ == '__main__': p=Pool(3) # 进程池中从无到有创建三个进程,以后一直是这三个进程在执行任务 res_l=[] for i in range(10): res=p.apply(work,args=(i,)) # 同步调用，直到本次任务执行完毕拿到res，等待任务work执行的过程中可能有阻塞也可能没有阻塞 # 但不管该任务是否存在阻塞，同步调用都会在原地等着 print(res_l) 进程池的异步调用1234567891011121314151617181920212223242526import osimport timeimport randomfrom multiprocessing import Pooldef work(n): print('%s run' %os.getpid()) time.sleep(random.random()) return n**2if __name__ == '__main__': p=Pool(3) # 进程池中从无到有创建三个进程,以后一直是这三个进程在执行任务 res_l=[] for i in range(10): res=p.apply_async(work,args=(i,)) # 异步运行，根据进程池中有的进程数，每次最多3个子进程在异步执行 # 返回结果之后，将结果放入列表，归还进程，之后再执行新的任务 # 需要注意的是，进程池中的三个进程不会同时开启或者同时结束 # 而是执行完一个就释放一个进程，这个进程就去接收新的任务。 res_l.append(res) # 异步apply_async用法：如果使用异步提交的任务，主进程需要使用jion，等待进程池内任务都处理完，然后可以用get收集结果 # 否则，主进程结束，进程池可能还没来得及执行，也就跟着一起结束了 p.close() p.join() for res in res_l: print(res.get()) #使用get来获取apply_aync的结果,如果是apply,则没有get方法,因为apply是同步执行,立刻获取结果,也根本无需get 回调函数需要回调函数的场景：进程池中任何一个任务一旦处理完了，就立即告知主进程：我好了额，你可以处理我的结果了。主进程则调用一个函数去处理该结果，该函数即回调函数 我们可以把耗时间（阻塞）的任务放到进程池中，然后指定回调函数（主进程负责执行），这样主进程在执行回调函数时就省去了I/O的过程，直接拿到的是任务的结果。 下面来看看实例： 1234567891011121314151617181920212223242526272829303132333435import refrom urllib.request import urlopenfrom multiprocessing import Pooldef get_page(url,pattern): response=urlopen(url).read().decode('utf-8') return pattern,responsedef parse_page(info): pattern,page_content=info res=re.findall(pattern,page_content) for item in res: dic=&#123; 'index':item[0].strip(), 'title':item[1].strip(), 'actor':item[2].strip(), 'time':item[3].strip(), &#125; print(dic)if __name__ == '__main__': regex = r'&lt;dd&gt;.*?&lt;.*?class="board-index.*?&gt;(\d+)&lt;/i&gt;.*?title="(.*?)".*?class="movie-item-info".*?&lt;p class="star"&gt;(.*?)&lt;/p&gt;.*?&lt;p class="releasetime"&gt;(.*?)&lt;/p&gt;' pattern1=re.compile(regex,re.S) url_dic=&#123; 'http://maoyan.com/board/7':pattern1, &#125; p=Pool() res_l=[] for url,pattern in url_dic.items(): res=p.apply_async(get_page,args=(url,pattern),callback=parse_page) res_l.append(res) for i in res_l: i.get() 注意：如果在主进程中等待进程池中所有任务都执行完毕后，再统一处理结果，则无需回调函数]]></content>
      <categories>
        <category>Python进程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>进程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进程间通信之队列]]></title>
    <url>%2Fpost%2F58b02bea.html</url>
    <content type="text"><![CDATA[进程间通信进程间通信有队列（multiprocess.Queue）和管道（multiprocess.Pipe），在这里只简单介绍队列 队列概念介绍创建共享的进程队列，Queue是多进程安全的队列，可以使用Queue实现多进程之间的数据传递。 1234Queue([maxsize]) 创建共享的进程队列。参数 ：maxsize是队列中允许的最大项数。如果省略此参数，则无大小限制。底层队列使用管道和锁定实现，另外，还需要运行支持线程以便队列中的数据传输到底层管道中 方法介绍 ： q.get( [ block [ ,timeout ] ] ) 返回q中的一个项目。如果q为空，此方法将阻塞，直到队列中有项目可用为止。block用于控制阻塞行为，默认为True. 如果设置为False，将引发Queue.Empty异常（定义在Queue模块中）。timeout是可选超时时间，用在阻塞模式中。如果在制定的时间间隔内没有项目变为可用，将引发Queue.Empty异常。 q.get_nowait( ) 同q.get(False)方法。 q.put(item [, block [,timeout ] ] ) 将item放入队列。如果队列已满，此方法将阻塞至有空间可用为止。block控制阻塞行为，默认为True。如果设置为False，将引发Queue.Empty异常（定义在Queue库模块中）。timeout指定在阻塞模式中等待可用空间的时间长短。超时后将引发Queue.Full异常。 q.qsize() 返回队列中目前项目的正确数量。此函数的结果并不可靠，因为在返回结果和在稍后程序中使用结果之间，队列中可能添加或删除了项目。在某些系统上，此方法可能引发NotImplementedError异常。 q.empty() 如果调用此方法时 q为空，返回True。如果其他进程或线程正在往队列中添加项目，结果是不可靠的。也就是说，在返回和使用结果之间，队列中可能已经加入新的项目。 q.full() 如果q已满，返回为True. 由于线程的存在，结果也可能是不可靠的（参考q.empty（）方法） q.close() 关闭队列，防止队列中加入更多数据。调用此方法时，后台线程将继续写入那些已入队列但尚未写入的数据，但将在此方法完成时马上关闭。如果q被垃圾收集，将自动调用此方法。关闭队列不会在队列使用者中生成任何类型的数据结束信号或异常。例如，如果某个使用者正被阻塞在get（）操作上，关闭生产者中的队列不会导致get（）方法返回错误。 q.cancel_join_thread() 不会再进程退出时自动连接后台线程。这可以防止join_thread()方法阻塞。 q.join_thread() 连接队列的后台线程。此方法用于在调用q.close()方法后，等待所有队列项被消耗。默认情况下，此方法由不是q的原始创建者的所有进程调用。调用q.cancel_join_thread()方法可以禁止这种行为。 实例 单看队列用法 12345678910111213141516171819202122232425262728from multiprocessing import Queueq = Queue(3)# put ,get ,put_nowait,get_nowait,full,emptyq.put(3)q.put(3)q.put(3)# q.put(3) # 如果队列已经满了，程序就会停在这里，等待数据被别人取走，再将数据放入队列。# 如果队列中的数据一直不被取走，程序就会永远停在这里。try: q.put_nowait(3) # 可以使用put_nowait，如果队列满了不会阻塞，但是会因为队列满了而报错。except: # 因此我们可以用一个try语句来处理这个错误。这样程序不会一直阻塞下去，但是会丢掉这个消息。 print('队列已经满了')# 因此，我们再放入数据之前，可以先看一下队列的状态，如果已经满了，就不继续put了。print(q.full()) # 满了print(q.get())print(q.get())print(q.get())# print(q.get()) # 同put方法一样，如果队列已经空了，那么继续取就会出现阻塞。try: q.get_nowait(3) # 可以使用get_nowait，如果队列满了不会阻塞，但是会因为没取到值而报错。except: # 因此我们可以用一个try语句来处理这个错误。这样程序不会一直阻塞下去。 print('队列已经空了')print(q.empty()) # 空了 上面这个例子还没有加入进程通信，只是先来看看队列为我们提供的方法，以及这些方法的使用和现象 子进程发送数据给父进程 1234567891011121314import timefrom multiprocessing import Process, Queuedef func(q): q.put([time.asctime(), 'from Eva', 'hello']) # 调用主函数中p进程传递过来的进程参数 put函数为向队列中添加一条数据。if __name__ == '__main__': q = Queue() # 创建一个Queue对象 p = Process(target=func, args=(q,)) # 创建一个进程 p.start() print(q.get()) p.join() 上面是一个queue的简单应用，使用队列q对象调用get函数来取得队列中最先进入的数据 批量生产数据放入队列再批量获取结果 123456789101112131415161718192021222324252627282930313233import timeimport randomimport osfrom multiprocessing import Process, Queuedef consumer(q): while True: res = q.get() time.sleep(random.randint(1, 3)) print('%s 吃 %s' % (os.getpid(), res))def producer(q): for i in range(10): time.sleep(random.randint(1, 3)) res = '包子%s' % i q.put(res) print('%s 生产了 %s' % (os.getpid(), res))if __name__ == '__main__': q = Queue() # 生产者们:即厨师们 p1 = Process(target=producer, args=(q,)) # 消费者们:即吃货们 c1 = Process(target=consumer, args=(q,)) # 开始 p1.start() c1.start() print('主') 问题来了，我们看结果主进程永远不会结束，原因是：生产者p在生产完后就结束了，但是消费者c在取空了q之后，则一直处于死循环中且卡在q.get()这一步。 在解决上面问题之前，这里引入一个生产者消费者模型 生产者消费者模型在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序的整体处理数据的速度 为什么要使用生产者和消费者模式 在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发当中，如果生产者处理速度很快，而消费者处理速度很慢，那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。为了解决这个问题于是引入了生产者和消费者模式。 什么是生产者消费者模式 生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。 要解决上面产生的问题无非是让生产者在生产完毕后，往队列中再发一个结束信号，这样消费者在接收到结束信号后就可以break出死循环 123456789101112131415161718192021222324252627282930313233from multiprocessing import Process, Queueimport time, random, osdef consumer(q): while True: res = q.get() if res is None: break # 收到结束信号则结束 time.sleep(random.randint(1, 3)) print('%s 吃 %s' % (os.getpid(), res))def producer(q): for i in range(10): time.sleep(random.randint(1, 3)) res = '包子%s' % i q.put(res) print('%s 生产了 %s' % (os.getpid(), res)) q.put(None) # 发送结束信号if __name__ == '__main__': q = Queue() # 生产者们:即厨师们 p1 = Process(target=producer, args=(q,)) # 消费者们:即吃货们 c1 = Process(target=consumer, args=(q,)) # 开始 p1.start() c1.start() print('主') 注意：结束信号None，不一定要由生产者发，主进程里同样可以发，但主进程需要等生产者结束后才应该发送该信号 下面再看看多个消费者的例子：有几个消费者就需要发送几次结束信号 1234567891011121314151617181920212223242526272829303132333435363738394041424344from multiprocessing import Process, Queueimport time, random, osdef consumer(q): while True: res = q.get() if res is None: break # 收到结束信号则结束 time.sleep(random.randint(1, 3)) print('%s 吃 %s' % (os.getpid(), res))def producer(name, q): for i in range(2): time.sleep(random.randint(1, 3)) res = '%s%s' % (name, i) q.put(res) print('%s 生产了 %s' % (os.getpid(), res))if __name__ == '__main__': q = Queue() # 生产者们:即厨师们 p1 = Process(target=producer, args=('啤酒鸭', q)) p2 = Process(target=producer, args=('烤鱼', q)) p3 = Process(target=producer, args=('烧鸡', q)) # 消费者们:即吃货们 c1 = Process(target=consumer, args=(q,)) c2 = Process(target=consumer, args=(q,)) # 开始 p1.start() p2.start() p3.start() c1.start() c2.start() p1.join() # 必须保证生产者全部生产完毕,才应该发送结束信号 p2.join() p3.join() q.put(None) # 有几个消费者就应该发送几次结束信号None q.put(None) # 发送结束信号 print('主') 有没有发现上面的方法很low，有几个消费者就要发送几次结束信号，下面再优化一下 12JoinableQueue([maxsize]) 创建可连接的共享进程队列，这就像是一个Queue对象，但队列允许项目的使用者通知生产者项目已经被成功处理。通知进程是使用共享的信号和条件变量来实现的。 JoinableQueue的实例p除了与Queue对象相同的方法之外，还具有以下方法 方法介绍 ： q.task_done() 使用者使用此方法发出信号，表示q.get()返回的项目已经被处理。如果调用此方法的次数大于从队列中删除的项目数量，将引发ValueError异常。 q.join() 生产者将使用此方法进行阻塞，直到队列中所有项目均被处理。阻塞将持续到为队列中的每个项目均调用q.task_done()方法为止。 下面的例子说明如何建立永远运行的进程，使用和处理队列上的项目。生产者将项目放入队列，并等待它们被处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from multiprocessing import Process, JoinableQueueimport time, random, osdef consumer(q): while True: res = q.get() if res is None: break # 收到结束信号则结束 time.sleep(random.randint(1, 3)) print('%s 吃 %s' % (os.getpid(), res)) q.task_done() # 向q.join()发送一次信号,证明一个数据已经被取走了def producer(name, q): for i in range(2): time.sleep(random.randint(1, 3)) res = '%s%s' % (name, i) q.put(res) print('%s 生产了 %s' % (os.getpid(), res)) q.join() # 生产完毕，使用此方法进行阻塞，直到队列中所有项目均被处理if __name__ == '__main__': q = JoinableQueue() # 生产者们:即厨师们 p1 = Process(target=producer, args=('啤酒鸭', q)) p2 = Process(target=producer, args=('烤鱼', q)) p3 = Process(target=producer, args=('烧鸡', q)) # 消费者们:即吃货们 c1 = Process(target=consumer, args=(q,)) c2 = Process(target=consumer, args=(q,)) c1.daemon = True c2.daemon = True # 开始 p_l = [p1, p2, p3, c1, c2] for p in p_l: p.start() p1.join() # 必须保证生产者全部生产完毕,才应该发送结束信号 p2.join() p3.join() print('主')# p1,p2,p3结束了,证明c1,c2肯定全都收完了p1,p2,p3发到队列的数据# 因而c1,c2也没有存在的价值了,不需要继续阻塞在进程中影响主进程了。应该随着主进程的结束而结束,所以设置成守护进程就可以了 参考资料： 1234http://www.cnblogs.com/Eva-J/articles/8253549.htmlhttp://www.cnblogs.com/linhaifeng/articles/6817679.htmlhttps://www.jianshu.com/p/1200fd49b583https://www.jianshu.com/p/aed6067eeac9]]></content>
      <categories>
        <category>Python进程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>队列</tag>
        <tag>进程通信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进程同步之锁]]></title>
    <url>%2Fpost%2F779a7de2.html</url>
    <content type="text"><![CDATA[锁 —— multiprocess.Lock通过刚刚的学习，我们千方百计实现了程序的异步，让多个任务可以同时在几个进程中并发处理，他们之间的运行没有顺序，一旦开启也不受我们控制。尽管并发编程让我们能更加充分的利用IO资源，但是也给我们带来了新的问题。 当多个进程使用同一份数据资源的时候，就会引发数据安全或顺序混乱问题。 接下来我们以模拟抢票为例，来看看数据安全的重要性。 先看看不使用锁保护的多进程同时抢购余票结果： 12345678910111213141516171819202122232425262728293031# 文件ticket的内容为：&#123;"count":1&#125;# 注意一定要用双引号，不然json无法识别# 并发运行，效率高，但竞争写同一文件，数据写入错乱from multiprocessing import Process, Lockimport time, jsondef search(): dic = json.load(open('ticket')) print('\033[31m剩余票数%s\033[0m' % dic['count'])def get(i): dic = json.load(open('ticket')) time.sleep(0.1) # 模拟读数据的网络延迟 if dic['count'] &gt; 0: dic['count'] -= 1 time.sleep(0.2) # 模拟写数据的网络延迟 json.dump(dic, open('ticket', 'w')) print('\033[32m用户%s购票成功\033[0m' % i)def task(i): search() get(i)if __name__ == '__main__': for i in range(10): # 模拟并发10个客户端抢票 p = Process(target=task, args=(i,)) p.start() 想想上面代码中会有什么样的结果出现？是只有一个用户购票成功，还是多个用户，下面一起来看看结果： 为什么会出现这样的情况呢？明明只有一张票，怎么会被这么多人购买成功，这个就是数据安全问题！！！ 下面来解决这个数据安全问题： 1234567891011121314151617181920212223242526272829303132333435# 文件ticket的内容为：&#123;"count":1&#125;# 注意一定要用双引号，不然json无法识别# 并发运行，效率高，但竞争写同一文件，数据写入错乱from multiprocessing import Process, Lockimport time, jsondef search(): dic = json.load(open('ticket')) print('\033[31m剩余票数%s\033[0m' % dic['count'])def get(i): dic = json.load(open('ticket')) time.sleep(0.1) # 模拟读数据的网络延迟 if dic['count'] &gt; 0: dic['count'] -= 1 time.sleep(0.2) # 模拟写数据的网络延迟 json.dump(dic, open('ticket', 'w')) print('\033[32m用户%s购票成功\033[0m' % i)def task(i, lock): search() lock.acquire() # 获取钥匙 get(i) # 有钥匙后进入方法执行代码 lock.release() # 归还钥匙if __name__ == '__main__': lock = Lock() for i in range(10): # 模拟并发10个客户端抢票 p = Process(target=task, args=(i, lock)) p.start() 结果： 加锁可以保证多个进程修改同一块数据时，同一时间只能有一个任务可以进行修改，即串行的修改，没错，速度是慢了，但牺牲了速度却保证了数据安全。虽然可以用文件共享数据实现进程间通信，但问题是：1、效率低（共享数据基于文件，而文件是硬盘上的数据）2、需要自己加锁处理 因此我们最好找寻一种解决方案能够兼顾： 1、效率高（多个进程共享一块内存的数据） 2、帮我们处理好锁问题。这就是mutiprocessing模块为我们提供的基于消息的IPC通信机制：队列和管道。队列和管道都是将数据存放于内存中队列又是基于（管道+锁）实现的，可以让我们从复杂的锁问题中解脱出来，我们应该尽量避免使用共享数据，尽可能使用消息传递和队列，避免处理复杂的同步和锁问题，而且在进程数目增多时，往往可以获得更好的可获展性。 参考资料： 1234http://www.cnblogs.com/Eva-J/articles/8253549.htmlhttp://www.cnblogs.com/linhaifeng/articles/6817679.htmlhttps://www.jianshu.com/p/1200fd49b583https://www.jianshu.com/p/aed6067eeac9]]></content>
      <categories>
        <category>Python进程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>进程同步</tag>
        <tag>进程锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之进程]]></title>
    <url>%2Fpost%2Fd3c89889.html</url>
    <content type="text"><![CDATA[什么是进程进程（Process）是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。在早期面向进程设计的计算机结构中，进程是程序的基本执行实体；在当代面向线程设计的计算机结构中，进程是线程的容器。程序是指令、数据及其组织形式的描述，进程是程序的实体。 狭义定义：进程是正在运行的程序的实例（an instance of a computer program that is being executed）。 广义定义：进程是一个具有一定独立功能的程序关于某个数据集合的一次运行活动。它是操作系统动态执行的基本单元，在传统的操作系统中，进程既是基本的分配单元，也是基本的执行单元。 进程的概念第一，进程是一个实体。每一个进程都有它自己的地址空间，一般情况下，包括文本区域（text region）、数据区域（data region）和堆栈（stack region）。文本区域存储处理器执行的代码；数据区域存储变量和进程执行期间使用的动态分配的内存；堆栈区域存储着活动过程调用的指令和本地变量。第二，进程是一个“执行中的程序”。程序是一个没有生命的实体，只有处理器赋予程序生命时（操作系统执行之），它才能成为一个活动的实体，我们称其为进程。进程是操作系统中最基本、重要的概念。是多道程序系统出现后，为了刻画系统内部出现的动态情况，描述系统内部各道程序的活动规律引进的一个概念,所有多道程序设计操作系统都建立在进程的基础上。 进程的特征动态性：进程的实质是程序在多道程序系统中的一次执行过程，进程是动态产生，动态消亡的。并发性：任何进程都可以同其他进程一起并发执行独立性：进程是一个能独立运行的基本单位，同时也是系统分配资源和调度的独立单位；异步性：由于进程间的相互制约，使进程具有执行的间断性，即进程按各自独立的、不可预知的速度向前推进结构特征：进程由程序、数据和进程控制块三部分组成。多个不同的进程可以包含相同的程序：一个程序在不同的数据集里就构成不同的进程，能得到不同的结果；但是执行过程中，程序不能发生改变。 进程与程序中的区别程序是指令和数据的有序集合，其本身没有任何运行的含义，是一个静态的概念。而进程是程序在处理机上的一次执行过程，它是一个动态的概念。程序可以作为一种软件资料长期存在，而进程是有一定生命期的。程序是永久的，进程是暂时的。 注意：同一个程序执行两次，就会在操作系统中出现两个进程，所以我们可以同时运行一个软件，分别做不同的事情也不会混乱。 进程的并行与并发并行 : 并行是指两者同时执行，比如赛跑，两个人都在不停的往前跑；（资源够用，比如三个线程，四核的CPU ） 并发 : 并发是指资源有限的情况下，两者交替轮流使用资源，比如一段路(单核CPU资源)同时只能过一个人，A走一段后，让给B，B用完继续给A ，交替使用，目的是提高效率。 区别: 并行是从微观上，也就是在一个精确的时间片刻，有不同的程序在执行，这就要求必须有多个处理器。 并发是从宏观上，在一个时间段上可以看出是同时执行的，比如一个服务器同时处理多个session。 同步异步阻塞非阻塞状态介绍 ​ 在了解其他概念之前，我们首先要了解进程的几个状态。在程序运行的过程中，由于被操作系统的调度算法控制，程序会进入几个状态：就绪，运行和阻塞。 （1）就绪(Ready)状态 当进程已分配到除CPU以外的所有必要的资源，只要获得处理机便可立即执行，这时的进程状态称为就绪状态。 （2）执行/运行（Running）状态当进程已获得处理机，其程序正在处理机上执行，此时的进程状态称为执行状态。 （3）阻塞(Blocked)状态正在执行的进程，由于等待某个事件发生而无法执行时，便放弃处理机而处于阻塞状态。引起进程阻塞的事件可有多种，例如，等待I/O完成、申请缓冲区不能满足、等待信件(信号)等。 同步和异步​ 所谓同步就是一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成，这是一种可靠的任务序列。要么成功都成功，失败都失败，两个任务的状态可以保持一致。 ​ 所谓异步是不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了。至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列。 ​ 比如我去银行办理业务，可能会有两种方式： 第一种 ：选择排队等候； 第二种 ：选择取一个小纸条上面有我的号码，等到排到我这一号时由柜台的人通知我轮到我去办理业务了； ​ 第一种：前者(排队等候)就是同步等待消息通知，也就是我要一直在等待银行办理业务情况； ​ 第二种：后者(等待别人通知)就是异步等待消息通知。在异步消息处理中，等待消息通知者(在这个例子中就是等待办理业务的人)往往注册一个回调机制，在所等待的事件被触发时由触发机制(在这里是柜台的人)通过某种机制(在这里是写在小纸条上的号码，喊号)找到等待该事件的人。 阻塞与非阻塞​ 阻塞和非阻塞这两个概念与程序（线程）等待消息通知(无所谓同步或者异步)时的状态有关。也就是说阻塞与非阻塞主要是程序（线程）等待消息通知时的状态角度来说的 ​ 继续上面的那个例子，不论是排队还是使用号码等待通知，如果在这个等待的过程中，等待者除了等待消息通知之外不能做其它的事情，那么该机制就是阻塞的，表现在程序中,也就是该程序一直阻塞在该函数调用处不能继续往下执行。相反，有的人喜欢在银行办理这些业务的时候一边打打电话发发短信一边等待，这样的状态就是非阻塞的，因为他(等待者)没有阻塞在这个消息通知上，而是一边做自己的事情一边等待。 注意：同步非阻塞形式实际上是效率低下的，想象一下你一边打着电话一边还需要抬头看到底队伍排到你了没有。如果把打电话和观察排队的位置看成是程序的两个操作的话，这个程序需要在这两种不同的行为之间来回的切换，效率可想而知是低下的；而异步非阻塞形式却没有这样的问题，因为打电话是你(等待者)的事情，而通知你则是柜台(消息触发机制)的事情，程序没有在两种不同的操作中来回切换。 同步/异步与阻塞/非阻塞同步阻塞形式​ 效率最低。拿上面的例子来说，就是你专心排队，什么别的事都不做。 异步阻塞形式​ 如果在银行等待办理业务的人采用的是异步的方式去等待消息被触发（通知），也就是领了一张小纸条，假如在这段时间里他不能离开银行做其它的事情，那么很显然，这个人被阻塞在了这个等待的操作上面； 异步操作是可以被阻塞住的，只不过它不是在处理消息时阻塞，而是在等待消息通知时被阻塞。 同步非阻塞形式​ 实际上是效率低下的。 ​ 想象一下你一边打着电话一边还需要抬头看到底队伍排到你了没有，如果把打电话和观察排队的位置看成是程序的两个操作的话，这个程序需要在这两种不同的行为之间来回的切换，效率可想而知是低下的。 异步非阻塞形式​ 效率更高，因为打电话是你(等待者)的事情，而通知你则是柜台(消息触发机制)的事情，程序没有在两种不同的操作中来回切换。 ​ 比如说，这个人突然发觉自己烟瘾犯了，需要出去抽根烟，于是他告诉大堂经理说，排到我这个号码的时候麻烦到外面通知我一下，那么他就没有被阻塞在这个等待的操作上面，自然这个就是异步+非阻塞的方式了。 很多人会把同步和阻塞混淆，是因为很多时候同步操作会以阻塞的形式表现出来，同样的，很多人也会把异步和非阻塞混淆，因为异步操作一般都不会在真正的IO操作处被阻塞。 下面来看看在python程序中的进程操作: 在python程序中的进程操作之前已经了解了很多进程相关的理论知识，了解进程是什么应该不再困难了，刚刚我们已经了解了，运行中的程序就是一个进程。所有的进程都是通过它的父进程来创建的。因此，运行起来的python程序也是一个进程，那么我们也可以在程序中再创建进程。多个进程可以实现并发效果，也就是说，当我们的程序中存在多个进程的时候，在某些时候，就会让程序的执行速度变快。以我们之前所学的知识，并不能实现创建进程这个功能，所以我们就需要借助python中强大的模块。 multiprocess模块仔细说来，multiprocess不是一个模块而是python中一个操作、管理进程的包。 之所以叫multi是取自multiple的多功能的意思,在这个包中几乎包含了和进程有关的所有子模块。由于提供的子模块非常多，为了方便大家归类记忆，我将这部分大致分为四个部分：创建进程部分，进程同步部分，进程池部分，进程之间数据共享。 multiprocess.process模块process模块介绍process模块是一个创建进程的模块，借助这个模块，就可以完成进程的创建。 123456789101112Process([group [, target [, name [, args [, kwargs]]]]])，由该类实例化得到的对象，表示一个子进程中的任务（尚未启动）强调：1. 需要使用关键字的方式来指定参数2. args指定的为传给target函数的位置参数，是一个元组形式，必须有逗号参数介绍：1. group参数未使用，值始终为None2. target表示调用对象，即子进程要执行的任务3. args表示调用对象的位置参数元组，args=(1,2,&apos;egon&apos;,)4. kwargs表示调用对象的字典,kwargs=&#123;&apos;name&apos;:&apos;egon&apos;,&apos;age&apos;:18&#125;5. name为子进程的名称 方法介绍 : p.start()：启动进程，并调用该子进程中的p.run() p.run():进程启动时运行的方法，正是它去调用target指定的函数，我们自定义类的类中一定要实现该方法 p.terminate():强制终止进程p，不会进行任何清理操作，如果p创建了子进程，该子进程就成了僵尸进程，使用该方法需要特别小心这种情况。如果p还保存了一个锁那么也将不会被释放，进而导致死锁 p.is_alive():如果p仍然运行，返回True p.join([timeout]):主线程等待p终止（强调：是主线程处于等的状态，而p是处于运行的状态）。timeout是可选的超时时间，需要强调的是，p.join只能join住start开启的进程，而不能join住run开启的进程 属性介绍 : p.daemon：默认值为False，如果设为True，代表p为后台运行的守护进程，当p的父进程终止时，p也随之终止，并且设定为True后，p不能创建自己的新进程，必须在p.start()之前设置 p.name:进程的名称 p.pid：进程的pid p.exitcode:进程在运行时为None、如果为–N，表示被信号N结束(了解即可) p.authkey:进程的身份验证键,默认是由os.urandom()随机生成的32字符的字符串。这个键的用途是为涉及网络连接的底层进程间通信提供安全性，这类连接只有在具有相同的身份验证键时才能成功（了解即可） 使用process模块创建进程在一个python进程中开启子进程，start方法和并发效果。 12345678910111213141516171819202122import osimport timefrom multiprocessing import Processdef func(args1, args2): print(args1, args2) time.sleep(3) print('子进程 :', os.getpid()) print('子进程的父进程 :', os.getppid()) print(12345)if __name__ == '__main__': p = Process(target=func, args=('参数1', '参数2')) # 注册 # p是一个进程对象,还没有启动进程 p.start() # 开启了一个子进程 # p.join() # 是感知一个子进程的结束,将异步的程序改为同步 print('我是父进程') print('*' * 10) print('父进程 :', os.getpid()) # 查看当前进程的进程号 print('父进程的父进程 :', os.getppid()) # 查看当前进程的父进程 运行结果： 上面是单个进程，那么多个进程同时运行又是怎么样的呢？一起来看看： 123456789101112131415from multiprocessing import Processdef func(filename, content): print(filename) with open(filename, 'w') as f: f.write(content * 10 * '*')if __name__ == '__main__': p_list = [] for i in range(10): p = Process(target=func, args=('info%s' % i, 0)) p_list.append(p) p.start() 这里同时启动了10个进程，并创建10文件向文件中写入内容，它们同时在10个进程中并发处理，但是它们之间的运行没有顺序 运行结果： 那我们又怎么写入内容后查看这10个文件呢？这里我们再次用到join方法 123456789101112131415161718import osfrom multiprocessing import Processdef func(filename, content): print(filename) with open(filename, 'w') as f: f.write(content * 10 * '*')if __name__ == '__main__': p_list = [] for i in range(10): p = Process(target=func, args=('info%s' % i, 0)) p_list.append(p) p.start() [p.join() for p in p_list] # 之前的所有进程必须在这里都执行完才能执行下面的代码 print([i for i in os.walk(r'C:\GavinLiu\projects\my_process')]) 除了上面这些开启进程的方法，还有一种以继承Process类的形式开启进程的方式 123456789101112131415161718192021from multiprocessing import Processclass MyProcess(Process): def __init__(self, arg1, arg2): super().__init__() self.arg1 = arg1 self.arg2 = arg2 def run(self): print(self.pid) print(self.name) print(self.arg1) print(self.arg2)if __name__ == '__main__': p1 = MyProcess(1, 2) p1.start() # start会自动调用run p2 = MyProcess(3, 4) p2.start() 运行结果： 进程之间的数据隔离问题 12345678910111213141516import osfrom multiprocessing import Processdef func(): global n # 声明了一个全局变量 n = 0 # 重新定义了一个n print('子进程: %s' % os.getpid(), n)if __name__ == '__main__': n = 100 p = Process(target=func) p.start() p.join() print('父进程: ', os.getpid(), n) 运行结果： 守护进程主进程创建守护进程 其一：守护进程会在主进程代码执行结束后就终止 其二：守护进程内无法再开启子进程,否则抛出异常：AssertionError: daemonic processes are not allowed to have children 注意：进程之间是互相独立的，主进程代码运行结束，守护进程随即终止 123456789101112131415161718192021import osimport timefrom multiprocessing import Processclass Myprocess(Process): def __init__(self, person): super().__init__() self.person = person def run(self): print(os.getpid(), self.name) print('%s正在和女主播聊天' % self.person)if __name__ == '__main__': p = Myprocess('少林') p.daemon = True # 一定要在p.start()前设置,设置p为守护进程,禁止p创建子进程,并且父进程代码执行结束,p即终止运行 p.start() time.sleep(10) # 在sleep时查看进程id对应的进程ps -ef|grep id(linux下) print('主') 运行结果： 12345678910111213141516171819202122232425import timefrom multiprocessing import Processdef foo(): print(123) time.sleep(1) print("end123")def bar(): print(456) time.sleep(3) print("end456")if __name__ == '__main__': p1 = Process(target=foo) p2 = Process(target=bar) p1.daemon = True p1.start() p2.start() time.sleep(0.1) print("main-------") # 打印该行则主进程代码结束,则守护进程p1应该被终止.#可能会有p1任务执行的打印信息123,因为主进程打印main----时,p1也执行了,但是随即被终止. 运行结果： 参考资料： 1234http://www.cnblogs.com/Eva-J/articles/8253549.htmlhttp://www.cnblogs.com/linhaifeng/articles/6817679.htmlhttps://www.jianshu.com/p/1200fd49b583https://www.jianshu.com/p/aed6067eeac9]]></content>
      <categories>
        <category>Python进程</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django之中间件]]></title>
    <url>%2Fpost%2F11a0df73.html</url>
    <content type="text"><![CDATA[Django中间件中间件介绍什么是中间件?官方的说法：中间件是一个用来处理Django的请求和响应的框架级别的钩子。它是一个轻量、低级别的插件系统，用于在全局范围内改变Django的输入和输出。每个中间件组件都负责做一些特定的功能。 但是由于其影响的是全局，所以需要谨慎使用，使用不当会影响性能。 说的直白一点中间件是帮助我们在视图函数执行之前和执行之后都可以做一些额外的操作，它本质上就是一个自定义类，类中定义了几个方法，Django框架会在请求的特定的时间去执行这些方法。 我们一直都在使用中间件，只是没有注意到而已，打开Django项目的settings.py文件，看到下图的MIDDLEWARE配置项。 123456789MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',] MIDDLEWARE配置项是一个列表，列表中是一个个字符串，这些字符串其实是一个个类，也就是一个个中间件。 我们之前已经接触过一个csrf相关的中间件了？我们一开始把他注释掉，再提交post请求的时候，就不会被forbidden了，后来学会使用csrf_token之后就不再注释这个中间件了。 那接下来就学习中间件中的方法以及这些方法什么时候被执行。 自定义中间件中间件可以定义五个方法，分别是：（主要的是process_request和process_response） process_request(self,request) process_view(self, request, view_func, view_args, view_kwargs) process_template_response(self,request,response) process_exception(self, request, exception) process_response(self, request, response) 以上方法的返回值可以是None或一个HttpResponse对象，如果是None，则继续按照django定义的规则向后继续执行，如果是HttpResponse对象，则直接将该对象返回给用户。 views.py中 : 123def index(request): print('index视图') return HttpResponse('OK') 下面我们来一一说明中间件的五个方法: process_request先来撸一段代码 12345678910111213from django.utils.deprecation import MiddlewareMixinclass MD1(MiddlewareMixin): def process_request(self, request): print('MD1里面的 process_request')class MD2(MiddlewareMixin): def process_request(self, request): print('MD2里面的 process_request') pass 在settings.py的MIDDLEWARE配置项中注册上述两个自定义中间件： 1234567891011MIDDLEWARE = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', 'middlewares.MD1', # 自定义中间件MD1 'middlewares.MD2' # 自定义中间件MD2] 完成以上操作后,我们运行一下项目并访问某个视图: 123MD1里面的 process_requestMD2里面的 process_requestindex视图 再把MD1和MD2在settings.py中的位置调换一下，再访问一个视图，会发现终端中打印的内容如下： 123MD2里面的 process_requestMD1里面的 process_requestindex视图 看结果我们知道：视图函数还是最后执行的，MD2比MD1先执行自己的process_request方法。 在打印一下两个自定义中间件中process_request方法中的request参数，会发现它们是同一个对象。 由此总结一下： 中间件的process_request方法是在执行视图函数之前执行的。 当配置多个中间件时，会按照MIDDLEWARE中的注册顺序，也就是列表的索引值，从前到后依次执行的。（在settings.py里面设置中 从上到下的顺序） 不同中间件之间传递的request都是同一个对象 返回None，继续执行后续的中间件的process_request方法，返回response , 不执行后续的中间件的process_request方法 process_responseprocess_response 有两个参数，一个是request，一个是response，request就是上述例子中一样的对象，response是视图函数返回的HttpResponse对象。该方法的返回值也必须是HttpResponse对象。 给上述的MD1和MD2加上process_response方法： 123456789101112131415161718192021from django.utils.deprecation import MiddlewareMixinclass MD1(MiddlewareMixin): def process_request(self, request): print('MD1里面的 process_request') def process_response(self, request, response): print('MD1里面的 process_response') return responseclass MD2(MiddlewareMixin): def process_request(self, request): print('MD2里面的 process_request') pass def process_response(self, request, response): print('MD2里面的 process_response') return response 访问一个视图，看一下终端的输出： 12345MD2里面的 process_requestMD1里面的 process_requestindex视图MD1里面的 process_responseMD2里面的 process_response 看结果可知：process_response方法是在视图函数之后执行的，并且顺序是MD1比MD2先执行。(此时settings.py中 MD2比MD1先注册) 由此总结一下： 多个中间件中的process_response方法是按照MIDDLEWARE中的注册顺序倒序执行的（在settings.py里面设置中 从下到上的顺序） 在请求有响应的时候执行process_response方法 该方法的返回值也必须是HttpResponse对象 process_viewprocess_view(self, request, view_func, view_args, view_kwargs) 该方法有四个参数 request是HttpRequest对象。 view_func是Django即将使用的视图函数。 （它是实际的函数对象，而不是函数的名称作为字符串。） view_args是将传递给视图的位置参数的列表. view_kwargs是将传递给视图的关键字参数的字典。 view_args和view_kwargs都不包含第一个视图参数（request）。 Django会在调用视图函数之前调用process_view方法。 它应该返回None或一个HttpResponse对象。 如果返回None，Django将继续处理这个请求，执行任何其他中间件的process_view方法，然后在执行相应的视图。 如果它返回一个HttpResponse对象，Django不会调用适当的视图函数。 它将执行中间件的process_response方法并将应用到该HttpResponse并返回结果。 给MD1和MD2添加process_view方法: 12345678910111213141516171819202122232425262728293031from django.utils.deprecation import MiddlewareMixinclass MD1(MiddlewareMixin): def process_request(self, request): print('MD1里面的 process_request') def process_response(self, request, response): print('MD1里面的 process_response') return response def process_view(self, request, view_func, view_args, view_kwargs): print('-' * 80) print('MD1 中的process_view') print(view_func, view_func.__name__)class MD2(MiddlewareMixin): def process_request(self, request): print('MD2里面的 process_request') pass def process_response(self, request, response): print('MD2里面的 process_response') return response def process_view(self, request, view_func, view_args, view_kwargs): print('-' * 80) print('MD2 中的process_view') print(view_func, view_func.__name__) 访问index视图函数，看一下输出结果： 1234567891011MD2里面的 process_requestMD1里面的 process_request--------------------------------------------------------------------------------MD2 中的process_view&lt;function index at 0x000001DE68317488&gt; index--------------------------------------------------------------------------------MD1 中的process_view&lt;function index at 0x000001DE68317488&gt; indexindex视图MD1里面的 process_responseMD2里面的 process_response process_view方法是在process_request之后，视图函数之前执行的，执行顺序按照MIDDLEWARE中的注册顺序从前到后顺序执行的，返回None，继续执行后续的中间件的process_view方法，返回response , 不执行后续的中间件的process_view方法。 process_exceptionprocess_exception(self, request, exception) 该方法两个参数: 一个HttpRequest对象 一个exception是视图函数异常产生的Exception对象。 这个方法只有在视图函数中出现异常了才执行，它返回的值可以是一个None也可以是一个HttpResponse对象。如果是HttpResponse对象，Django将调用模板和中间件中的process_response方法，并返回给浏览器，否则将默认处理异常。如果返回一个None，则交给下一个中间件的process_exception方法来处理异常。它的执行顺序也是按照中间件注册顺序的倒序执行。 给MD1和MD2添加上这个方法： 123456789101112131415161718192021222324252627282930313233343536373839from django.utils.deprecation import MiddlewareMixinclass MD1(MiddlewareMixin): def process_request(self, request): print('MD1里面的 process_request') def process_response(self, request, response): print('MD1里面的 process_response') return response def process_view(self, request, view_func, view_args, view_kwargs): print('-' * 80) print('MD1 中的process_view') print(view_func, view_func.__name__) def process_exception(self, request, exception): print(exception) print('MD1 中的process_exception')class MD2(MiddlewareMixin): def process_request(self, request): print('MD2里面的 process_request') pass def process_response(self, request, response): print('MD2里面的 process_response') return response def process_view(self, request, view_func, view_args, view_kwargs): print('-' * 80) print('MD2 中的process_view') print(view_func, view_func.__name__) def process_exception(self, request, exception): print(exception) print('MD2 中的process_exception') 如果视图函数中无异常，process_exception方法不执行。 想办法，在视图函数中抛出一个异常： 1234def index(request): print('index视图') raise ValueError('呵呵') return HttpResponse('OK') 在MD1的process_exception中返回一个响应对象： 123456789101112131415161718class MD1(MiddlewareMixin): def process_request(self, request): print('MD1里面的 process_request') def process_response(self, request, response): print('MD1里面的 process_response') return response def process_view(self, request, view_func, view_args, view_kwargs): print('-' * 80) print('MD1 中的process_view') print(view_func, view_func.__name__) def process_exception(self, request, exception): print(exception) print('MD1 中的process_exception') return HttpResponse(str(exception)) # 返回一个响应对象 看输出结果： 12345678910111213MD2里面的 process_requestMD1里面的 process_request--------------------------------------------------------------------------------MD2 中的process_view&lt;function index at 0x0000022C09727488&gt; index--------------------------------------------------------------------------------MD1 中的process_view&lt;function index at 0x0000022C09727488&gt; indexindex视图呵呵MD1 中的process_exceptionMD1里面的 process_responseMD2里面的 process_response 注意，这里并没有执行MD2的process_exception方法，因为MD1中的process_exception方法直接返回了一个响应对象。 process_template_responseprocess_template_response(self, request, response) 它的参数，一个HttpRequest对象，response是TemplateResponse对象（由视图函数或者中间件产生）。 process_template_response是在视图函数执行完成后立即执行，但是它有一个前提条件，那就是视图函数返回的对象有一个render()方法（或者表明该对象是一个TemplateResponse对象或等价方法）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class MD1(MiddlewareMixin): def process_request(self, request): print('MD1里面的 process_request') def process_response(self, request, response): print('MD1里面的 process_response') return response def process_view(self, request, view_func, view_args, view_kwargs): print('-' * 80) print('MD1 中的process_view') print(view_func, view_func.__name__) def process_exception(self, request, exception): print(exception) print('MD1 中的process_exception') return HttpResponse(str(exception)) def process_template_response(self, request, response): print('MD1 中的process_template_response') return responseclass MD2(MiddlewareMixin): def process_request(self, request): print('MD2里面的 process_request') pass def process_response(self, request, response): print('MD2里面的 process_response') return response def process_view(self, request, view_func, view_args, view_kwargs): print('-' * 80) print('MD2 中的process_view') print(view_func, view_func.__name__) def process_exception(self, request, exception): print(exception) print('MD2 中的process_exception') def process_template_response(self, request, response): print('MD2 中的process_template_response') return response 修改views.py中代码 : 123456789def index(request): print('index视图') def render(): print('render') return HttpResponse('render') rep = HttpResponse('OK') rep.render = render return rep 访问index视图，终端输出的结果： 1234567891011121314MD2里面的 process_requestMD1里面的 process_request--------------------------------------------------------------------------------MD2 中的process_view&lt;function index at 0x000001C111B97488&gt; index--------------------------------------------------------------------------------MD1 中的process_view&lt;function index at 0x000001C111B97488&gt; indexindex视图MD1 中的process_template_responseMD2 中的process_template_responserenderMD1里面的 process_responseMD2里面的 process_response 从结果看出： 视图函数执行完之后，立即执行了中间件的process_template_response方法，顺序是倒序，先执行MD1的，在执行MD2的，接着执行了视图函数返回的HttpResponse对象的render方法，返回了一个新的HttpResponse对象，接着执行中间件的process_response方法。 中间件的执行流程上面我们一一了解了中间件中的5个方法，它们的参数、返回值以及什么时候执行，现在总结一下中间件的执行流程。 请求到达中间件之后，先按照正序执行每个注册中间件的process_reques方法，process_request方法返回的值是None，就依次执行，如果返回的值是HttpResponse对象，不再执行后面的process_request方法，而是执行当前对应中间件的process_response方法，将HttpResponse对象返回给浏览器。也就是说：如果MIDDLEWARE中注册了6个中间件，执行过程中，第3个中间件返回了一个HttpResponse对象，那么第4,5,6中间件的process_request和process_response方法都不执行，顺序执行3,2,1中间件的process_response方法。 process_request方法都执行完后，匹配路由，找到要执行的视图函数，先不执行视图函数，先执行中间件中的process_view方法，process_view方法返回None，继续按顺序执行，所有process_view方法执行完后执行视图函数。假如中间件3 的process_view方法返回了HttpResponse对象，则4,5,6的process_view以及视图函数都不执行，直接从最后一个中间件，也就是中间件6的process_response方法开始倒序执行。 process_template_response和process_exception两个方法的触发是有条件的，执行顺序也是倒序。总结所有的执行流程如下： Django请求流程图]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django(二)之模型]]></title>
    <url>%2Fpost%2F2116a065.html</url>
    <content type="text"><![CDATA[本文档使用Django 2.x，Django是一个基于MVC架构的Web框架，MVC架构要追求的是模型和视图的解耦合，而其中的模型说得更直白一些就是数据，所以通常也被称作数据模型。在实际的项目中，数据模型通常通过数据库实现持久化操作，而关系型数据库在很长一段时间都是持久化的首选方案，在我们的OA项目中，我们选择使用MySQL来实现数据持久化。 配置关系型数据库MySQL 进入oa文件夹，修改项目的settings.py文件，首先将我们之前创建的应用hrs添加已安装的项目中，然后配置MySQL作为持久化方案。 12(venv)$ cd oa(venv)$ vim settings.py 123456789101112131415161718192021222324# 此处省略上面的代码INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'hrs',]DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'oa', 'HOST': 'localhost', 'PORT': 3306, 'USER': 'root', 'PASSWORD': '123456', &#125;&#125;# 此处省略下面的代码 在配置ENGINE属性时，常用的可选值包括： &#39;django.db.backends.sqlite3&#39;：SQLite嵌入式数据库 &#39;django.db.backends.postgresql&#39;：BSD许可证下发行的开源关系型数据库产品 &#39;django.db.backends.mysql&#39;：转手多次目前属于甲骨文公司的经济高效的数据库产品 &#39;django.db.backends.oracle&#39;：甲骨文公司的旗舰关系型数据库产品 其他的配置可以参考官方文档中数据库配置的部分。 NAME属性代表数据库的名称，如果使用SQLite它对应着一个文件，在这种情况下NAME的属性值应该是一个绝对路径。如果使用其他关系型数据库，还要配置对应的HOST（主机）、PORT（端口）、USER（用户名）、PASSWORD（口令）等属性。 安装MySQL客户端工具，Python 3中使用PyMySQL，Python 2中用MySQLdb。 1(venv)$ pip install pymysql 如果使用Python 3需要修改项目的__init__.py文件并加入如下所示的代码，这段代码的作用是将PyMySQL视为MySQLdb来使用，从而避免Django找不到连接MySQL的客户端工具而询问你：“Did you install mysqlclient? ”（你安装了mysqlclient吗？）。 123import pymysqlpymysql.install_as_MySQLdb() 运行manage.py并指定migrate参数实现数据库迁移，为应用程序创建对应的数据表，当然在此之前需要先启动MySQL数据库服务器并创建名为oa的数据库，在MySQL中创建数据库的语句如下所示。 12drop database if exists oa;create database oa default charset utf8; 12345678910111213141516171819(venv)$ cd ..(venv)$ python manage.py migrateOperations to perform: Apply all migrations: admin, auth, contenttypes, sessionsRunning migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying auth.0009_alter_user_last_name_max_length... OK Applying sessions.0001_initial... OK 可以看到，Django帮助我们创建了10张表，这些都是使用Django框架需要的东西，稍后我们就会用到这些表。除此之外，我们还应该为我们自己的应用创建数据模型。如果要在hrs应用中实现对部门和员工的管理，我们可以创建如下所示的数据模型。 12(venv)$ cd hrs(venv)$ vim models.py 12345678910111213141516171819202122232425262728from django.db import modelsclass Dept(models.Model): """部门类""" no = models.IntegerField(primary_key=True, db_column='dno', verbose_name='部门编号') name = models.CharField(max_length=20, db_column='dname', verbose_name='部门名称') location = models.CharField(max_length=10, db_column='dloc', verbose_name='部门所在地') class Meta: db_table = 'tb_dept'class Emp(models.Model): """员工类""" no = models.IntegerField(primary_key=True, db_column='eno', verbose_name='员工编号') name = models.CharField(max_length=20, db_column='ename', verbose_name='员工姓名') job = models.CharField(max_length=10, verbose_name='职位') # 自参照完整性多对一外键关联 mgr = models.ForeignKey('self', on_delete=models.SET_NULL, null=True, blank=True, verbose_name='主管编号') sal = models.DecimalField(max_digits=7, decimal_places=2, verbose_name='月薪') comm = models.DecimalField(max_digits=7, decimal_places=2, null=True, blank=True, verbose_name='补贴') dept = models.ForeignKey(Dept, db_column='dno', on_delete=models.PROTECT, verbose_name='所在部门') class Meta: db_table = 'tb_emp' 说明：上面定义模型时使用了字段类及其属性，其中IntegerField对应数据库中的integer类型，CharField对应数据库的varchar类型，DecimalField对应数据库的decimal类型，ForeignKey用来建立多对一外键关联。字段属性primary_key用于设置主键，max_length用来设置字段的最大长度，db_column用来设置数据库中与字段对应的列，verbose_name则设置了Django后台管理系统中该字段显示的名称。如果对这些东西感到很困惑也不要紧，文末提供了字段类、字段属性、元数据选项等设置的相关说明，不清楚的读者可以稍后查看对应的参考指南。 通过模型创建数据表。 1234567891011(venv)$ cd ..(venv)$ python manage.py makemigrations hrsMigrations for 'hrs': hrs/migrations/0001_initial.py - Create model Dept - Create model Emp(venv)$ python manage.py migrateOperations to perform: Apply all migrations: admin, auth, contenttypes, hrs, sessionsRunning migrations: Applying hrs.0001_initial... OK 执行完数据迁移操作之后，可以在通过图形化的MySQL客户端工具查看到E-R图（实体关系图）。 在后台管理模型 创建超级管理员账号。 123456(venv)$ python manage.py createsuperuserUsername (leave blank to use 'hao'): jackfruedEmail address: jackfrued@126.comPassword: Password (again): Superuser created successfully. 启动Web服务器，登录后台管理系统。 1(venv)$ python manage.py runserver 访问http://127.0.0.1:8000/admin，会来到如下图所示的登录界面。 登录后进入管理员操作平台。 至此我们还没有看到之前创建的模型类，需要在应用的admin.py文件中模型进行注册。 注册模型类。 12(venv)$ cd hrs(venv)$ vim admin.py 123456from django.contrib import adminfrom hrs.models import Emp, Deptadmin.site.register(Dept)admin.site.register(Emp) 注册模型类后，就可以在后台管理系统中看到它们。 对模型进行CRUD操作。 可以在管理员平台对模型进行C（新增）R（查看）U（更新）D（删除）操作，如下图所示。 添加新的部门。 查看所有部门。 更新和删除部门。 注册模型管理类。 再次修改admin.py文件，通过注册模型管理类，可以在后台管理系统中更好的管理模型。 12345678910111213141516171819from django.contrib import adminfrom hrs.models import Emp, Deptclass DeptAdmin(admin.ModelAdmin): list_display = ('no', 'name', 'location') ordering = ('no', )class EmpAdmin(admin.ModelAdmin): list_display = ('no', 'name', 'job', 'mgr', 'sal', 'comm', 'dept') search_fields = ('name', 'job')admin.site.register(Dept, DeptAdmin)admin.site.register(Emp, EmpAdmin) 为了更好的查看模型数据，可以为Dept和Emp两个模型类添加__str__魔法方法。 1234567891011121314151617181920212223242526272829from django.db import modelsclass Dept(models.Model): """部门类""" # 此处省略上面的代码 def __str__(self): return self.name # 此处省略下面的代码class Emp(models.Model): """员工类""" # 此处省略上面的代码 mgr = models.ForeignKey('self', on_delete=models.SET_NULL, null=True, blank=True, verbose_name='直接主管') # 此处省略下面的代码 # 此处省略上面的代码 def __str__(self): return self.name # 此处省略下面的代码 修改代码后刷新查看Emp模型的页面，效果如下图所示。 使用ORM完成模型的CRUD操作在了解了Django提供的模型管理平台之后，我们来看看如何从代码层面完成对模型的CRUD（Create / Read / Update / Delete）操作。我们可以通过manage.py开启Shell交互式环境，然后使用Django内置的ORM框架对模型进行CRUD操作。 1234567(venv)$ cd ..(venv)$ python manage.py shellPython 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwinType "help", "copyright", "credits" or "license" for more information.(InteractiveConsole)&gt;&gt;&gt; 新增1234&gt;&gt;&gt;&gt;&gt;&gt; from hrs.models import Dept, Emp&gt;&gt;&gt; dept = Dept(40, '研发2部', '深圳')&gt;&gt;&gt; dept.save() 更新123&gt;&gt;&gt;&gt;&gt;&gt; dept.name = '研发3部'&gt;&gt;&gt; dept.save() 查询查询所有对象。 123&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.all()&lt;QuerySet [&lt;Dept: 研发1部&gt;, &lt;Dept: 销售1部&gt;, &lt;Dept: 运维1部&gt;, &lt;Dept: 研发3部&gt;]&gt; 过滤数据。 123456789101112&gt;&gt;&gt; &gt;&gt;&gt; Dept.objects.filter(name='研发3部') # 查询部门名称为“研发3部”的部门&lt;QuerySet [&lt;Dept: 研发3部&gt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.filter(name__contains='研发') # 查询部门名称包含“研发”的部门(模糊查询)&lt;QuerySet [&lt;Dept: 研发1部&gt;, &lt;Dept: 研发3部&gt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.filter(no__gt=10).filter(no__lt=40) # 查询部门编号大于10小于40的部门&lt;QuerySet [&lt;Dept: 销售1部&gt;, &lt;Dept: 运维1部&gt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.filter(no__range=(10, 30)) # 查询部门编号在10到30之间的部门&lt;QuerySet [&lt;Dept: 研发1部&gt;, &lt;Dept: 销售1部&gt;, &lt;Dept: 运维1部&gt;]&gt; 查询单个对象。 123456789&gt;&gt;&gt; &gt;&gt;&gt; Dept.objects.get(pk=10)&lt;Dept: 研发1部&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.get(no=20)&lt;Dept: 销售1部&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.get(no__exact=30)&lt;Dept: 运维1部&gt; 排序数据。 123456&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.order_by('no') # 查询所有部门按部门编号升序排列&lt;QuerySet [&lt;Dept: 研发1部&gt;, &lt;Dept: 销售1部&gt;, &lt;Dept: 运维1部&gt;, &lt;Dept: 研发3部&gt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.order_by('-no') # 查询所有部门按部门编号降序排列&lt;QuerySet [&lt;Dept: 研发3部&gt;, &lt;Dept: 运维1部&gt;, &lt;Dept: 销售1部&gt;, &lt;Dept: 研发1部&gt;]&gt; 切片数据。 123456&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.order_by('no')[0:2] # 按部门编号排序查询1~2部门&lt;QuerySet [&lt;Dept: 研发1部&gt;, &lt;Dept: 销售1部&gt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.order_by('no')[2:4] # 按部门编号排序查询3~4部门&lt;QuerySet [&lt;Dept: 运维1部&gt;, &lt;Dept: 研发3部&gt;]&gt; 高级查询。 123456789&gt;&gt;&gt;&gt;&gt;&gt; Emp.objects.filter(dept__no=10) # 根据部门编号查询该部门的员工&lt;QuerySet [&lt;Emp: 乔峰&gt;, &lt;Emp: 张无忌&gt;, &lt;Emp: 张三丰&gt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt; Emp.objects.filter(dept__name__contains='销售') # 查询名字包含“销售”的部门的员工&lt;QuerySet [&lt;Emp: 黄蓉&gt;]&gt;&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.get(pk=10).emp_set.all() # 通过部门反查部门所有的员工&lt;QuerySet [&lt;Emp: 乔峰&gt;, &lt;Emp: 张无忌&gt;, &lt;Emp: 张三丰&gt;]&gt; 说明1：由于员工与部门之间存在多对一外键关联，所以也能通过部门反向查询该部门的员工（从一对多关系中“一”的一方查询“多”的一方），反向查询属性默认的名字是类名小写_set（如上面例子中的emp_set），当然也可以在创建模型时通过ForeingKey的related_name属性指定反向查询属性的名字。如果不希望执行反向查询可以将related_name属性设置为&#39;+&#39;或以&#39;+&#39;开头的字符串。 说明2：查询多个对象的时候返回的是QuerySet对象，QuerySet使用了惰性查询，即在创建QuerySet对象的过程中不涉及任何数据库活动，等真正用到对象时（求值QuerySet）才向数据库发送SQL语句并获取对应的结果，这一点在实际开发中需要引起注意！ 说明3：可以在QuerySet上使用update()方法一次更新多个对象。 删除123&gt;&gt;&gt;&gt;&gt;&gt; Dept.objects.get(pk=40).delete()(1, &#123;'hrs.Dept': 1&#125;) Django模型最佳实践 正确的模型命名和关系字段命名。 设置适当的related_name属性。 用OneToOneField代替ForeignKeyField(unique=True)。 通过“迁移操作”（migrate）来添加模型。 用NoSQL来应对需要降低范式级别的场景。 如果布尔类型可以为空要使用NullBooleanField。 在模型中放置业务逻辑。 用&lt;ModelName&gt;.DoesNotExists取代ObjectDoesNotExists。 在数据库中不要出现无效数据。 不要对QuerySet调用len()函数。 将QuerySet的exists()方法的返回值用于if条件。 用DecimalField来存储货币相关数据而不是FloatField。 定义__str__方法。 不要将数据文件放在同一个目录中。 说明：以上内容来自于STEELKIWI网站的Best Practice working with Django models in Python，有兴趣的小伙伴可以阅读原文。 模型定义参考字段对字段名称的限制 字段名不能是Python的保留字，否则会导致语法错误 字段名不能有多个连续下划线，否则影响ORM查询操作 Django模型字段类 字段类 说明 AutoField 自增ID字段 BigIntegerField 64位有符号整数 BinaryField 存储二进制数据的字段，对应Python的bytes类型 BooleanField 存储True或False CharField 长度较小的字符串 DateField 存储日期，有auto_now和auto_now_add属性 DateTimeField 存储日期和日期，两个附加属性同上 DecimalField 存储固定精度小数，有max_digits（有效位数）和decimal_places（小数点后面）两个必要的参数 DurationField 存储时间跨度 EmailField 与CharField相同，可以用EmailValidator验证 FileField 文件上传字段 FloatField 存储浮点数 ImageField 其他同FileFiled，要验证上传的是不是有效图像 IntegerField 存储32位有符号整数。 GenericIPAddressField 存储IPv4或IPv6地址 NullBooleanField 存储True、False或null值 PositiveIntegerField 存储无符号整数（只能存储正数） SlugField 存储slug（简短标注） SmallIntegerField 存储16位有符号整数 TextField 存储数据量较大的文本 TimeField 存储时间 URLField 存储URL的CharField UUIDField 存储全局唯一标识符 字段属性通用字段属性 选项 说明 null 数据库中对应的字段是否允许为NULL，默认为False blank 后台模型管理验证数据时，是否允许为NULL，默认为False choices 设定字段的选项，各元组中的第一个值是设置在模型上的值，第二值是人类可读的值 db_column 字段对应到数据库表中的列名，未指定时直接使用字段的名称 db_index 设置为True时将在该字段创建索引 db_tablespace 为有索引的字段设置使用的表空间，默认为DEFAULT_INDEX_TABLESPACE default 字段的默认值 editable 字段在后台模型管理或ModelForm中是否显示，默认为True error_messages 设定字段抛出异常时的默认消息的字典，其中的键包括null、blank、invalid、invalid_choice、unique和unique_for_date help_text 表单小组件旁边显示的额外的帮助文本。 primary_key 将字段指定为模型的主键，未指定时会自动添加AutoField用于主键，只读。 unique 设置为True时，表中字段的值必须是唯一的 verbose_name 字段在后台模型管理显示的名称，未指定时使用字段的名称 ForeignKey属性 limit_choices_to：值是一个Q对象或返回一个Q对象，用于限制后台显示哪些对象。 related_name：用于获取关联对象的关联管理器对象（反向查询），如果不允许反向，该属性应该被设置为&#39;+&#39;，或者以&#39;+&#39;结尾。 to_field：指定关联的字段，默认关联对象的主键字段。 db_constraint：是否为外键创建约束，默认值为True。 on_delete：外键关联的对象被删除时对应的动作，可取的值包括django.db.models中定义的： CASCADE：级联删除。 PROTECT：抛出ProtectedError异常，阻止删除引用的对象。 SET_NULL：把外键设置为null，当null属性被设置为True时才能这么做。 SET_DEFAULT：把外键设置为默认值，提供了默认值才能这么做。 ManyToManyField属性 symmetrical：是否建立对称的多对多关系。 through：指定维持多对多关系的中间表的Django模型。 throughfields：定义了中间模型时可以指定建立多对多关系的字段。 db_table：指定维持多对多关系的中间表的表名。 模型元数据选项 选项 说明 abstract 设置为True时模型是抽象父类 app_label 如果定义模型的应用不在INSTALLED_APPS中可以用该属性指定 db_table 模型使用的数据表名称 db_tablespace 模型使用的数据表空间 default_related_name 关联对象回指这个模型时默认使用的名称，默认为_set get_latest_by 模型中可排序字段的名称。 managed 设置为True时，Django在迁移中创建数据表并在执行flush管理命令时把表移除 order_with_respect_to 标记对象为可排序的 ordering 对象的默认排序 permissions 创建对象时写入权限表的额外权限 default_permissions 默认为(&#39;add&#39;, &#39;change&#39;, &#39;delete&#39;) unique_together 设定组合在一起时必须独一无二的字段名 index_together 设定一起建立索引的多个字段名 verbose_name 为对象设定人类可读的名称 verbose_name_plural 设定对象的复数名称 查询参考按字段查找可以用的条件： exact / iexact：精确匹配/忽略大小写的精确匹配查询 contains / icontains / startswith / istartswith / endswith / iendswith：基于like的模糊查询 in：集合运算 gt / gte / lt / lte：大于/大于等于/小于/小于等于关系运算 range：指定范围查询（SQL中的between…and…） year / month / day / week_day / hour / minute / second：查询时间日期 isnull：查询空值（True）或非空值（False） search：基于全文索引的全文检索 regex / iregex：基于正则表达式的模糊匹配查询 Q对象（用于执行复杂查询）的使用： 1234567&gt;&gt;&gt;&gt;&gt;&gt; from django.db.models import Q&gt;&gt;&gt; Emp.objects.filter(... Q(name__startswith='张'),... Q(sal__gte=5000) | Q(comm__gte=1000)... ) # 查询名字以“张”开头 工资大于等于5000或补贴大于等于1000的员工&lt;QuerySet [&lt;Emp: 张三丰&gt;]&gt;]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django(一)]]></title>
    <url>%2Fpost%2Fe8c9ba97.html</url>
    <content type="text"><![CDATA[编写自己的第一个Django应用 创建项目 Windows环境下 1234567891011121314151617181920### Windows环境下# 创建根目录也就是你项目的容器$ mkdir mysite# 切换到根目录$ cd mysite/# 建虚拟环境$ python -m venv venv# 进入Scripts目录$ cd venv$ cd Scripts# 激活环境$ activate# 在虚拟环境下安装django依赖库(venv)$ pip install django# 查看django版本(venv)$ django-admin --version# 切换到根目录(venv)$ cd ../..# 创建项目(venv)$ django-admin startproject mysite . Linux和macOS 环境 1234567891011121314151617## Linux和macOS 环境# 创建项目文件夹$ mkdir mysite# 切换到项目目录$ cd mysite/# 使用venv模块创建虚拟环境，目录名venv$ python3 -m venv venv# 激活虚拟环境$ source venv/bin/activate# 更新pip到最新版本(venv)$ python -m pip install --upgrade pip# 使用pip安装django(venv)$ pip install django# 通过安装django时安装的脚本工具django-admin检查django版本(venv)$ django-admin --version# 开启新项目(venv)$ django-admin startproject mysite . 好了到现在为止,我们已经创建好了自己的一个Django项目,让我们看看 startproject 创建了些什么: 1234567mysite/ manage.py mysite/ __init__.py settings.py urls.py wsgi.py 下面我们简单的介绍上面每个文件是什么有什么作用 manage.py:用各种方式管理 Django 项目的命令行工具 __init__.py:一个空文件，告诉 Python 这个目录应该被认为是一个 Python 包 settings.py:整个Django 项目的配置文件 urls.py:就像是地图,负责把URL模式映射到应用程序 wsgi.py:用于项目部署 启动服务器 1(venv)$ python manage.py runserver 你刚刚启动的是 Django 自带的用于开发的简易服务器，它是一个用纯 Python 写的轻量级的 Web 服务器。 现在我们用浏览器访问以下https://127.0.0.1:8000/，如果看到了小火箭那么恭喜你已经成功访问项目 Django默认端口是8000，如果你想更换服务器的监听端口，那么请运行python manage.py runserver 8080,如果你想在本地访问你服务器上的项目那么你需要执行python manage.py runserver 0:8000,python manage.py runserver 0.0.0.0:8000,这里的0相当于0.0.0.0 创建应用 一个项目下我们可以有多个应用,现在我们来创建自己的第一个应用 1(venv)$ python manage.py startapp hrs 下面我们来看看startapp命令创建了些什么: 123456789hrs/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py views.py __init__.py: 一个空文件,告诉Python这个目录应该被认为是一个包 admin.py: 可以用来注册模型,让Django自动创建管理界面 apps.py: 当前应用的配置 migrations: 存放与模型有关的数据库信息 __init__.py: 也是一个空文件,告诉Python这个目录应该被认为是一个包 models.py:存放应用的数据模型,即实体类及其之间的关系(MVC/MVT中的M) tests.py: 测试应用的各种测试函数 views.py: 处理请求并返回响应的函数(MVC中的C,MVT中的V) 创建一个视图 hrs/views.py 123from django.http import HttpResponsedef home(request): return HttpResponse('&lt;h1&gt;Hello, Django!&lt;/h1&gt;') 完成后,到项目目录,修改该目录下的urls.py文件 12345678from django.contrib import adminfrom django.urls import pathfrom hrs import viewsurlpatterns = [ path('hrs/', views.home), path('admin/', admin.site.urls),] 注意:我们还可以为应用单独建立一个urls.py文件来映射请求的URL,如下面: 在hrs引用下创建一个urls.py 123456from django.urls import pathfrom hrs import viewsurlpatterns = [ path('', views.home, name='index'),] 那么相应的我们在项目的urls.py文件中应当做出改变: 1234567from django.contrib import adminfrom django.urls import path, includeurlpatterns = [ path('hrs/', include('hrs.urls')), path('admin/', admin.site.urls),] 使用视图模板 在manage.py所在的同级目录下创建一个templates文件夹 接下来便是创建一个个模板页面,在这里我创建一个index.html模板页面 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;首页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;&#123;&#123; greeting &#125;&#125;&lt;/h1&gt; &lt;h2&gt;&#123;&#123; current_time &#125;&#125;&lt;/h2&gt; &lt;hr&gt; 今天为你推荐&#123;&#123; num &#125;&#125;种水果: &lt;ul&gt; &#123;% for fruit in fruits %&#125; &lt;li&gt;&#123;&#123; fruit &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt;&lt;/body&gt;&lt;/html&gt; 要使用模板我们要改相应的配置,切换到配置文件目录找到setting.py文件 我们需要修改TEMPLATES的配置,代码如下所示: 123456789101112131415TEMPLATES = [ &#123; 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')], # 将DIRS路径配置成模板页面所在的路径 'APP_DIRS': True, 'OPTIONS': &#123; 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], &#125;, &#125;,] 要渲染模板我们需要在views.py中传递参数 12345678910111213141516171819202122from datetime import datetimefrom random import randrangefrom django.shortcuts import renderdef home(req): fruit = ['大苹果', '水晶葡萄', '大西瓜', '石榴', '桃子', '李子', '梨子'] len_f = randrange(1, len(fruit)) fruits = set() for _ in range(len_f): index = randrange(0, len(fruit)) fruits.add(fruit[index]) ctx = &#123; 'greeting': '你好,世界', 'current_time': datetime.now, 'num': len(fruits), 'fruits': fruits, &#125; return render(req, 'index.html', ctx) 到这里我们完成了一个简单的Django项目,现在我们将项目重新运行查看一下结果 1(venv)$ python manage.py runserver 0:8000]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3.x 连接MySQL数据库]]></title>
    <url>%2Fpost%2F64b3833f.html</url>
    <content type="text"><![CDATA[Python3.x 连接MySQL数据库由于 MySQLdb 模块还不支持 Python3.x，所以 Python3.x 如果想连接MySQL需要安装 pymysql 模块。 pymysql 模块可以通过 pip 安装pip install PyMySQL 用pymysql连接数据库 12345678conn = pymysql.connect(host='localhost', port=3306, user='root', password='root', db='hrs', charset='utf8', autocommit=False # 默认不自动提交 ) 常用参数说明: ​host:主机IP ​port:端口号 ​user:mysql登录用户名 ​password:mysql登录密码 ​db:数据库名称 ​charset:连接数据库采用的字符编码 autocommit:默认值是False,DML(数据操纵语言)不会自动提交,如果为True则会自动提交 cursorclass:pymysql.cursors.DictCursor - 设置游标的类型,查询返回的结果是以字典的方式 测试是否连接成功 12print(conn)$ &lt;pymysql.connections.Connection object at 0x05656EB0&gt; 如果运行上面的语句不报错,并且看到了输出到控制台的值,那么我们则用pymysql连接上了MySQL数据库. 方法介绍 12345678910111213connection对象常用的方法cursor() 使用该连接创建并返回游标commit() 提交当前事务rollback() 回滚当前事务close() 关闭连接cursor对象常用的方法和属性execute(sql) 执行一个数据库的查询命令fetchone() 取得结果集的下一行fetchmany(size) 获取结果集的下几行fetchall() 获取结果集中的所有行rowcount 返回数据条数或影响行数close() 关闭游标对象 在介绍上面方法使用前再看看连接数据库的代码块,其中有一个很重要的参数db (数据库名称),所以我们应当在连接数据库之前，先创建一个数据库，方便测试 pymysql 的功能 使用Python实现增删改查和事务处理 首先我们在上面说的hrs数据库中创建一张部门表 1234567891011121314-- 创建部门表create table tbdept(dno int, -- 部门编号dname varchar(10) not null, -- 部门名称dloc varchar(20) not null, -- 部门所在地primary key (dno));-- 添加部门记录insert into tbdept values (10, '会计部', '北京'),(20, '研发部', '成都'),(30, '销售部', '重庆'),(40, '运维部', '深圳'); 12345678910111213def get_conn(): config = &#123; 'host': 'localhost', 'port': 3306, 'user': 'root', 'password': 'root', 'db': 'hrs', 'charset': 'utf8', 'autocommit': False, # 默认不自动提交 'cursorclass': pymysql.cursors.DictCursor # 设置游标的类型,查询返回的结果是以字典的方式 &#125; conn = pymysql.connect(**config) return conn 下面我们先来看看添加的操作: 12345678910111213141516171819202122232425262728def insert(): """ 插入 """ # Connection(连接) / Cursor(游标) conn = get_conn() try: # 创建Cursor对象,cursor支持上下文语法,可以放在with中 with conn.cursor() as cursor: # 向数据库发出sql语句 dno = input('部门编号:') dname = input('部门名称:') dloc = input('部门地址:') # 如果使用字符串格式化的方式来组装SQL语句 # 最大的风险是用被SQL注射攻击 # sql = "insert into tbdept values (%d, '%s', '%s')" % (dno, dname, dloc) # result = cursor.execute(sql) # result = cursor.execute('insert into tbdept values (%s, %s, %s)', (dno, dname, dloc)) # 这个方式传参是以字典的方式,但是要注意的是在占位的时候用%(name)s result = cursor.execute( 'insert into tbdept values (%(dno)s, %(dname)s, %(dloc)s)', &#123;'dno': dno, 'dname': dname, 'dloc': dloc&#125; ) # print('成功插入', cursor.rowcount, '条数据') # 这里cursor.rowcount是获取到受影响的行 print('成功插入', result, '条数据') conn.commit() finally: conn.close() 修改操作 12345678910111213141516171819def update(): """ 修改 """ conn = get_conn() try: with conn.cursor() as cursor: dno = input('部门编号:') dname = input('部门名称:') # 这个方式传参是以字典的方式,但是要注意的是在占位的时候用%(name)s result = cursor.execute( 'update tbdept set dname=%(dname)s where dno=%(dno)s', &#123;'dno': dno, 'dname': dname&#125; ) # print('成功插入', cursor.rowcount, '条数据') # 这里cursor.rowcount是获取到受影响的行 print('成功修改', result, '条数据') conn.commit() finally: conn.close() 删除操作 1234567891011121314151617181920def delete(dno): """ 根据编号删除 :param dno: 编号 """ conn = get_conn() try: with conn.cursor() as cursor: # 向数据库发出sql语句 # execute方法中占位后传参除了元组和字典外,还可以是列表 result = cursor.execute('delete from tbdept where dno=%s', [dno]) # 如果事务中的所有操作全部成功了最后手动提交 conn.commit() print('删除成功' if result == 1 else '删除失败') except Exception as e: print(e) # 如果事务操作有任何一个操作发生异常,那么就会回滚事务 conn.rollback() finally: conn.close() 查询操作 123456789101112131415161718192021def select(): """ 查询 """ conn = get_conn() print(conn) try: # 创建Cursor对象 with conn.cursor() as cursor: # 向数据库发出sql语句 cursor.execute('select dno, dname, dloc from tbdept') result = cursor.fetchone() # 程序中最好不要使用fetchall(),如果库中数据量很大,那么脑补一下会有什么样的结果呢 while result: print(result) # 取出部门名称 # 在这里我上面连接数据时,使用了cursorclass参数,查询时返回的结果是以字典的方式 print(result['dname']) result = cursor.fetchone() finally: conn.close() 到现在我们已经简单的介绍了用pymysql完成了对数据库的CURD操作]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pymysql</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux环境下Git安装与使用]]></title>
    <url>%2Fpost%2F9fdd7050.html</url>
    <content type="text"><![CDATA[Linux环境下Git安装和使用 Linux环境下Git安装与使用 安装 官网下载并解压 123[root@VM_0_11_centos ~]# wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.17.0.tar.gz[root@VM_0_11_centos ~]# tar -xvf git-2.17.0.tar.gz[root@VM_0_11_centos ~]# cd git-2.17.0 移除旧版本 1[root@VM_0_11_centos ~]# yum remove git 安装依赖库 12[root@VM_0_11_centos ~]# yum install libcurl-devel[root@VM_0_11_centos ~]# yum install autoconf automake libtool 执行 1234567[root@VM_0_11_centos ~]# make configureGIT_VERSION = 2.17.0 GEN configure[root@VM_0_11_centos ~]# ./configure --prefix=/usr/local/git --with-iconv =/usr/local/lib（建议优先尝试后者）或者./configure --prefix=/usr/local/git --with-iconv --with-curl --with-expat=/usr/local/lib（如果没有安装libiconv请自行安装）[root@VM_0_11_centos git-2.17.0]# make &amp;&amp; make install 配置环境变量 1234[root@VM_0_11_centos git-2.17.0]# vim ~/.bash_profile在文件末尾追加上下面命令:PATH=$PATH:/usr/local/git/binexport PATH 重新加载环境变量 1[root@VM_0_11_centos git-2.17.0]# source ~/.bash_profile 查看git版本 1[root@VM_0_11_centos git-2.17.0]# git --version 使用 在本地建立本地仓库 123[root@VM_0_11_centos ~]# mkdir test[root@VM_0_11_centos test]# cd test[root@VM_0_11_centos test]# git init 把文件纳入版本控制(加入暂存区) 12[root@VM_0_11_centos test]# git add &lt;filename&gt; # 将修改后的文件加入暂存区[root@VM_0_11_centos test]# git add . # add后跟.是将当前文件夹下面的所有文件及文件夹都加入暂存区 提交到仓库(-m 后是描述) 1[root@VM_0_11_centos test]# git commit -m '本次提交文件的相关描述信息' 如果提交报错,看否是缺少user.name、user.email,可执行下面的命令解决: 12[root@VM_0_11_centos test]# git config --global user.name 'your-name'[root@VM_0_11_centos test]# git config --global user.email 'your-email' 查看放入暂存区的文件 1[root@VM_0_11_centos test]# git status 查看版本 1[root@VM_0_11_centos test]# git log 回滚到某个版本 1[root@VM_0_11_centos test]# git reset --hard 版本号 显示版本包括历史版本 12[root@VM_0_11_centos test]# git reflog[root@VM_0_11_centos test]# git reflog --pretty=oneline # 单行显示 把暂存区的内容全撤回来(可以在本地做修改,然后再次add进暂存区做提交) 1[root@VM_0_11_centos test]# git checkout -- [可跟上文件名] 添加远端仓库 1[root@VM_0_11_centos test]# git remote add origin https://git.coding.net/gavinliu/test.git 将本地仓库和远端仓库同步 1[root@VM_0_11_centos test]# git push -u origin master 创建分支 1[root@VM_0_11_centos test]# git branch [分支名] 查看所有分支 1[root@VM_0_11_centos test]# git branch 切换分支 1[root@VM_0_11_centos test]# git checkout [分支名] 删除文件 1[root@VM_0_11_centos test]# git rm [filename] 合并分支 1[root@VM_0_11_centos test]# git merge [分支名] 克隆项目到本地 1[root@VM_0_11_centos ~]# git clone https://git.coding.net/jackfrued/HelloGit.git 推送到服务器,origin是原始名字,master是分支 1[root@VM_0_11_centos test]# git push origin master 拉取服务器代码 1[root@VM_0_11_centos test]# git pull ​]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell常用命令]]></title>
    <url>%2Fpost%2Fcdf8534a.html</url>
    <content type="text"><![CDATA[shell的基本常用命令 指令 常用操作 指令 操作 pwd 当前路径 cd 更改目录 不加参数 进入主目录 ls 当前目录内容 ls -a 查看隐藏文件 ls -l 文件详情 ls -ld 目录本身信息 mkdir 新建文件夹 rm 删除文件 rm -rf dir/ 删除文件夹 cp file1 file2 复制文件 cp -r dir1 dir2 复制文件夹 mv file .. 移动文件到上一级 mv file dir/ 移动到dir目录 mv file1 file2 替换文件 mv dir1 dir2 替换文件夹 touch a.text 创建文件 &gt;a.text 创建文件 touch .file.text 创建隐藏文件 [cat / less / more] file 查看文件 file a.txt 查看文件类型 man xx 打开xx指令的手册 /x 查找某参数 n 查找下一处 table 可以补全路径名 echo 打印 date 显示时间 cal 显示日历 解压缩 指令 操作 unzip 解压 zip -r 压缩 file.zip dir 压缩后的名称 压缩对象 tar zxvf 解压.tar.gz tar zcvf 压缩文件 tar jxvf 解压tar.bz2 tar jcvf 解压tar.bz2 重定向 指令 操作 丨 管道线 &lt; 标准输入重定向 &gt; 把输出流保存到文件中 重定向前把文件清空 &gt;&gt; 把输出流保存到文件中 不清空文件 cat file1 &gt;file 把file1中的内容写入file中 cat file1 &gt;&gt;file 把file1中的内容写入file内容后 ls shit 2 &gt;out.txt 标准错误输出 权限 用户在自己的主目录有写权限，在其它目录没有。 指令 操作 su - 更改用户为root sudo 超级用户 chmod +x +r +w 添加执行/读/写权限 chmod 777 添加全部权限 进程 指令 操作 ps -[ef丨aux] 丨grep 应用名称 查看进程 kill [-9] pid 结束某个程序 搜索 指令 操作 find file -type f 搜索文件 find dir/ -type d 搜索目录 概念 概念 解释 绝对路径 以 / 开头的路径 相对路径 相对于当前工作路径 ./当前工作目录 ../上一级目录 通配符 例如 *]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux环境下安装Python3.X]]></title>
    <url>%2Fpost%2Fc8a586cb.html</url>
    <content type="text"><![CDATA[Linux下大部分系统默认自带python2.x的版本，最常见的是python2.6或python2.7版本，默认的python被系统很多程序所依赖，比如centos下的yum就是python2写的，所以默认版本不要轻易删除，否则会有一些问题，如果需要使用最新的Python3那么我们可以编译安装源码包到独立目录，这和系统默认环境之间是没有任何影响的，python3和python2两个环境并存即可 Linux环境下安装Python3.X 1.下载Python源代码并解压缩到指定目录 123[root@iZwz95cxo3u633jbk49xrpZ ~]# wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tar.xz[root@iZwz95cxo3u633jbk49xrpZ ~]# xz -d Python-3.6.5.tar.xz[root@iZwz95cxo3u633jbk49xrpZ ~]# tar -xvf Python-3.6.5.tar 2.安装依赖库,没有安装可能导致Python在最后安装失败 1[root@iZwz95cxo3u633jbk49xrpZ ~]# yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel 3.进入Python源码目录进行配置和安装 12345[root@iZwz95cxo3u633jbk49xrpZ ~]# cd Python-3.6.5[root@iZwz95cxo3u633jbk49xrpZ ~]# ./configure --prefix=/usr/local/python36 --enable-optimizations# 待上面命令成功后[root@iZwz95cxo3u633jbk49xrpZ ~]# make &amp;&amp; make install#如果在中途遇见错误,解决错误信息再重新执行make &amp;&amp; make install即可 4.创建软链接 123[root@iZwz95cxo3u633jbk49xrpZ ~]# ln -s /usr/local/python36/bin/python3 /usr/bin/python3[root@iZwz95cxo3u633jbk49xrpZ ~]# ln -s /usr/local/python36/bin/pip3 /usr/bin/pip3[root@iZwz95cxo3u633jbk49xrpZ ~]# ln -s /usr/local/python36/bin/2to3 /usr/bin/2to3 123注意:要用ipython环境要先安装ipyton[root@iZwz95cxo3u633jbk49xrpZ ~]# pip3 install ipython[root@iZwz95cxo3u633jbk49xrpZ ~]# ln -s /usr/local/python36/bin/ipython3 /usr/bin/ipython3]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo]]></title>
    <url>%2Fpost%2Fb132932.html</url>
    <content type="text"><![CDATA[使用hexo搭建一个博客，并托管在github pages上的简易教程。 什么是Hexo? Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装前提 安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js Git 如果您的电脑中已经安装上述必备程序，那么恭喜您！接下来只需要使用 npm 即可完成 Hexo 的安装。 1$ npm install -g hexo-cli 如果有环境问题,请参考Hexo文档 如何使用Hexo搭建Github Pages博客 创建仓库 首先我们需要在Github创建一个账号,然后我们登录进去创建仓库 仓库名称必须为 username.github.io (注：username为你在github的用户名) 如果报错就是仓库已经存在。 仓库创建成功后，github会给你该仓库的https和ssh地址，复制ssh地址作为备用 建站 上面的操作完成后,我们就可以进行建站操作了,可以参照Hexo建站文档,也可以运行下面的命令 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 上面操作完成后我们建站就成功了,现在我们可以看看自己的博客是什么样子了。在命令行运行下面这条的命令： 1$ npm install hexo-server --save server安装成功后，启动server便可以把博客跑起来： 1$ hexo server 现在默认是在本地运行,运行hexo server成功后会出现一个http://localhost:4000/地址 部署 到现在我们自己的博客就已经搭建起来,现在我们修改项目的部署信息 在博客的根目录下可以找到_config.yml文件,在文件最后可以找到deploy信息,现做如下配置 1234deploy: type: git repository: git@github.com:&#123;username&#125;/&#123;username&#125;.github.io.git # 这个地址就是github创建仓库后的shh地址 branch: master 以上配置好后,我们再配置一下SSH key,关于SSH key的生成和配置，github有详细的帮助文档可以参考。这里做简单介绍： 完成以上操作就完成了SSH key的配置。 有了这些配置后便可以提交部署到Github Pages,执行下面的命令: 1$ hexo clean # 删除database和public文件夹,因为执行了hexo server 成功后再执行下面的命令 12$ hexo generate # 生成新的部署所需要的文件$ hexo deploy 在执行了hexo deploy命令后可能会出现报错ERROR Deployer not found : github,这个需要再安装hexo-deployer-git 1$ npm install hexo-deployer-git --save 完成后再次执行hexo deploy命令,待命令执行成功后我们就成功部署了自己的博客,直接访问https://your-github-username.github.io/这样就可以看到默认主题的博客了]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3基础之函数]]></title>
    <url>%2Fpost%2F5e13db2f.html</url>
    <content type="text"><![CDATA[Python3基础之函数 三元运算符补充 123456# 三元运算符(或者三目运算)# 格式# 结果 = 值1 if 条件 else 值2result = True if 1&gt;2 else False# 如果条件成立则返回'值1',否则返回'值2' 函数的定义 12345678910111213141516171819在开始之前我们先上一段代码:while True： if cpu利用率 &gt; 90%: # 发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 if 硬盘使用空间 &gt; 90%: # 发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 if 内存占用 &gt; 80%: # 发送邮件提醒 连接邮箱服务器 发送邮件 关闭连接 不知道大家是否注意到以上代码有很多重复的代码,要写出高质量的代码首先要解决的就是重复代码的问题。我们可以将上面的’发送邮件提醒’功能封装成一个函数,当满足条件时调用函数即可。所以某些具有特殊功能的代码块, 将这些特定的代码块给他分封装起来,这个封装起来的代码块就是函数。 1234567891011121314151617181920212223242526# 定义函数函数的定义主要有如下要点： 1.def：表示函数的关键字 2.函数名：函数的名称，日后根据函数名调用函数（函数的命名规则和变量的命名规则一致） 3.函数体：函数中进行一系列的逻辑计算，如：发送邮件 4.参数：为函数体提供数据 5.返回值：当函数执行完毕后，可以给调用者返回数据。下面我们重构一下上面发送邮件的代码：def send(): 发送邮件的代码... if success: #发送成功 return True else: return False while True: # 每次执行发送邮件函数，都会将返回值自动赋值给result # 之后，可以根据result来写日志，或重发等操作 # 在这儿根据函数名'send'来调用函数 result = send() if result == False: 记录日志，短信发送失败... 匿名函数 123456789对于简单的函数，也存在一种简便的表示方式，即：lambda表达式# 普通方式def sum(arg): return arg + 5print(sum(5))# 匿名函数lambda_sum = lambda x, y: x + yprint(lambda_sum(5, 5)) 函数的运用 1234567891011121314151617181920212223242526272829# 递归调用,遍历文件夹目录import osdef get_all_dir(path, str="|--"): """ 实现目录的遍历 :param path: 遍历目录的路径 :param str: 样式 :return: """ # 返回一个指定文件夹(目录)包含文件和文件夹(目录),并且返回一个列表,但是不包含.和..,他一般按照英文首字母排序 fill_all = os.listdir(path) # print(fillAll) for filename in fill_all: # 一定不能少了全路径的拼接 file_path = os.path.join(path, filename) # print(filePath) # 判断filePath是否是目录 if os.path.isdir(file_path): print(str + '文件夹' + filename) # filePath是目录 get_all_dir(file_path, "\t" + str) else: # filePath不是目录,即是文件 print(str + '文件:' + filename)get_all_dir('../gavinliu_study')]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3基础之分支结构和循环结构]]></title>
    <url>%2Fpost%2F8b5ec2d0.html</url>
    <content type="text"><![CDATA[在Python中，要构造分支结构可以使用if、elif和else关键字。所谓关键字就是有特殊含义的单词，像if和else就是专门用于构造分支结构的关键字，很显然你不能够使用它作为变量名（事实上，用作其他的标识符也是不可以） 在程序中我们需要重复的执行某条或某些指令，例如比如在我们的程序中要实现每隔1秒中在屏幕上打印一个”hello, world”这样的字符串并持续一个小时，我们肯定不能够将print(&#39;hello, world&#39;)这句代码写上3600遍。当然你可能已经注意到了，刚才的描述中其实是重复的动作 Python3基础之分支结构和循环结构 分支结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273第一种: if结构格式: if 表达式: 语句块执行流程:程序遇到if结构时,判断表达式的真假,如果为真,则执行语句块,否则结束if结构 表达式为假的情况: 0 '' None False [] () &#123;&#125;num1 = 78num2 = 45tmp = 0if num1 &gt; num2: tmp = num1 num1 = num2 num2 = tmp print(num1, num2)第二种: if-else结构格式: if 表达式: 语句块1 else: 语句块2执行流程:程序执行到if结构,判断表达式的真假,如果为真,则执行语句块1,如果为假,则执行else中的语句块的内容# 判断一个年份是否是闰年year = int(input('请输入你要输入的年份:'))if ((year % 4 == 0) and (year % 100 != 0)) or (year % 400 == 0): print('是润年')else: print('不是闰年')第三种: if-elif-else格式: if 表达式1: 语句块1 elif 表达式2: 语句块2 elif 表达式3: 语句块3 else: 语句块n执行流程:程序执行到if语句,判断表达式1的真假,如果为真,则执行语句1,否则判断表达式2的真假,如果为真,则执行表达式2,一次类推,直到最后一个表达式为假,就执行else里边的语句块nimport random# 产生一个1-6的随机数num = random.randint(1, 6)# print(num)if num == 1: print('这个随机数乘以1后是%d' % (num * 1))elif num == 2: print('这个随机数乘以2后是%d' % (num * 2))elif num == 3: print('这个随机数乘以3后是%d' % (num * 3))elif num == 4: print('这个随机数乘以4后是%d' % (num * 4))elif num == 5: print('这个随机数乘以5后是%d' % (num * 5))else: print('这个随机数乘以本身后是%d' % (num * num)) 第四种: 嵌套if 表达式1: if 表达式2: 语句块1 else: 语句块2else: 语句块3name = 'xiaoming'password = '123456'user = input('请输入用户名')pwd = input('请输入密码')if user == name: if pwd == password: print('恭喜你,登录成功') else: print('用户名或者密码错误2')else: print('用户名或者密码错误1') 循环结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#for-in循环#如果明确的知道循环执行的次数或者是要对一个容器进行迭代，使用for-in循环"""用for循环实现1~100求和"""sum = 0for x in range(101): sum += xprint(sum)"""输入两个正整数计算最大公约数和最小公倍数"""num1 = int(input("请输入第一个整数:"))num2 = int(input("请输入第二个整数:"))# actual = Noneactual = min(num1, num2) # 取最小的整数# commonDivisorVal = Nonefor i in range(1, actual + 1): if (num1 % i == 0) and (num2 % i == 0): commonDivisorVal = iprint("%d和%d的最大公约数是%d" % (num1, num2, commonDivisorVal))#while循环#如果要构造不知道具体循环次数的循环结构，推荐使用while循环，while循环通过一个能够产生或转换出bool值的表达式来控制循环，表达式的值为True循环继续，表达式的值为False循环结束。"""猜数字游戏计算机出一个1~100之间的随机数由人来猜计算机根据人猜的数字分别给出提示大一点/小一点/猜对了"""import randomanswer = random.randint(1, 100)counter = 0while True: counter += 1 number = int(input('请输入: ')) if number &lt; answer: print('大一点') elif number &gt; answer: print('小一点') else: print('恭喜你猜对了!') breakprint('你总共猜了%d次' % counter) 注意:上面的代码中使用了break关键字来提前终止循环，需要注意的是break只能终止它所在的那个循环，这一点在使用嵌套的循环结构需要注意。除了break之外，还有另一个是continue，它可以用来跳过本次循环直接进入下一轮循环。]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>分支结构</tag>
        <tag>循环结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3基础之字符串、列表、元组、字典、集合等相关操作]]></title>
    <url>%2Fpost%2F9a7b293f.html</url>
    <content type="text"><![CDATA[下面将一一介绍Python中的字符串、列表、元组、集合的相关操作 字符串、列表、元组、字典、集合等相关操作 字符串操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104字符串一般情况使用单引号或者双引号引起来规则: 单不能套单,单可以套双 双不能套双,双可以套单## 多行字符串# 方式二str3 = '3月25号我来到了美丽的成都,刚下飞机,'\ '就碰到一个老外帅锅,结果他还跟我打招呼.但是我不会英语,好尴尬'# 方式一str4 = '''洁白的婚纱,手捧着鲜花 ,美丽的想通话.想起那年初夏,我为你牵挂.'''# 方式三str5 = """洁白的婚纱,手捧着鲜花 ,美丽的想通话.想起那年初夏,我为你牵挂."""print(str5)#--------------------------------------------------------------------------------##字符串的长度'''len(): 获取字符串的长度'''str1 = 'lhy'str1 = '刘海艳'str1 = '幸福像花儿一样'print(len(str1))总结: 无论是字符还是汉字,只要有一个,则长度就是几个#-------------------------------------------------------------------------------##提取某个字符'''提取字符串中的某一个字符方式:从左往右开始, 下标从0开始提取str[0] str[1] str[2] ..... str[n]从优往左开始, 下标从-1开始str[-1] str[-2] ..... str[-n]'''str1 = 'It is a dog'print(str1[0])print(str1[-2])# pytjon中的字符串一旦定义好之后,是不可以修改的str1[0] = 'i'print('str1 =', str1)#------------------------------------------------------------------------------##转义字符'''转义字符: 将原来的意义给他去掉 \(这是反斜线) '''str1 = '我最喜欢的一首歌是\'咱们结婚吧\''str1 = 'you\'re a good man'print(str1)'''系统里边有一些特殊的字符: \t(制表符) \n(换行符)'''print('你好\\t宝强哥,你的媳妇是马蓉吗?')print('你好\\n宋喆,你的情人是\t马蓉\n吗?')# 如果想将原来具有特殊含义的字符失去本身的意义,我们可以直接在整个字符串前边添加一个rprint(r'你好\t宝强哥,你的媳妇是马蓉吗?')print(r'你好\n宋喆,你的情人是马蓉吗?')#-------------------------------------------------------------------------------##字符串截取'''字符串截取:str1[开始下标:结束下标]: 从开始下标截取,到结束下标=结尾,.包含开始下标,但不包含结束下标str1[0:5] 提取是的结果你是zhousstr1[:3] 默认从下标0开始到结束的开区间str1[3:] 从指定的下标开始到结尾str1[-n:]: 从最后边提取n个str1[:]: 提取全部字符str1[::2] 根据下标每个n个提取一次str1[::-1] 将字符逆序排列'''#------------------------------------------------------------------------------##字符串的格式化%s: 给字符串站位%d: 给int类型站位%f: 给浮点类型站位, 默认保留6为小数 %.2f: 保留两位小数 %10.2f 共10位,保留两位小数,其他为使用空格补齐 %010.2f 共10位,保留两位小数,其他为使用0补齐%c: 打印一个字符%o: 将十进制转换成八进制%x: 将十进制转换成十六进制name = '宝强'age = 36like = 'green'print('他是&#123;&#125;,今年&#123;&#125;,他喜欢&#123;&#125;' .format(name, age, like))print('他是&#123;lala&#125;,今年&#123;wawa&#125;, 他喜欢&#123;heihei&#125;' .format(wawa=age, heihei=like, lala=name))#--------------------------------------------------------------------------------##字符串比较大小[规则:]从第一个字符开始比较,将字符转换成ascii值进行比较如果小于则返回True,否则返回False 列表操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113'''列表: 格式: 列表名 = [元素1, 元素2,,...]'''# append()往列表后边追加一个元素list1 = [1, 2, 3, 4]list1.append(100)# print(list1)list1.append([200, 400, 600])# print(list1)# 在末尾一次性追加另外一个列表中的多个值list2 = [3, 4, 5]list2.extend([200, 300, 400])# print(list2)'''insert():在指定下标出添加一个元素,原来位置处的元素往后移动@参数一: 列表的下标@参数二: 列表中下标所对应的值'''list3 = [5, 6, 7]list3.insert(1, 250)# print(list3)'''pop():将列表中指定下标的元素删除, 如果默认不传递参数,则删除的是最后一个元素@参数一: [可有可无] 列表对应的下标'''list4 = [3, 4, 5, 6, 7, 8]list4.pop()list4.pop()# print(list4)list4.pop(1)# print(list4)'''remove():移除列表中指定的元素@参数一: 列表中的元素'''list5 = [3, 4, 5, 6, 7]list5.remove(3)# print(list5)'''clear():清除列表中所有的元素'''list6 = [1, 3, 4]list6.clear()# print(list6)'''index():获取列表元素锁对应的下标@参数一: 列表中的某一个元素返回值: 列表中元素所对应的下标'''list7 = [1, 2, 3, 4, 5, 6]index1 = list7.index(3)# print(index1)# 获取列表的长度list8 = [1, 2, 3, 4, 5]# print(len(list8))list9 = [1, 3, 4, 6, 7]# print(max(list9))list10 = [1, 3, 4, 6, 7]# print(min(list9))'''count(): 计算列表中元素出现的次数'''list11 = [1, 3, 4, 3, 3, 3, 3, 6, 7]# print(list11.count(3))'''reverse():将列表进行倒序排列'''list12 = [1, 2, 3, 4, 5, 10, 7, 8, 9]list12.reverse()# print(list12)'''sort():将列表中的元素进行升序排列'''list13 = [1, 100, 78, 23, 65, 43]list13.sort()# print(list13)'''引用传递: 在列表中,如果修改一个列表的元素,那么对应的另外一个列表的元素也改变'''list14 = [2, 3, 4]list15 = list14list15[1] = 200# print(list14)# print(list15)# id():查看内存的地址# print(id(list14))# print(id(list15))'''值传递在列表中修改一个元素的值时,对应的另外一个列表中的值是不发生改变的'''list16 = [1, 2, 3, 4]list17 = list16.copy()list17[1] = 250print(list16)print(list17)print(id(list16))print(id(list17)) 元组操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104'''元组:也是一种有序集合特点:1.跟列表非常相似: list1 = [] tuple1 = ()2.一旦定义好之后不能修改3.使用小括号括起一个集合'''# 创建一个空元祖tuple1 = ()# print(tuple1)# print(type(tuple1))tuple2 = (23, 3.14, 'aaa', None, True)# print(tuple2)tuple3 = (4, )#tupletuple3 = (4)#int# print(tuple3)# print(type(tuple3))# 访问元祖的元素tuple4 = (2, 3, 4, 5, 6)# print(tuple4[0])# print(tuple4[1])# 元祖在访问的时候一定不能溢出(越界),直接报错# print(tuple4[5])# 获取元祖中最后一个元素# print(tuple4[-1])# print(tuple4[-2])# 一定不能越界# print(tuple4[-6])tuple5 = (1, 2, 3, 4, [12, 34, 45])# print(tuple5)# tuple5[0] = 150#直接报错, 元祖定义好之后是不能修改的# print(tuple5)# print(tuple5[-1][-2])# print(tuple5[4][1])# 删除元祖的用法跟删除变量的用法一致tuple6 = (2, 3, 5, 6)del tuple6# print(tuple6)# 元祖进行操作tuple7 = (3, 4, 5)tuple8 = (6, 7, 8)# 将两个元祖合并成一个新的元祖tuple9 = tuple7 + tuple8# print(tuple9)# print(tuple7, tuple8)# 将元祖重复n次,最后返回一个新的元祖tuple10 = (3, 4, 5)# print(tuple10 * 10)# 判断一个元素是否在一个元祖中,如果在返回True,否则返回Falsetuple11 = (3, 5, 7)# print(4 in tuple11)# 元祖的截取# 格式: 元祖名[开始下标:结束下标]tuple12 = (1, 2, 3, 4, 5, 6, 7, 8, 9,10)# print(tuple12[1:5])# print(tuple12[3:])# print(tuple12[:6])# 将元祖进行逆序排列# print(tuple12[::-1])# 二维元祖:tuple13 = ((2, 3, 4), (5, 6, 7))# print(tuple13[1][1])# 元祖的方法# 求一个元祖的长度tuple14 = (2, 3, 4, 5)# print(len(tuple14))# print(max(tuple14))# print(min(tuple14))# 对列表进行遍历# for i in [1, 2, 3, 4, 5]:# print(i)# 对元祖进行遍历# for x in (3, 4, 5, 6, 7):# print(x)# 将列表转换成元祖list11 = [3, 5, 6, 8]tuple15 = tuple(list11)# print(tuple15)# 将元祖转换成列表tuple16 = (4, 6, 8, 9)list12 = list(tuple16)# print(list12)list13 = range(0, 10)print(type(list13))# range(): 从开始值开始,但不包含结束值[0, 10)# for i in range(0, 10):# print(i) 字典操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162'''dict(字典): 在字典里边一般存放的是键值对的形式.键1 = 值1 key = value键1 : 值1写法: &#123;键1:值1, 键2:值2, 键3:值3&#125;例子:dict1 = &#123;'name': '刘海艳', 'age': 17, 'sex': 'girl'&#125;注意事项:1.在字典中键(key)的值必须是唯一2.在字典中可以存放多个键值对3.在字典中键(key)必须是不可变类型 字符串 整数都可以作为键(key)4.list和tuple都是有序集合, 而dictsahib无序集合'''# 必须保证字典中的key是唯一的dict1 = &#123;'yelei': 100, 'xiaoming' : 99, 'jianfei': 59, 'zhouying': 110&#125;# print(dict1)# 元素的访问# 获取的方式: 字典名[key]# print(dict1['xiaoming'])# print(dict1.get('yelei'))# print('yelei' in dict1)ret = dict1.get('lhy')# if ret == None:# print('没有')# else:# print('有')dict2 = &#123;'user': '大黄', 'sex': '男', 'age': 30, 'height': 150 &#125;# 添加元素dict2['lover'] = '小白'# 一个key只能对应一个valuedict2['lover'] = '小绿'# 修改dict2['lover'] = '小花'# 删除dict2.pop('sex')# 直接报错,因为他是无需集合# dict2.pop()# print(dict2)# 遍历# for x in dict2:# print(x, dict2[x])# 获取字典所有的value和key# print(dict2.values())# print(dict2.keys())# for i in dict2.values():# print(i)## for i in dict2.keys():# print(i)# for k,v in dict2.items():# print(k, v)#for k,v in enumerate(dict2):# print(k, v) 集合操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162'''set(集合):类似于dict, 也是无序的,以key-value新的形势存在,但是没有value作用: 是对list,tuple,dict进行去重的, 求交集.并集1.set是无序2.set集合是不可改变的'''# set1 = set([1, 2, 3, 5, 3, 2])# print(set1)# print(type(set1))# set2 = set((1, 2, 3, 5, 3, 2, 4, 5))# print(set2)# print(type(set2))# set3 = set(&#123;3, 4, 5,5, 6, 3, 7&#125;)# print(set3)# print(type(set3))# 添加set4 = set([3, 3, 4, 5, 7, 2, 1, 2])set4.add(8)# set4.add(3)#可以添加重复的值,但是没效果# 总结:list和dict是可改变的, 而tuple是不可改变# set4.add([10, 9])#直接报错,不能添加list# set4.add((10, 9))# set4.add(&#123;'a':1&#125;)#直接报错,不能添加字典## print(set4)# 修改set5 = set([1, 2, 3, 4, 5])# 将list dict tuple 等等整个插入进去# set5.update([6, 7, 8])# set5.update(&#123;9, 10&#125;)# set5.update((11, 56))# print(set5)# 删除# set6= set([3, 4, 5, 6, 7])# set6.remove(4)# print(set6)# 遍历set7 = set([1, 2, 3, 4, 6])set7 = set(['aaa', 'bbb', 'ccc'])set7 = set((1, 2, 3, 2, 4, 2, 3))# 在set集合中,没有value,即使有value也遍历不出来set7 = set(&#123;'name':'小花', 'age': 18&#125;)# for i in set7:# print(i, end = ',')# &amp; | - ^set8 = set([1, 2, 3, 4])set9 = set([3, 2, 4, 5])set10 = set8 &amp; set9set11 = set8 | set9set12 = set8 - set9set13 = set8 ^ set9print(set10)print(set11)print(set12)print(set13)]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>字符串</tag>
        <tag>列表</tag>
        <tag>元组</tag>
        <tag>字典</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3基础之基本数据类型、变量和运算符]]></title>
    <url>%2Fpost%2F510bec9b.html</url>
    <content type="text"><![CDATA[Python3基础之基本数据类型、变量和运算符 基本数据类型 123456789101112131415在Python中，能够直接处理的数据类型有以下几种： 1）整型:Python可以处理任意大小的整数，当然包括负整数 如：100 -100 计算机由于使用二进制，所以Python支持二进制（0b100）、八进制(0o100)和十六进制(0xff00)表示 2)浮点数:浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的 如: 123.456 但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x10^9就是1.23e9 3)字符串型:字符串是以单引号或双引号括起来的任意文本 如: 'hello world' 或者 "hello world" 4)布尔型:布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写） 如: True False 3 &gt; 2 2 &lt; 5)空类型:空类型是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。(请注意不要与其它语言中的null混淆)在Python中还支持复数、列表、字典、元组、集合等数据类型，之后会一一说明。 变量 12345678910111213141516171819什么是变量 在计算机程序中，变量不仅可以是数字，还可以是任意数据类型。变量是一种存储数据的载体，其值可以被读取和修改。变量命名 a.只能有数字,字母,下划线组成 b.不能以数字开始 c.不要跟关键字和系统保留字冲突 `关键字: ['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'] d. 严格区分大小写 e. 见名之意 f. 遵循驼峰原则: UserName userName user_name变量的实用和作用 a = 100 b = -100 c = 3.1415926 d = True e = False f = None 作用是将不同的数据类型存到内存中 运算符 Python支持多种运算符，下表大致按照优先级从高到低的顺序列出了所有的运算符，我们会陆续使用到它们。说明:此片段引用原文https://blog.csdn.net/jackfrued/article/details/79392196]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据类型</tag>
        <tag>变量</tag>
        <tag>运算符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之禅]]></title>
    <url>%2Fpost%2F6be2dbca.html</url>
    <content type="text"><![CDATA[用过 Python的人，基本上都知道在交互式解释器中输入 import this 就会显示 Tim Peters 的 The Zen of Python，但它那偈语般的语句有点令人费解，所以我想分享一下我对它的体会，顺带给出我的翻译。 Python之禅1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Beautiful is better than ugly. 优美胜于丑陋（Python 以编写优美的代码为目标）Explicit is better than implicit. 明了胜于晦涩（优美的代码应当是明了的，命名规范，风格相似）Simple is better than complex. 简洁胜于复杂（优美的代码应当是简洁的，不要有复杂的内部实现）Complex is better than complicated. 复杂胜于凌乱（如果复杂不可避免，那代码间也不能有难懂的关系，要保持接口简洁）Flat is better than nested. 扁平胜于嵌套（优美的代码应当是扁平的，不能有太多的嵌套）Sparse is better than dense. 间隔胜于紧凑（优美的代码有适当的间隔，不要奢望一行代码解决问题）Readability counts. 可读性很重要（优美的代码是可读的）Special cases aren&apos;t special enough to break the rules. Although practicality beats purity.即便假借特例的实用性之名，也不可违背这些规则（这些规则至高无上）Errors should never pass silently. 不要包容所有错误Unless explicitly silenced. 除非你确定需要这样做（精准地捕获异常，不写 except:pass 风格的代码）In the face of ambiguity, refuse the temptation to guess. 当存在多种可能，不要尝试去猜测There should be one-- and preferably only one --obvious way to do it. 而是尽量找一种，最好是唯一一种明显的解决方案（如果不确定，就用穷举法）Although that way may not be obvious at first unless you&apos;re Dutch. 虽然这并不容易，因为你不是 Python 之父（这里的 Dutch 是指 Guido ）Now is better than never. 做也许好过不做Although never is often better than right now. 但不假思索就动手还不如不做If the implementation is hard to explain, it&apos;s a bad idea. If the implementation is easy to explain, it may be a good idea.如果你无法向人描述你的方案，那肯定不是一个好方案；反之亦然（方案测评标准）Namespaces are one honking great idea -- let&apos;s do more of those! 命名空间是一种绝妙的理念，我们应当多加利用（倡导与号召）]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
